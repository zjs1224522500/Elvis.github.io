<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.shunzi.tech</id>
    <title>Elvis Zhang</title>
    <updated>2021-02-28T14:57:12.106Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.shunzi.tech"/>
    <link rel="self" href="https://blog.shunzi.tech/atom.xml"/>
    <subtitle>The easy way or the right way.</subtitle>
    <logo>https://blog.shunzi.tech/images/avatar.png</logo>
    <icon>https://blog.shunzi.tech/favicon.ico</icon>
    <rights>All rights reserved 2021, Elvis Zhang</rights>
    <entry>
        <title type="html"><![CDATA[Linux Software Management]]></title>
        <id>https://blog.shunzi.tech/post/linux-software-mangement/</id>
        <link href="https://blog.shunzi.tech/post/linux-software-mangement/">
        </link>
        <updated>2021-02-26T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Linux 软件包管理安装。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Linux 软件包管理安装。</li>
</ul>
</blockquote>
<!--more-->
<h2 id="参考书目链接">参考书目/链接</h2>
<ul>
<li><a href="https://www.linuxprobe.com/linux-basic-manage.html">Linux 就该这么学：Linux软件包管理基本操作入门</a></li>
<li><a href="https://segmentfault.com/a/1190000011325357">segmentfault: Linux系统中软件的“四”种安装原理详解</a></li>
<li><a href="https://www.cnblogs.com/zwj-linux/p/11643033.html">博客园：Linux下rpm、yum和源码三种安装方式简介</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-rpm1/index.html">IBM Developer - 构建和分发包</a></li>
</ul>
<h2 id="软件包">软件包</h2>
<h3 id="windows">Windows</h3>
<ul>
<li>开始介绍 Linux 软件包之前，先大致回忆一下 Windows 场景下的我们常常使用的软件包形态。</li>
<li>Win 环境下：
<ul>
<li>大量的软件应用程序都是使用诸如 .exe/.msi 等类型的文件进行安装，直接运行该安装程序，按照步骤勾选相应的可选配置就能安装对应的软件。
<ul>
<li>MSI 就是 microsoft installer 的简写，msi 文件就是 window installer 的数据包，把所有和安装文件相关的内容封装在一个包里。</li>
<li>exe 是一个安装引导程序。主要是用于检查安装的环境，当检查成功后，会自动再安装 msi 文件。</li>
</ul>
</li>
<li>除了安装程序以外，常常还有一些压缩包的形式来安装软件（也就是所谓的绿色版），本质就是把程序的全部数据以及依赖的环境给压缩到一个文件夹中，而下载该文件夹就可以直接执行其中保存的可执行程序。</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210225220304.png" alt="20210225220304" loading="lazy"></figure>
<h3 id="linux">Linux</h3>
<ul>
<li>大多数现代类 Unix 操作系统都提供了一个集中的软件包管理机制，以帮助用户搜索、安装和管理软件。而软件通常以「包」的形式存储在仓库「repository」中，对软件包的使用和管理被称为包管理。</li>
<li>Linux 包的基本组成部分通常有：共享库、应用程序、服务和文档。</li>
<li>Linux 软件包分类:
<ul>
<li>源码包</li>
<li>二进制包（RPM/DEB包）</li>
<li>yum/apt 源在线安装</li>
<li>脚本安装包（本质还是源码包和二进制包）</li>
</ul>
</li>
</ul>
<h2 id="linux-软件包管理">Linux 软件包管理</h2>
<h3 id="软件包管理系统">软件包管理系统</h3>
<ul>
<li>大多数包管理系统是建立在包文件上的集合，包文件通常包含编译好的二进制文件和其它资源组成的：软件、安装脚本、元数据及其所需的依赖列表。</li>
<li>因为 Linux 有很多发行版，各自的管理系统也有一定的差异</li>
</ul>
<table>
<thead>
<tr>
<th>系统</th>
<th>格式</th>
<th>工具</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debian</td>
<td>.deb</td>
<td>apt, apt-cache、apt-get、dpkg</td>
</tr>
<tr>
<td>Ubuntu</td>
<td>.deb</td>
<td>apt、apt-cache、apt-get、dpkg</td>
</tr>
<tr>
<td>CentOS</td>
<td>.rpm</td>
<td>yum</td>
</tr>
<tr>
<td>Fedora</td>
<td>.rpm</td>
<td>dnf</td>
</tr>
</tbody>
</table>
<ul>
<li>Debian 及其衍生产品如：Ubuntu、Linux Mint 和 Raspbian 的包格式为.deb文件，APT 是最常见包操作命令，可：搜索库、安装包及其依赖和管理升级。而要直接安装现成.deb包时需要使用dpkg命令。</li>
<li>CentOS、Fedora 及 Red Hat 系列 Linux 使用RPM包文件，并使用yum命令管理包文件及与软件库交互。在最新的 Fedora 版本中，yum命令已被dnf取代进行包管理。</li>
</ul>
<h3 id="常见操作">常见操作</h3>
<ul>
<li>更新本地包数据库列表：
<ul>
<li>apt-get update</li>
<li>yum check-update</li>
<li>dnf check-update</li>
</ul>
</li>
<li>升级已安装的包：
<ul>
<li>apt-get upgrade</li>
<li>apt-get dist-upgrade</li>
<li>yum update /dnf upgrade</li>
</ul>
</li>
<li>查找/搜索软件包：
<ul>
<li>apt-cache search xxx</li>
<li>yum search xxx</li>
<li>yum search all xxx</li>
<li>dnf search xxx</li>
<li>dnf search all xxx</li>
</ul>
</li>
<li>查看某个软件包信息:
<ul>
<li>apt-cache show [pkg_name]</li>
<li>dpkg -s [pkg_name] (显示包的安装状态)</li>
<li>yum info [pkg_name]</li>
<li>yum deplist [pkg_name] （列出包的依赖）</li>
<li>dnf info  [pkg_name]</li>
<li>dnf repoquery –requires [pkg_name]</li>
</ul>
</li>
<li>安装包：
<ul>
<li>apt-get install xxx</li>
<li>yum instal xxx</li>
<li>dnf install xxx</li>
</ul>
</li>
<li>移除包：
<ul>
<li>apt-get remove xxx</li>
<li>apt-get autoremove (自动移除已知不需要的包)</li>
<li>yum remove xxx</li>
<li>dnf erase xxx</li>
</ul>
</li>
</ul>
<h3 id="本地文件系统安装">本地文件系统安装</h3>
<ul>
<li>首先简要介绍本地安装软件包的场景和主要命令，然后以 RPM 为例详细说明。</li>
</ul>
<h4 id="从本地文件系统直接安装包">从本地文件系统直接安装包</h4>
<ul>
<li>很多时候，我们在进行测试或从某个地方直接拿到软件包之后需要从本地文件系统直接安装包。（特别是针对一些无公网的环境，无法直接使用包管理工具直接安装相关依赖）</li>
<li>Debian 及衍生系统可以使用 dpkg 进行安装，CentOS 和 Fedora 系统使用 yum 和 dnf 命令进行安装。</li>
</ul>
<h5 id="本地安装命令">本地安装命令</h5>
<ul>
<li>dpkg -i [pkg_name].deb</li>
<li>apt-get install -y gdebi&amp;&amp; sudo gdebi [pkg_name].deb （使用gdebi检索缺少的依赖关系）</li>
<li>yum install [pkg_name].rpm</li>
<li>dnf install [pkg_name].rpm</li>
</ul>
<h3 id="rpm">RPM</h3>
<ul>
<li>RPM命名“RedHat Package Manager”，简称则为RPM。这个机制最早由Red Hat这家公司开发出来的，后来实在很好用，因此很多distributons就使用这个机制来作为软件安装的管理方式，包括Fedora，CentOS，SuSE等知名的开发商都是用它。</li>
<li>RPM最大的特点就是需要安装的软件已经编译过，并已经打包成RPM机制的安装包，通过里头默认的数据库记录这个软件安装时需要的依赖软件。当安装在你的Linux主机时，RPM会先依照软件里头的数据查询Linux主机的依赖属性软件是否满足，若满足则予以安装，若不满足则不予安装。</li>
</ul>
<h4 id="构建流程">构建流程</h4>
<p>要构建 RPM，必须：</p>
<ul>
<li>依照 rpmbuild 规范设定一个目录结构。</li>
<li>将源代码和附带文件放在目录中合适的位置。</li>
<li>创建 spec 文件。</li>
<li>编译 RPM。可以选择编译源 RPM，以与其他人共享您的源代码。</li>
</ul>
<h5 id="目录结构">目录结构</h5>
<ul>
<li><strong>BUILD</strong>。BUILD 用作实际编译软件的暂存空间。</li>
<li><strong>RPMS</strong>。RPMS 包含 rpmbuild 所编译的二进制 RPM。</li>
<li><strong>SOURCES</strong>。SOURCES 存储源代码。</li>
<li><strong>SPECS</strong>。SPECS 包含您的 spec 文件，您想要构建的一个 RPM 对应一个 spec 文件。</li>
<li><strong>SRPMS</strong>。SRPMS 包含在这个过程中构建的源 RPM。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210226153713.png" alt="20210226153713" loading="lazy"></figure>
<h5 id="准备文件">准备文件</h5>
<ul>
<li><strong>step1</strong>. 将源代码（理想情况下应捆绑为一个 tarball 压缩文件）复制到 SOURCES 目录</li>
<li><strong>step2</strong>. 创建 spec 文件。spec 文件只是一个具有特殊语法的文本文件。
<ul>
<li>此处举一个简单例子 wget 打包</li>
</ul>
</li>
</ul>
<pre><code class="language-shell"># This is a sample spec file for wget

# 定义了五个变量 
%define _topdir     /home/strike/mywget
%define name            wget 
%define release     1
%define version     1.12
%define buildroot %{_topdir}/%{name}-%{version}-root
 
# 设置若干关键参数
BuildRoot:  %{buildroot}
Summary:        GNU wget
License:        GPL
Name:           %{name}
Version:        %{version}
Release:        %{release}
Source:         %{name}-%{version}.tar.gz
Prefix:         /usr
Group:          Development/Tools

# 简单明了地描述软件。这一行将在用户运行 rpm -qi 来查询 RPM 数据库时显示。您可以说明包的用途，描述任何警告或额外的配置说明等。
%description
The GNU wget program downloads files from the Internet using the command-line.

# 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行
# 准备源代码
# %setup -q 是一个 %prep 宏，用于自动解压 Source 中的特定 tarball 压缩文件
%prep
%setup -q

# 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行
# 手动配置和启动构建过程的步骤 
%build
./configure
make

# 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行
%install
make install prefix=$RPM_BUILD_ROOT/usr

# 列出应该捆绑到 RPM 中的文件，还可以设置权限和其他信息。
%files
%defattr(-,root,root)
/usr/local/bin/wget

# %doc 告诉 RPM 该文件为一个文档文件，所以如果用户使用 --excludedocs 安装包，将不会安装该文件。
%doc %attr(0444,root,root) /usr/local/share/man/man1/wget.1
</code></pre>
<ul>
<li>再来看 tcmu-runner.spec 的例子</li>
</ul>
<pre><code class="language-spec">%global _hardened_build 1

# 运行 rpmbuild 时读取的一些条件参数
# without rbd dependency
# if you wish to exclude rbd handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without rbd
%bcond_without rbd

# without glusterfs dependency
# if you wish to exclude glfs handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without glfs
%bcond_without glfs

# without qcow dependency
# if you wish to exclude qcow handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without qcow
%bcond_without qcow

# without zbc dependency
# if you wish to exclude zbc handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without zbc
%bcond_without zbc

# without file backed optical dependency
# if you wish to exclude fbo handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without fbo
%bcond_without fbo

# without tcmalloc dependency
# if you wish to exclude tcmalloc, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without tcmalloc
%bcond_without tcmalloc

# 设置若干关键参数
Name:          tcmu-runner
Summary:       A daemon that handles the userspace side of the LIO TCM-User backstore
Group:         System Environment/Daemons
License:       ASL 2.0 or LGPLv2+
Version:       1.0
URL:           https://github.com/open-iscsi/tcmu-runner

Release:       0%{dist}
BuildRoot:     %(mktemp -udp %{_tmppath}/%{name}-%{version})
Source:       %{name}-%{version}.tar.gz
ExclusiveOS:   Linux

BuildRequires: cmake make gcc
BuildRequires: libnl3-devel glib2-devel zlib-devel kmod-devel

# 针对不同条件参数下的依赖处理
%if %{with rbd}
BuildRequires: librbd1-devel librados2-devel
Requires(pre): librados2, librbd1
%endif

%if %{with glfs}
BuildRequires: glusterfs-api-devel
Requires(pre): glusterfs-api
%endif

%if %{with tcmalloc}
BuildRequires: gperftools-devel
Requires:      gperftools-libs
%endif

# 基本依赖
Requires(pre): kmod, zlib, libnl3, glib2, logrotate, rsyslog
Requires:      libtcmu = %{version}-%{release}

# 软件描述
%description
A daemon that handles the userspace side of the LIO TCM-User backstore.

LIO is the SCSI target in the Linux kernel. It is entirely kernel code, and
allows exported SCSI logical units (LUNs) to be backed by regular files or
block devices. But, if we want to get fancier with the capabilities of the
device we're emulating, the kernel is not necessarily the right place. While
there are userspace libraries for compression, encryption, and clustered
storage solutions like Ceph or Gluster, these are not accessible from the
kernel.

The TCMU userspace-passthrough backstore allows a userspace process to handle
requests to a LUN. But since the kernel-user interface that TCMU provides
must be fast and flexible, it is complex enough that we'd like to avoid each
userspace handler having to write boilerplate code.

tcmu-runner handles the messy details of the TCMU interface -- UIO, netlink,
pthreads, and DBus -- and exports a more friendly C plugin module API. Modules
using this API are called &quot;TCMU handlers&quot;. Handler authors can write code just
to handle the SCSI commands as desired, and can also link with whatever
userspace libraries they like.

# 生成的 RPM 包 libtcmu
%package -n libtcmu
Summary:       A library supporting LIO TCM-User backstores processing
Group:         Development/Libraries

%description -n libtcmu
libtcmu provides a library for processing SCSI commands exposed by the
LIO kernel target's TCM-User backend.

# 生成的 RPM 包 libtcmu-devel
%package -n libtcmu-devel
Summary:       Development headers for libtcmu
Group:         Development/Libraries
Requires:      %{name} = %{version}-%{release}
Requires:      libtcmu = %{version}-%{release}

%description -n libtcmu-devel
Development header(s) for developing against libtcmu.

%global debug_package %{nil}

# 准备源代码
%prep
%setup -n %{name}-%{version}

# 手动配置和启动构建
%build
%{__cmake} \
 -DSUPPORT_SYSTEMD=ON -DCMAKE_INSTALL_PREFIX=%{_usr} \
 %{?_without_rbd:-Dwith-rbd=false} \
 %{?_without_zbc:-Dwith-zbc=false} \
 %{?_without_qcow:-Dwith-qcow=false} \
 %{?_without_glfs:-Dwith-glfs=false} \
 %{?_without_fbo:-Dwith-fbo=false} \
 %{?_without_tcmalloc:-Dwith-tcmalloc=false} \
 .
%{__make}

%install
%{__make} DESTDIR=%{buildroot} install
%{__rm} -f %{buildroot}/etc/tcmu/tcmu.conf.old
%{__rm} -f %{buildroot}/etc/logrotate.d/tcmu-runner.bak/tcmu-runner

# 列出应该捆绑到 RPM 中的文件
%files
%{_bindir}/tcmu-runner
%dir %{_sysconfdir}/dbus-1/
%dir %{_sysconfdir}/dbus-1/system.d
%config %{_sysconfdir}/dbus-1/system.d/tcmu-runner.conf
%dir %{_datadir}/dbus-1/
%dir %{_datadir}/dbus-1/system-services/
%{_datadir}/dbus-1/system-services/org.kernel.TCMUService1.service
%{_unitdir}/tcmu-runner.service
%dir %{_libdir}/tcmu-runner/
%{_libdir}/tcmu-runner/*.so
%{_mandir}/man8/*
%doc README.md LICENSE.LGPLv2.1 LICENSE.Apache2
%dir %{_sysconfdir}/tcmu/
%config %{_sysconfdir}/tcmu/tcmu.conf
%config(noreplace) %{_sysconfdir}/logrotate.d/tcmu-runner
%ghost %attr(0644,-,-) %{_sysconfdir}/tcmu/tcmu.conf.old
%ghost %attr(0644,-,-) %{_sysconfdir}/logrotate.d/tcmu-runner.bak/tcmu-runner

%files -n libtcmu
%{_libdir}/libtcmu*.so.*

%files -n libtcmu-devel
%{_libdir}/libtcmu*.so

</code></pre>
<h5 id="构建-rpm">构建 RPM</h5>
<ul>
<li>例如构建 wget:</li>
</ul>
<blockquote>
<p><code>rpmbuild -v -bb --clean SPECS/wget.spec</code></p>
</blockquote>
<p>此命令使用指定的 spec 文件构建一个二进制包（-bb 表示 “构建二进制包”），还会生成详细的输出（-v）。构建实用程序在生成包之后删除构建树（--clean）。如果还希望构建源 RPM，指定 -ba（“构建所有包”）来代替 -bb。（查看 rpmbuild 清单页面，了解完整的选项列表。）</p>
<ul>
<li>rpmbuild 执行以下步骤：
<ul>
<li>读取并解析 wget.spec 文件。</li>
<li>运行 %prep 节，将源代码解压到临时目录。在这里，临时目录为 BUILD。</li>
<li>运行 %build 节，编译代码。</li>
<li>运行 %install 节，将代码安装到构建机器上的目录中。</li>
<li>从 %files 节读取文件列表，将它们收集到一起，然后创建一个二进制 RPM（和源 RPM 文件，如果已选择）。</li>
</ul>
</li>
</ul>
<h4 id="安装流程">安装流程</h4>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210226151530.png" alt="20210226151530" loading="lazy"></figure>
<ul>
<li><code>rpm -ivh package-name</code>
<ul>
<li>-i：install的意思，安装</li>
<li>-v：查看更详细的安装信息画面（provide more detailed output）</li>
<li>-h：以安装信息栏显示安装进度</li>
</ul>
</li>
</ul>
<h5 id="错误处理">错误处理</h5>
<ul>
<li>安装rpm包时提示错误：依赖检测失败: <code>--nodeps --force</code></li>
</ul>
<pre><code class="language-cmd">[root@localhost ~]# rpm -ivh tcmu-runner-2.0.1-0.el7.x86_64.rpm 
错误：依赖检测失败：
	libhcs_obj_util.so()(64bit) 被 tcmu-runner-2.0.1-0.el7.x86_64 需要

[root@localhost ~]# rpm -ivh tcmu-runner-2.0.1-0.el7.x86_64.rpm --nodeps --force
准备中...                          ################################# [100%]
正在升级/安装...
   1:tcmu-runner-2.0.1-0.el7          ################################# [100%]
[root@localhost ~]# systemctl status tcmu-runner
● tcmu-runner.service - LIO Userspace-passthrough daemon
   Loaded: loaded (/usr/lib/systemd/system/tcmu-runner.service; disabled; vendor preset: disabled)
   Active: inactive (dead)
     Docs: man:tcmu-runner(8)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reading Group Notes]]></title>
        <id>https://blog.shunzi.tech/post/ReadingGroup/</id>
        <link href="https://blog.shunzi.tech/post/ReadingGroup/">
        </link>
        <updated>2021-02-19T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>参加 Systems Reading Group 记录的论文笔记。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>参加 Systems Reading Group 记录的论文笔记。</li>
</ul>
</blockquote>
<!--more-->
<h1 id="system-reading-group">System Reading Group</h1>
<ul>
<li>Group 介绍以及 Presentation 安排： https://learn-sys.github.io/cn/reading/</li>
<li>THU AOS 2020: http://os.cs.tsinghua.edu.cn/oscourse/AOS2020</li>
</ul>
<h2 id="week-1-operating-system">Week 1: Operating System</h2>
<h3 id="course-notes">Course Notes</h3>
<ul>
<li>Video: THU AOS P7 - P11</li>
</ul>
<h4 id="os-architecture-structure">OS Architecture &amp; Structure</h4>
<ul>
<li><strong>OS Structure</strong>:
<ul>
<li>Simple kernel</li>
<li>Monolithic kernel</li>
<li>Micro kernel</li>
<li>Exokernel</li>
<li>VMM(Virtual Machine Monitor), etc...</li>
</ul>
</li>
<li><strong>Monolithic kernel</strong>
<ul>
<li>UNIX Arch<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219163131.png" alt="20210219163131" loading="lazy"></li>
<li>Linux Arch<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219163153.png" alt="20210219163153" loading="lazy"></li>
</ul>
</li>
<li><strong>Micro kernel</strong>
<ul>
<li><strong>微内核：功能相对较少的内核，只提供某些核心功能。从而相比于单体内核，把很多单体内核中的事情放到用户空间去做，解耦了内核的各个 features，让整个系统的稳定性和灵活性得到了提升。但也就因为 IPC 的开销导致性能表现不尽如人意。</strong></li>
<li>Kernel with minimal features</li>
<li>Moves as much from the kernel into user space
<ul>
<li>Address spaces</li>
<li>Interprocess communication (IPC)</li>
<li>Scheduling</li>
</ul>
</li>
<li>Benefits
<ul>
<li>Flexibility</li>
<li>Safety</li>
<li>Modularity</li>
</ul>
</li>
<li>Detriments (Poor Performance)
<ul>
<li>Address spaces</li>
<li>Interprocess communication (IPC)</li>
<li>Scheduling</li>
</ul>
</li>
<li><strong>Mach</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165141.png" alt="20210219165141" loading="lazy"></li>
<li><strong>L4</strong> - Microkernel– L4Second generation microkernel
<ul>
<li>synchronous IPCs –&gt; async IPCs (like epoll in Linux)</li>
<li>smaller, Mach 3(330 KB) –&gt; L4 (12KB)</li>
<li>IPC security checks moved to user process</li>
<li>IPC is hardware dependent</li>
</ul>
</li>
</ul>
</li>
<li><strong>Exokernel</strong>
<ul>
<li><strong>Exokernel 要做的事情其实是把内核也近乎给 PASS 掉，尽可能减少抽象层次，允许应用程序直接访问硬件，而ExoKernel只负责保护和分配系统资源。说白了就是把硬件资源都直接交给应用程序自己来组织了，因为有大量的应用程序想要自己独立控制可管理硬件，而不需要你操作系统层面的过多干涉。</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165849.png" alt="20210219165849" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165610.png" alt="20210219165610" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="paper-1-the-multikernel-a-new-os-architecture-for-scalable-multicore-systems">Paper 1: The Multikernel: A new OS architecture for scalable multicore systems</h3>
<ul>
<li><strong>SOSP09</strong></li>
<li><strong>SIGOPS Hall of Fame Award 2020</strong></li>
</ul>
<h4 id="abstract">Abstract</h4>
<ul>
<li>普通计算机系统包含越来越多的处理器核心，并呈现出越来越多的架构权衡，包括内存层次结构、互连、指令集和变体，以及IO配置。 以前的高性能计算系统在特定情况下进行了扩展，但是现代客户机和服务器工作负载的动态特性，加上不可能针对所有工作负载和硬件变体静态地优化操作系统，对操作系统结构构成了严重的挑战。</li>
<li>我们认为，迎接未来多核硬件挑战的最好方法是拥抱机器的网络化本质，重新思考使用来自分布式系统的思想的操作系统架构。我们研究了一种新的操作系统结构，即 Multikernel，它将机器视为一个由独立核心组成的网络，假定在最低层次上没有核间共享，并将传统的操作系统功能转移到一个通过消息传递进行通信的分布式进程系统。</li>
<li>我们已经实现了一个多内核操作系统来证明这种方法是有前途的，并且我们描述了操作系统的传统的可伸缩性问题(如内存管理)是如何通过消息有效地重新解决的，以及如何利用分布式系统和网络的洞察力。在多核系统上对我们的原型的评估表明，即使在现在的机器上，多内核的性能也可以与传统的相媲美，并且可以更好地扩展以支持未来的硬件。</li>
</ul>
<h4 id="problems">Problems</h4>
<ul>
<li>随着不断变化的技术对摩尔定律的限制，处理器架构变得越来越多样化，且逐渐转向异构化，并向可扩展的架构发展，以适应高性能的应用。传统的单体操作系统在解决可伸缩性问题和针对不同硬件结构进行优化方面面临着巨大的挑战。
<ul>
<li>Systems are increasingly diverse</li>
<li>Cores are increasingly diverse</li>
<li>The interconnect matters</li>
<li>Messages cost less than shared memory</li>
<li>Cache coherence is not a panacea</li>
<li>Messages are getting easier</li>
</ul>
</li>
<li>本文作者尝试通过在内核之间使用显式消息传递和在内核之间复制内核状态来解决这个问题，而不是使用共享内存模型。他们的另一个主要目标是使这个操作系统与硬件无关，不针对任何机器架构。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210220221342.png" alt="20210220221342" loading="lazy"></li>
</ul>
<h4 id="contributions">Contributions</h4>
<ul>
<li>多内核操作系统的主要贡献嵌入在它们的三个设计原则中：
<ul>
<li>通过消息传递显式地实现内核间通信</li>
<li>使操作系统结构与硬件无关</li>
<li>在内核之间复制内核状态</li>
</ul>
</li>
<li>该系统侧重于非共享内存模型，通过消息的显式通信来维护缓存的一致性。基于以上原则，设计实现了 MultiKernel 原型 Barrelfish<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210224113033.png" alt="20210224113033" loading="lazy"></li>
</ul>
<h2 id="week-2-virtualization">Week 2: Virtualization</h2>
<h3 id="course-notes-2">Course Notes</h3>
<ul>
<li>Video
<ul>
<li>THU AOS P12 - P21</li>
<li>IPADS MOS P70 - P87</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MapReduce: Simplified Data Processing on Large Clusters]]></title>
        <id>https://blog.shunzi.tech/post/MapReduce/</id>
        <link href="https://blog.shunzi.tech/post/MapReduce/">
        </link>
        <updated>2021-02-08T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。</li>
</ul>
</blockquote>
<!--more-->
<h2 id="before-beginning">Before Beginning</h2>
<ul>
<li>为什么要读这篇论文呢？其实这篇文章之前也已经简单看过了，只是最近开始刷 MIT6.824，本来是想直接做相关 Lab 的，但是发现还是有整理不清楚的思路，觉得还是有必要回顾一下，那就多花点时间继续研读吧~</li>
<li>网上关于 6.824 以及 MapReduce 的资料很多了，我在这里只是做一些简单的记录，如果有发现其他大佬做的比较好的笔记，也会贴在这里，以供膜拜学习。我的 6.824 系列的的博客介绍大抵都是如此。</li>
<li>话不多说，学习开始~</li>
</ul>
<h2 id="abstract">Abstract</h2>
<ul>
<li><strong>大致流程</strong>: 用户指定一个 map 函数来处理键/值对以生成一组中间键/值对，以及一个 reduce 函数来合并与同一中间键相关的所有中间值</li>
<li><strong>为什么这么做？</strong> 是想充分利用不同机器的并行性来处理大量的数据，分别执行 map 和 reduce 任务来完成大数据任务，提高每个 host 的利用率。（也就是分布式系统的原型）</li>
<li><strong>需要解决的问题：</strong>
<ul>
<li>数据输入的切分</li>
<li>不同机器上执行的任务的调度</li>
<li>机器故障处理</li>
<li>机器间通信的管理</li>
</ul>
</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>根本矛盾</strong>：少数据量时单机能够直接运行简单的任务来完成相关计算，但面对大数据量的情况下，引入多计算实例组成的系统的复杂性和本身计算任务的简单性之间的矛盾。</li>
<li><strong>思想起源</strong>：来自于 Lisp 语言的函数式编程思想中的 map/reduce 函数。
<ul>
<li>在 lisp 语言中，map 作为一个输入函数接受一个序列，然后处理每个序列中 value 值，然后 reduce 将最终的 map 计算出来的结果整理成最终程序输出。</li>
</ul>
</li>
</ul>
<h2 id="programming-model">Programming Model</h2>
<ul>
<li>MapReduce 本质是一种编程模型
<ul>
<li>Map: 由用户编写，接受一个输入对并生成一组中间键/值对</li>
</ul>
</li>
</ul>
<pre><code class="language-Java">// map (k1,v1) → list(k2,v2)
map(String key, String value): 
    // key: document name 
    // value: document contents 
    for each word w in value: 
        EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<ul>
<li>Reduce: 也由用户编写，接受一个中间键和该键的一组值。它将这些值合并在一起，形成一个可能更小的值集</li>
</ul>
<pre><code class="language-Java">// reduce (k2,list(v2)) → list(v2)
reduce(String key, Iterator values): 
    // key: a word 
    // values: a list of counts 
    int result = 0; 
    for each v in values: 
        result += ParseInt(v); 
    Emit(AsString(result));
</code></pre>
<ul>
<li><strong>应用实例</strong>：
<ul>
<li>Distributed Grep</li>
<li>Count of URL Access Frequency：map &lt;URL, 1&gt;, reduce &lt;URL, total count&gt;</li>
<li>Reverse Web-Link Graph: map &lt;target, source&gt;, reduce &lt;target, list(source)&gt;</li>
<li>Term-Vector per Host</li>
<li>Inverted Index: map &lt;word, document ID&gt;, reduce &lt;word, list(document ID)&gt;</li>
<li>Distributed Sort</li>
</ul>
</li>
</ul>
<h2 id="implementation">Implementation</h2>
<ul>
<li>Map 函数分布在多个机器上，相应地自动将输入数据划分为 M 份，然后可以由分布了 Map 函数的机器并行处理这些数据。而对于 Reduce 则是将中间数据划分为 R 份，通常需要使用一个分割函数，常见的就是 <code>hash(key) mod R</code> 来将中间 Key 进行区分。</li>
<li>下图演示了整个 MapReduce 的流程，当用户程序调用 MapReduce 函数时将按照以下顺序执行：
<ul>
<li>
<ol>
<li>MapReduce Library 首先将输入文件划分为 M 个分片，每个分片大小通常为 16MB or 64MB，可以由用户控制，然后开始将程序拷贝到各个机器上，也就是图中的 <strong>fork</strong> 过程</li>
</ol>
</li>
<li>
<ol start="2">
<li>fork 的过程中会有一个特殊的情况，即 master 节点上运行的程序。剩下的 worker 对应执行的任务都是由 master 分配的，有 M 个 map task 和 R 个 reduce task 需要分配，master 选择空闲的 worker 来执行 map 或者 reduce task。</li>
</ol>
</li>
<li>
<ol start="3">
<li>被分配到 map task 的 worker 首先读取分片的数据内容，它从输入数据中解析键/值对，并将每对键/值传递给用户定义的 Map 函数，然后由 Map 产生的中间键值对将被缓冲在内存中；</li>
</ol>
</li>
<li>
<ol start="4">
<li>缓冲在内存中的中间数据将定期执行刷回操作写到磁盘，然后再由用户定义的分割函数执行将中间数据分割为 R 个区域，这些原本缓冲在内存中的数据持久化到磁盘之后的地址将传递给 master，然后 master 负责告诉 reduce task worker 这些数据在哪里。</li>
</ol>
</li>
<li>
<ol start="5">
<li>执行 reduce task 的 worker 在接收到来自 master 的数据地址的通知之后，使用 RPC 来从 map worker 的本地磁盘中读取数据，当一个 reducer 读取到了所有的中间数据之后，就可以根据中间键对它进行排序，以便将所有出现的相同键组合在一起。之所以需要排序，是因为通常有许多不同的键映射到同一个reduce任务。如果中间数据量太大，无法装入内存，则使用外部排序。</li>
</ol>
</li>
<li>
<ol start="6">
<li>reduce worker 迭代已排序的中间数据，对于遇到的每个惟一的中间键，它将键和相应的中间值集传递给用户的 reduce 函数。Reduce 函数的输出被追加到这个 Reduce 分区的最终输出文件中；</li>
</ol>
</li>
<li>
<ol start="7">
<li>所有的 map 任务和 reduce 任务都完成后，master 唤醒用户程序。此时，用户程序中的MapReduce 调用返回到用户程序。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210208135434.png" alt="20210208135434" loading="lazy"></li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="master-data-structures">Master Data Structures</h3>
<ul>
<li>master 节点保存了几个数据结构：
<ul>
<li>对于每个 map 和 reduce task，它存储了 task 对应的状态（idle, in-progress, completed）</li>
<li>worker machine 的标识（非空闲任务运行所在的机器）</li>
</ul>
</li>
<li>master 是一个管道，通过它将中间文件区域的位置从 map task 传播到 reduce task。因此，对于每个完成的 map task，master 存储了 map task 产生的 R 个中间文件区域的位置和大小，当 map task 完成时，master 将接收对该位置和大小信息的更新。信息被递增地推送给正在进行 reduce task 的 worker。</li>
</ul>
<h3 id="fault-tolerance">Fault Tolerance</h3>
<ul>
<li>由于 MapReduce 库被设计用来帮助处理使用成百上千台机器的大量数据，所以这个库必须能够优雅地容忍机器故障。</li>
</ul>
<h4 id="worker-failure">Worker Failure</h4>
<ul>
<li>master 周期地 ping 每个 worker，如果在确定时间内未收到对应的响应，则认为该 worker 宕机，标记该 worker 为 failed，由 worker <strong>已经完成</strong>的任何 map task 都将被重置回初始的空闲 <em>idle</em> 状态，因此有资格对其他 worker 进行重新调度，类似地，在一个失败的 worker 上<strong>正在进行</strong>的任何 map task 或reduce task 也会被重置为空闲，并可以重新调度。</li>
<li>在发生故障时，完成的 map task 将被重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。<strong>已完成的 reduce 任务</strong>不需要重新执行，因为它们的输出存储在全局文件系统中。</li>
<li>假设一个 map task A 一开始由 A 执行，之后由 B 执行（因为 A failed），所有正在执行 reduce task 的 workers 将被通知重新执行，任何尚未从 worker A 读取数据的 reduce task 都将从 worker B 读取数据。</li>
</ul>
<h4 id="master-failure">Master Failure</h4>
<ul>
<li>让 master 定期对上面描述的 master 节点存取的数据结构做 checkpoint 很容易。如果 master task 失效，可以从最后一个检查点状态启动一个新的副本。然而，考虑到只有一个主机，它的失败是不太多见，因此，如果 master 失败，我们当前的实现将终止 MapReduce 计算。客户端可以检查这种情况，如果他们愿意，可以重试 MapReduce 操作。</li>
</ul>
<h4 id="semantics-in-the-presence-of-failures">Semantics in the Presence of Failures</h4>
<ul>
<li>当用户提供的 map 和 reduce 操作符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的非故障顺序执行产生的输出相同。</li>
<li>我们依赖 map 和 reduce task 输出的原子提交来实现该属性。每个正在进行的任务都将其输出写入私有临时文件。一个 reduce task 生成一个这样的文件，map task 生成 R 个这样的文件(每个 reduce task 一个)。当 map task 完成时，worker 向 master 发送一条消息，并在消息中包含 R 临时文件的名称，如果 master 接收到一个<strong>已经完成</strong>的 map task 的完成消息，它将忽略该消息。否则，它将在主数据结构中记录 R 文件的名称。</li>
<li>当一个 reduce task 完成之后，reduce worker 自动地将其临时输出文件重命名为最终输出文件。如果在多台机器上执行相同的 reduce 任务，那么将对相同的最终输出文件执行多个 rename 调用。我们依赖于底层文件系统提供的原子重命名操作，以确保最终文件系统状态只包含一次执行 reduce 任务所产生的数据。</li>
<li>我们的 map 和 reduce 操作符绝大多数都是确定性的，在这种情况下，我们的语义等价于顺序执行，这使得程序员可以很容易地推断他们的程序行为。当 map 或 reduce 操作符是不确定的时，我们提供较弱但仍然合理的语义。在存在非确定性操作符的情况下，特定 reduce 任务 R1 的输出等价于由非确定性程序的顺序执行产生的 R1 的输出。然而，不同 reduce 任务 R2 的输出可能对应于非确定性程序的不同顺序执行产生的 R2 输出。</li>
<li>考虑 map 任务 M 和 reduce 任务 R1 和 R2。设 e(Ri) 是所承诺的 Ri 的执行(只有一个这样的执行)。由于 e(R1) 可能读取了 M 的一次执行产生的输出，而 e(R2) 可能读取了 M 的另一次执行产生的输出，所以语义较弱。</li>
</ul>
<h3 id="locality">Locality</h3>
<ul>
<li>在我们的计算环境中，网络带宽是一个相对稀缺的资源。通过利用输入数据(由 GFS 管理)存储在组成集群的机器的本地磁盘这一事实，我们节约了网络带宽。GFS 将每个文件划分为64 MB的块，并在不同的机器上存储每个块的多个副本(通常是3个副本)，MapReduce master 将输入文件的位置信息考虑在内，并尝试在包含相应输入数据副本的机器上调度map任务。如果失败，它将尝试调度靠近该任务输入数据副本的 map 任务(例如，在与包含数据的机器在同一网络交换机上的工作机器上)。当在集群中相当一部分 worker 上运行大型MapReduce 操作时，大部分输入数据都是在本地读取的，不会消耗网络带宽</li>
</ul>
<h3 id="task-granularity">Task Granularity</h3>
<ul>
<li>如上所述，我们将 map 阶段细分为 M 个部分，将 reduce 阶段细分为 R 个部分。理想情况下，M 和 R 应该远远大于工作机器的数量。让每个 worker 执行许多不同的任务可以改善动态负载平衡，并在 worker 失败时加快恢复速度:它完成的许多 map 任务可以分散到所有其他 worker 机器上。</li>
<li>在我们的实现中，M 和 R 的大小最多有多大是有实际限制的，因为如上所述，master 必须做出 O(M + R) 调度决策，并在内存中保持 O(M*R) 状态。(内存使用的常量是很小的:状态的 O(M∗R) 部分由每个 map任务/reduce任务对大约一个字节的数据组成。)</li>
<li>此外，R 常常受到用户的限制，因为每个 reduce 任务的输出都以单独的输出文件结束。在实践中，我们倾向于选择 M，以便每个单独的任务大约有 16MB 到 64MB 的输入数据(以便上面描述的局部性优化最有效)，并且我们将 R 设为预期使用的工作机器数量的小倍数。我们经常使用 2000 台 worker 机器进行 M = 200000 和 R = 5000 的 MapReduce 计算。</li>
</ul>
<h3 id="backup-tasks">Backup Tasks</h3>
<ul>
<li>导致 MapReduce 操作总时间延长的一个常见原因是“掉线”(straggler)。一种需要异常长时间才能完成计算过程中最后几个 map 或 reduce 任务之一的机器。掉队者出现的原因有很多。例如，磁盘有问题的机器可能会经常出现可纠正错误，导致读性能从 30MB/s 降至 1MB/s。集群调度系统可能已经调度了机器上的其他任务，由于 CPU、内存、本地磁盘或网络带宽的竞争，导致它执行 MapReduce 代码的速度变慢。我们最近遇到的一个问题是，机器初始化代码中的一个bug导致了处理器缓存被禁用:受影响机器的计算速度降低了 100 倍以上。</li>
<li>我们有一个一般性的机制来缓解掉队者的问题。当 MapReduce 操作接近完成时，master 会对剩余的正在执行的任务进行备份。只要主执行或备份执行完成，任务就被标记为完成。我们已经调优了这种机制，因此它通常不会增加操作使用的计算资源超过几个百分点。我们发现，这大大减少了完成大型 MapReduce 操作的时间。以5.3中所述的排序程序为例，关闭备份机制后，排序程序完成的时间会增加 44%。</li>
</ul>
<h2 id="refinements">Refinements</h2>
<h3 id="partitioning-function">Partitioning Function</h3>
<ul>
<li>MapReduce 的用户指定他们想要的 reduce 任务/输出文件的数量(R)，使用中间键上的分区函数在这些任务之间对数据进行分区。提供了一个默认的分区函数，使用哈希(例如&quot; hash(key) mod R &quot;)。这往往会导致相当平衡的分区。然而，在某些情况下，通过键的其他函数来分区数据是有用的。例如，有时输出键是url，我们希望单个主机的所有条目都在同一个输出文件中结束。为了支持这种情况，MapReduce 库的用户可以提供一个特殊的分区函数。例如，使用&quot; hash(Hostname(urlkey)) mod R &quot;作为分区函数会导致来自同一主机的所有 url 最终出现在同一个输出文件中。</li>
</ul>
<h3 id="ordering-guarantees">Ordering Guarantees</h3>
<ul>
<li>我们保证在给定的分区中，中间键/值对按键的递增顺序进行处理。这种排序保证使得为每个分区生成有序的输出文件变得很容易，当输出文件格式需要支持按键进行有效的随机访问查找，或者输出的用户发现对数据进行排序很方便时，这很有用。</li>
</ul>
<h3 id="combiner-function">Combiner Function</h3>
<ul>
<li>在某些情况下，每个 map 任务产生的中间键有显著的重复，并且用户指定的 Reduce 函数是可交换的和关联的。单词计数示例就是一个很好的例子。由于单词频率倾向于遵循 Zipf 分布，每个 map 任务将产生数百或数千个 &lt;the, 1&gt; 形式的记录。所有这些计数将通过网络发送到一个 reduce 任务，然后由 reduce 函数相加产生一个数字。我们允许用户指定一个可选的Combiner函数，该函数在通过网络发送数据之前对数据进行部分合并。</li>
<li>Combiner 函数在每一个执行 map task 上的机器执行，通常使用相同的代码来实现 combiner 和 reduce 函数，reduce 函数和 combiner 函数之间的唯一区别是 MapReduce 库如何处理函数的输出。reduce 函数的输出被写入最终的输出文件。combiner 函数的输出被写入一个中间文件，该文件将被发送到reduce 任务。</li>
<li>部分 Combine 大大加快了 MapReduce 操作的某些类。</li>
</ul>
<h3 id="input-and-output-types">Input and Output Types</h3>
<ul>
<li>MapReduce 库支持以几种不同的格式读取输入数据。text 模式的输入将每一行视为键/值对：键是文件中的偏移量，值是行内容。另一种常见的支持格式存储按键排序的键/值对序列。每个输入类型实现都知道如何将自己分割成有意义的范围，以便作为单独的 map 任务进行处理(例如，文本模式的范围分割确保范围分割只发生在行边界)。用户可以通过提供一个简单的 <em>reader</em> 接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义输入类型中的一种。</li>
<li><em>reader</em> 并不一定需要提供从文件中读取的数据。例如，很容易定义从数据库或映射在内存中的数据结构中读取记录的 <em>reader</em>。</li>
<li>以类似的方式，我们支持一组输出类型来生成不同格式的数据，并且用户代码很容易添加对新输出类型的支持。</li>
</ul>
<h3 id="side-effects">Side-effects</h3>
<ul>
<li>在某些情况下，MapReduce 的用户发现从他们的 map 或 reduce 操作生成辅助文件作为额外的输出是很方便的。我们依靠应用程序 writer 使这些副作用具有原子性和幂等性。通常，应用程序会写入一个临时文件，并在完全生成该文件后自动重命名该文件。</li>
<li>我们不支持单个任务生成的多个输出文件的原子两阶段提交。因此，产生具有跨文件一致性要求的多个输出文件的任务应该是确定的。这种限制在实践中从来就不是问题。</li>
</ul>
<h3 id="skipping-bad-records">Skipping Bad Records</h3>
<ul>
<li>有时，用户代码中的错误会导致 Map 或 Reduce 函数在特定记录上崩溃。此类 bug 会导致 MapReduce 操作无法完成。通常的做法是修复 bug，但有时这是不可行的;这个 bug 可能存在于源代码不可用的第三方库中。此外，有时忽略一些记录是可以接受的，例如在对一个大数据集进行统计分析时。我们提供了一个可选的执行模式，MapReduce 库会检测哪些记录导致确定性崩溃，并跳过这些记录以便继续前进。</li>
<li>每个工作进程都安装一个信号处理程序来捕获分割违规和总线错误。在调用 Map 或 Reduce 操作之前，MapReduce 库会将参数的序号存储在全局变量中。如果用户代码产生信号，信号处理器发送一个包含序列号的“最后一口气” UDP 包给 MapReduce master。当主服务器在一个特定的记录上看到多个失败时，它指示在下一次重新执行对应的 Map 或 Reduce 任务时应该跳过该记录。</li>
</ul>
<h3 id="local-execution">Local Execution</h3>
<ul>
<li>在 Map 或 Reduce 函数中调试问题可能会很棘手，因为实际的计算发生在分布式系统中，通常在几千台机器上，由 master 动态地做出工作分配决策。为了方便调试、分析和小规模测试，我们开发了 MapReduce 库的替代实现，在本地机器上顺序执行 MapReduce 操作的所有工作。控件提供给用户，以便计算可以限制到特定的映射任务。用户可以用一个特殊的标志来调用他们的程序，然后可以很容易地使用任何他们认为有用的调试或测试工具(例如gdb)。</li>
</ul>
<h3 id="status-information">Status Information</h3>
<ul>
<li>主服务器运行一个内部HTTP服务器，并导出一组状态页面供人们使用。状态页面显示了计算的进度，例如有多少任务已经完成，有多少任务正在进行，输入字节数，中间数据字节数，输出字节数，处理速率等。这些页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算将花费多长时间，以及是否应该向计算中添加更多的资源。这些页面还可以用于计算何时会比预期的慢得多。</li>
<li>此外，顶级状态页面显示哪些 worker 失败了，以及当他们失败时正在处理哪些 map 和 reduce 任务。当试图诊断用户代码中的错误时，此信息非常有用。</li>
</ul>
<h3 id="counters">Counters</h3>
<ul>
<li>MapReduce 库提供了一个计数器来计算各种事件的发生次数。例如，用户代码可能需要计算已处理的字的总数或索引的德文文档的数量，等等。</li>
<li>要使用这个功能，用户代码创建一个命名的计数器对象，然后在 Map 或 Reduce 函数中适当地增加计数器。例如:</li>
</ul>
<pre><code class="language-C++">Counter* uppercase; 
uppercase = GetCounter(&quot;uppercase&quot;);
map(String name, String contents): 
  for each word w in contents: 
    if (IsCapitalized(w)): 
      uppercase-&gt;Increment(); 
    EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<ul>
<li>来自各个 worker 机器的计数器值定期传播到 master (在 ping 响应中附带)。master 聚合成功的 map 和 reduce 任务的计数器值，并在 MapReduce 操作完成时将它们返回给用户代码。当前计数器值也显示在 master 状态页面上，以便人们可以观看实时计算的进度。在聚合计数器值时，master 消除了重复执行同一个 map 或 reduce 任务的影响，以避免重复计算。(重复执行可能是由于我们使用了备份任务以及由于失败而重新执行任务引起的。)</li>
<li>一些计数器值由 MapReduce 库自动维护，例如处理的输入键/值对的数量和产生的输出键/值对的数量。</li>
<li>用户已经发现 counter 工具对于检查 MapReduce 操作的行为是非常有用的。例如，在一些 MapReduce 操作中，用户代码可能希望确保生成的输出对的数量恰好等于处理的输入对的数量，或者确保处理的德文文档的比例在处理的文档总数的某个可容忍的比例内。</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>MapReduce编程模型已经在谷歌上成功地用于许多不同的目的。我们认为这种成功有几个原因。首先，该模型易于使用，即使对于没有并行和分布式系统经验的程序员也是如此，因为它隐藏了并行化、容错、局部性优化和负载平衡的细节。其次，大量的问题可以通过MapReduce计算很容易地表达出来。例如，MapReduce被用于谷歌生产web搜索服务的数据生成、排序、数据挖掘、机器学习以及许多其他系统。第三，我们开发了一个MapReduce的实现，它可以扩展到由数千台机器组成的大型机器集群。该实现有效地利用了这些机器资源，因此适合用于在谷歌中遇到的许多大型计算问题。</li>
<li>我们从这项工作中学到了一些东西。首先，对编程模型的限制使得并行化和分布式计算变得容易，并使这些计算具有容错性。其次，网络带宽是一种稀缺资源。因此，我们系统中的许多优化都旨在减少通过网络发送的数据量:局部性优化允许我们从本地磁盘读取数据，将中间数据的单个副本写入本地磁盘可以节省网络带宽。第三，冗余执行可以用来减少慢速机器的影响，并处理机器故障和数据丢失。</li>
</ul>
<pre><code class="language-C++">#include &quot;mapreduce/mapreduce.h&quot;
// 用户实现map函数
class WordCounter : public Mapper {
 public:
    virtual void Map(const MapInput&amp; input) {
      const string&amp; text = input.value();
      const int n = text.size();
      for (int i = 0; i &lt; n; ) {
        // 跳过前导空格
        while ((i &lt; n) &amp;&amp; isspace(text[i]))
             i++;
         // 查找单词的结束位置
         int start = i;
         while ((i &lt; n) &amp;&amp; !isspace(text[i]))
              i++;
         if (start &lt; i)
            Emit(text.substr(start,i-start),&quot;1&quot;);
        }
 
     }
 
};
 
REGISTER_MAPPER(WordCounter);
// 用户实现reduce函数
class Adder : public Reducer {
    virtual void Reduce(ReduceInput* input) {
              // 迭代具有相同key的所有条目,并且累加它们的value
              int64 value = 0;
              while (!input-&gt;done()) {
                     value += StringToInt(input-&gt;value());
                     input-&gt;NextValue();
              }
              // 提交这个输入key的综合
              Emit(IntToString(value));
       }
 
};
REGISTER_REDUCER(Adder);
int main(int argc, char** argv) {
       ParseCommandLineFlags(argc, argv);
       MapReduceSpecification spec;
       // 把输入文件列表存入&quot;spec&quot;
       for (int i = 1; i &lt; argc; i++) {
              MapReduceInput* input = spec.add_input();
              input-&gt;set_format(&quot;text&quot;);
              input-&gt;set_filepattern(argv[i]);
              input-&gt;set_mapper_class(&quot;WordCounter&quot;);
       }
        //指定输出文件:
       // /gfs/test/freq-00000-of-00100
       // /gfs/test/freq-00001-of-00100
      // ...
       MapReduceOutput* out = spec.output();
       out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);
       out-&gt;set_num_tasks(100);
       out-&gt;set_format(&quot;text&quot;);
       out-&gt;set_reducer_class(&quot;Adder&quot;);
       // 可选操作:在map任务中做部分累加工作,以便节省带宽
       out-&gt;set_combiner_class(&quot;Adder&quot;);
       // 调整参数: 使用2000台机器,每个任务100MB内存
       spec.set_machines(2000);
       spec.set_map_megabytes(100);
       spec.set_reduce_megabytes(100);
       // 运行它
       MapReduceResult result;
       if (!MapReduce(spec, &amp;result)) abort();
       // 完成: 'result'结构包含计数,花费时间,和使用机器的信息
       return 0;
</code></pre>
<hr>
<ul>
<li>论文的部分到此结束，后面展开讲一下 MapReduce 的其他东西。</li>
</ul>
<h2 id="other">Other</h2>
<ul>
<li>MapReduce 最重要的贡献：MR takes care of, and hides, all aspects of distribution!</li>
</ul>
<h3 id="problems">Problems</h3>
<ul>
<li><strong>What if the master gives two workers the same Map() task?</strong>
<ul>
<li>Perhaps the master incorrectly thinks one worker died. it will tell Reduce workers about only one of them.</li>
</ul>
</li>
<li><strong>What if the master gives two workers the same Reduce() task?</strong>
<ul>
<li>they will both try to write the same output file on GFS! atomic GFS rename prevents mixing; one complete file will be visible.</li>
</ul>
</li>
<li><strong>What if a single worker is very slow -- a &quot;straggler&quot;?</strong>
<ul>
<li>perhaps due to flakey hardware. master starts a second copy of last few tasks.</li>
</ul>
</li>
<li><strong>What if a worker computes incorrect output, due to broken h/w or s/w?</strong>
<ul>
<li>too bad! MR assumes &quot;fail-stop&quot; CPUs and software.</li>
</ul>
</li>
</ul>
<h3 id="current-status">Current status</h3>
<ul>
<li>Hugely influential (Hadoop, Spark, &amp;c).</li>
<li>Probably no longer in use at Google.
<ul>
<li>Replaced by Flume / FlumeJava (see paper by Chambers et al).</li>
<li>GFS replaced by Colossus (no good description), and BigTable.</li>
</ul>
</li>
</ul>
<h3 id="conclusion-2">Conclusion</h3>
<ul>
<li>MapReduce single-handedly made big cluster computation popular.
<ul>
<li>-Not the most efficient or flexible.</li>
<li>+Scales well.</li>
<li>+Easy to program -- failures and data movement are hidden.</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is license for source code?]]></title>
        <id>https://blog.shunzi.tech/post/license/</id>
        <link href="https://blog.shunzi.tech/post/license/">
        </link>
        <updated>2021-02-06T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。</li>
</ul>
</blockquote>
<!--more-->
<h3 id="开源许可长啥样">开源许可长啥样？</h3>
<ul>
<li>我们常常在 Github 上看到关于 License 的信息，仿佛 NB 点的项目都挂了个 License（啊没有不挂就不 NB 的意思），一般都长成下面这样，只是可能协议啥的会有区别。如 RocksDB 的 <a href="https://github.com/facebook/rocksdb">repo</a>，使用了 <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GPLv2</a> 和 <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a> 协议，然后 repo 内也有相应的协议文件与之对应，COPYING 和 LICENSE.Apache<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206203249.png" alt="20210206203249" loading="lazy"></li>
</ul>
<h3 id="到底啥是开源许可">到底啥是开源许可？</h3>
<ul>
<li>License 可能大家听说的相对于 Copyright 少一点。那么不妨先说啥是 Copyright。</li>
</ul>
<h4 id="copyrightc">Copyright(C)</h4>
<ul>
<li><strong>Copyright</strong>：中文译作版权，大家在一些数字媒体软件上可能感受的真切一点，比如某首歌只有部分音乐公司拥有其版权，那为什么有的公司没有版权就不能提供相关音乐的播放和下载服务呢，不妨拆词解义 copy + right，即复制的权利，说的简单点就是没有某个产品 Copyright 的公司就无法对该产品进行复制，就更别说进行修改发布了。</li>
</ul>
<blockquote>
<p>百度百科：版权是对计算机程序、文学著作、音乐作品、照片、游戏，电影等的复制权利的合法所有权。除非转让给另一方，版权通常被认为是属于作者的。大多数计算机程序不仅受到版权的保护，还受软件许可证的保护。版权只保护思想的表达形式，而不保护思想本身。算法、数学方法、技术或机器的设计均不在版权的保护之列。</p>
</blockquote>
<ul>
<li>如果有去公司实习或者工作过的同学应该就知道，往往在公司的项目里写相关代码的时候往往会有一条编程规范的限制，即 Copyright 的声明，许多 IDE 也有相关 Copyright 模板和自动生成插件的提供。如下为 RocksDB 源代码中关于 CopyRight 的声明。Copyright 约定了版权归属谁，并归定了这个软件的使用许可证方式。</li>
</ul>
<pre><code class="language-Java">// Copyright (c) 2011-present, Facebook, Inc.  All rights reserved.
//  This source code is licensed under both the GPLv2 (found in the
//  COPYING file in the root directory) and Apache 2.0 License
//  (found in the LICENSE.Apache file in the root directory).

package org.rocksdb;

public abstract class Cache extends RocksObject {
  protected Cache(final long nativeHandle) {
    super(nativeHandle);
  }
}
</code></pre>
<ul>
<li>Copyright 是作者或者创建者因为其原创性的工作，所拥有的复制，分发，出售以及其他一系列的排他性权利，如上所示代码声明了“这段代码的版权归属于 Facebook”，拥有一切版权保护的相关权利。</li>
</ul>
<h4 id="copyleftɔ">Copyleft(Ɔ)</h4>
<ul>
<li>其实还有个东西叫 <strong>Copyleft(Ɔ)</strong>，
<ul>
<li>“Copyleft”最初是为反对商业软件而生，但它并不是放弃版权。反对软件一切权利归作者私有，保护知识共享、权利共享。</li>
<li>软件的版权归原作者所有，其它一切权利归任何人所有。用户和软件的作者享有除版权外的完全同等的权利，包括复制软件和重新发布修改过的软件的权利。</li>
<li>自由软件在承认著作权的基础上，可以通过许可协议，与公众共享作品的其它权利</li>
</ul>
</li>
</ul>
<blockquote>
<p>百度百科：著佐权（Copyleft）是一个由自由软件运动所发展的概念，是一种利用现有著作权体制来保护所有用户和二次开发者的自由的授权方式。在自由软件授权方式中增加著佐权条款之后，该自由软件除了允许使用者自由使用、散布、修改之外，著佐权许可证更要求使用者修改后的衍生作品必须要以同等的授权方式（除非许可证或者版权声明里面例外条款所规定的外）释出以回馈社会。</p>
</blockquote>
<ul>
<li>所以正是因为 <strong>Copyleft(Ɔ)</strong> 的思想，才逐渐衍生出后来的 <strong>License</strong>。可以简单理解为 <code>Copyleft = Copyright+GPL</code></li>
</ul>
<h4 id="license">License</h4>
<ul>
<li>版权法默认禁止共享，也就是说，没有许可证的软件，就等同于保留版权，虽然开源了，用户只能看看源码，不能用，一用就会侵犯版权。所以软件开源的话，必须明确地授予用户开源许可证。</li>
<li>License 是 Copyright 拥有者授予其他人处置其原创性成果的权利，如上代码版权声明所示，“Facebook 授予了这段代码 GPLv2 和 Apache2.0 的许可”。</li>
<li>开放源码许可证是符合开放源码定义的许可证——简而言之，它们允许“<strong>自由</strong>”地使用、修改和共享软件。这里的自由其实是相对的，相应地需要遵守对应 License 下的规定。</li>
<li>软件许可是告诉其他人，他们能够对您的代码做什么，不能做什么。</li>
<li>大多数人将其许可文件放在仓库根目录的文件 <code>LICENSE.txt</code>（或 <code>LICENSE.md</code>）中，如 RocksDB 中的 COPYING 和 LICENSE.Apache。</li>
<li>GPLv2 COPYING</li>
</ul>
<pre><code class="language-txt">                    GNU GENERAL PUBLIC LICENSE
                       Version 2, June 1991

 Copyright (C) 1989, 1991 Free Software Foundation, Inc.,
 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.
 ...
</code></pre>
<ul>
<li>Apache2.0 LICENSE.Apache</li>
</ul>
<pre><code class="language-txt">

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.
   ...
</code></pre>
<ul>
<li>上面其实介绍的都是开源 license，但其实在软件市场中还是有大量的<strong>商业 license</strong> 的，毕竟也是要恰饭的嘛。主要就是一些商业软件的使用，常常需要大家购买对应软件的 license 才能使用，而且很多 license 大多都是有时间期限的，例如 IDEA 可能还有一些 license server 的机制，但总体思想都是通过销售 license 来获取盈利。
<ul>
<li>BTW，很多盗版软件其实就是尝试着去碰撞出一个可能有效的 license，甚至有人共享对应的 license 来进行多用户使用。</li>
</ul>
</li>
</ul>
<h3 id="有哪些开源许可">有哪些开源许可</h3>
<ul>
<li>https://www.gnu.org/licenses/license-list.zh-cn.html</li>
<li>几种常见的开源许可：
<ul>
<li><a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GPL（GNU General Public License）</a>：GNU 通用公共许可协议，免费使用、引用、修改代码，但不能用在闭源软件中发布及销售。“传染性” 表示如果一个软件使用了 GPL 协议的开源代码，那么这个软件也必须开源，仍然免费使用。不能用于商业产品。</li>
<li><a href="https://www.gnu.org/licenses/lgpl-3.0.html">LGPL（GNU Lesser General Public License）</a>：宽松GPL，规定：如果A项目采用LGPL许可证，那么基于A开发出来的B项目也必须采用LGPL，即必须也开源，但是，如果B项目不是基于A开发出来的，而仅仅调用了A的接口，那么B项目可不必开源，倘若换做GPL的话，那么B项目也是要开源的（所以叫做宽松的GPL）。</li>
<li><a href="https://en.wikipedia.org/wiki/BSD_licenses">BSD License（original BSD license、FreeBSD license、Original BSD license）</a>：伯克利软件套装，规定：如果A项目采用BSD许可证，那么基于A开发出来的B项目可以选择闭源，即私有化、商业化，但是必须注明B项目采用了A这个开源项目。<strong>主要限制在于不能用开源代码的作者或机构进行商品推广。</strong></li>
<li><a href="https://en.wikipedia.org/wiki/MIT_License">MIT(The MIT License)</a>：麻省理工学院许可证，规定：这是一个自由度很高的开源许可证，几乎同意了可以随意使用一个开源项目（使用、复制、修改、合并、出版发行、散布、再授权、贩售软件及软件的副本），只要在你的项目中包含或提及原开源项目的MIT许可证。<strong>至于你会不会通过它进行商品推广，作者并不关心，只想保留版权。</strong></li>
<li><a href="https://www.apache.org/licenses/">Apache Licence</a>：Apache软件基金会，规定：大致上和BSD许可证类似，只是有一点细微差别，它除了需要注明B项目源于开源项目A，也要在每个修改过的A项目的文件注明此文件已被修改，并且原文件是A开源项目中的哪个文件。<strong>相对于 MIT，如果修改了源代码，需要进行说明</strong>。</li>
</ul>
</li>
<li>不推荐用于商业产品的协议：GPL (eg. Linux), LGPL, MPL</li>
<li>适用于商业产品的协议：BSD, MIT, Apache (eg. RocksDB)</li>
<li><strong>Dual-Licensed</strong>: 但是我们在如上的 RocksDB 中的例子观察到，RocksDB 中包含了两个开源许可，一个是不推荐用于商业产品的 GPLv2，一个是推荐用于商业产品的 Apache2.0，而在 RocksDB 关于 License 的介绍中我们发现本身该项目就是基于两个开源协议的，而其他软件开发者可以根据自己的实际需求来决定使用哪一个 License。</li>
</ul>
<h4 id="other">Other</h4>
<ul>
<li>可能还会有小伙伴看过这样的版权例子，但大多都是一些知识产品，比如博客、slides、文档以及网页等等。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206230938.png" alt="20210206230938" loading="lazy"></li>
<li><strong>CC License</strong>: Creative Commons license，简称CC许可，是一种公共版权许可协议，知识共享许可协议，其允许分发受版权保护的作品。一个创作共用许可，用于一个作者想给他人分享、使用、甚至创作派生作品的权利。创作共用提供给作者灵活性（例如，他们可以选择允许非商业用途使用他们的作品），保护使用或重新分配他人作品的人，所以他们只要遵守由作者指定的条件，不必担心侵犯版权。</li>
</ul>
<blockquote>
<p>百度百科：知识共享（Creative Commons，简称CC，台湾译名创用CC）是一个非营利组织，也用是一种创作的授权方式。此组织的主要宗旨是增加创意作品的流通可及性，作为其它人据以创作及共享的基础，并寻找适当的法律以确保上述理念。</p>
</blockquote>
<ul>
<li><strong>CC-BY-NC-SA</strong> 本质是几种权利的组合：
<ul>
<li><strong>CC</strong>：创作共用</li>
<li><strong>BY</strong>：署名：您（用户）可以复制、发行、展览、表演、放映、广播或通过信息网络传播本作品；您必须按照作者或者许可人指定的方式对作品进行署名。</li>
<li><strong>NC</strong>：非商业性使用（英语：Noncommercial，nc）您可以自由复制、散布、展示及演出本作品；您不得为商业目的而使用本作品。</li>
<li><strong>SA</strong>：相同方式共享（英语：ShareAlike，sa）您可以自由复制、散布、展示及演出本作品；若您改变、转变或更改本作品，仅在遵守与本作品相同的许可条款下，您才能散布由本作品产生的派生作品。（参见copyleft。）</li>
</ul>
</li>
<li>除此以外还包含一种权利：
<ul>
<li><strong>ND</strong>：禁止演绎（英语：No Derivative Works，nd)，您可以自由复制、散布、展示及演出本作品；您不得改变、转变或更改本作品。</li>
</ul>
</li>
</ul>
<h3 id="怎么选开源许可">怎么选开源许可</h3>
<ul>
<li>如何选 License: https://www.gnu.org/licenses/license-recommendations.html</li>
<li>千言万语一大堆，不如一张图。图源阮一峰博客 http://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206213620.png" alt="20210206213620" loading="lazy"></li>
<li>https://choosealicense.com/<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206220305.png" alt="20210206220305" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous Merging]]></title>
        <id>https://blog.shunzi.tech/post/Dostoevsky/</id>
        <link href="https://blog.shunzi.tech/post/Dostoevsky/">
        </link>
        <updated>2021-01-12T14:50:15.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based<br>
Key-Value Stores via Adaptive Removal of Superfluous Merging</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based<br>
Key-Value Stores via Adaptive Removal of Superfluous Merging</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>无论是学术界还是工业界，所有主流的基于 LSM 树的键值存储都在更新的 I/O 成本和查询和存储空间的 I/O 成本之间进行了权衡。因为在所有的 LSM Tree 的级别上都需要执行 Compaction 操作来限制查询遍历的 runs，并删除 obsolete 的数据项来腾出存储空间。即便是最先进的 LSM Tree 设计，来自 LSM Tree 所有层此的合并操作（除了最大的层此）减少的点查询成本、大范围查询成本和存储空间，减少的效果可以忽略不计；与此同时还增加了更新操作的平摊开销。</li>
<li>为了解决这个问题，我们提出了 Lazy Leveling，一种新的设计，从除开最大层以外的所有 level 中删除合并操作。同时提出了 Fluid LSM-tree，一种可以涵盖整个 LSM-tree 设计领域的通用设计，可以参数化以假设任何现有的设计。相对于 Lazy level, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。</li>
<li>Dostoevsky，一种键值存储，通过基于应用程序工作负载和硬件来动态调整的弹性的 LSM-tree 设计，自适应地消除多余的合并。基于 RocksDB 实现，测试表明无论是性能还是存储空间方面都优于目前最先进的设计。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>LSM-tree 将要插入/更新的条目缓冲在内存中，并在缓冲区填满时将缓冲区作为 sorted run 刷新到次要存储。LSM-tree 稍后对这些 runs 进行排序合并，以限制查找必须扫描的run 数量，并删除过时的条目。LSM-tree 将运行组织成指数级增长的容量，更大的级别包含更老的运行。当条目被替换更新时，点查找通过从最小到最大的级别查找条目的最新版本，并在查找目标键时终止查找。另一方面，范围查找必须在所有级别的所有 run 中访问相关的键范围，并从结果集中删除过时的条目。为了提高单个 run 的查询速度，设计中常常包含了两个额外的内存中的数据结构。首先，对于每个 run，都有一组包含每个 run 块的第一个键的 fence 指针，这允许查找在一个 run 中只使用一个 I/O 就可以访问特定的键；第二，每个 run 会有一个 BoolmFilter，这允许点查询跳过不包含目标键的 runs。这个设计被应用到了大量的现代 KV 存储中，如 LevelDB、BigTable 等。</li>
<li><strong>问题</strong>：LSM-tree 中的合并操作的频率控制了在 更新的 I/O 成本 和 查询和存储空间放大的 I/O 成本之间的 trade-off，另外的问题就是现有的设计在这些指标之间的 trade-off 并不理想。下图表示了指标之间的权衡关系，虽然这些 y 轴指标具有不同的单位，但它们相对于 x 轴的权衡曲线具有相同的形状。两个极端分别是 log 和 sorted array。LSM-tree在完全不合并或尽可能多地合并时，分别退化为这些边缘点。我们将主流系统放置在这些边缘点之间的顶部曲线上，基于它们的默认合并频率，我们为 Monkey 绘制了一个优越的权衡曲线，我们证明了存在一个甚至比Monkey更好的权衡曲线。现有的设计放弃了大量的性能和/或存储空间，因为没有沿着这条底部曲线设计。</li>
<li><strong>问题来源</strong>：通过分析最先进的 LSM 树的设计空间，我们指出了问题的根源，即最坏情况下的更新代价、点查询代价、范围查询代价和空间放大在不同的层次上产生不同的结果。
<ul>
<li>Update: 更新的 I/O 成本稍后通过更新条目参与的合并操作来分担。虽然较大级别的合并操作需要成倍地增加工作，但它们发生的频率却成倍地减少。因此，更新从所有级别的合并操作中同等地获得它们的 I/O 成本。</li>
<li>Point lookups：虽然 图1 中沿顶部曲线的主流设计将跨 LSM-tree 所有级别的 Bloom flters 假阳性率设置为相同，但目前最先进的 Monkey 为更小的层此设置更低的假阳性率。他被证明可以最小化所有筛选器的误报率之和，从而最小化点查找的 I/O。与此同时，这意味着进入较小层此的可能性呈指数级下降，因此大多数点查询 I/O 将直接命中最大的层次。</li>
<li>Long range lookup：因为 LSM Tree 的容量呈指数级增长，最大曾通常包含绝大部分数据，所以该层更可能包含给定键范围内的数据，因此大多数由大范围查询引起的 I/O 都将对最大层进行操作。</li>
<li>Short range lookup： 使用极小键范围的范围查找在每次 run 中只能访问大约一个块，而不管 run 的大小，因为每一层 run 的最大个数是固定的，因此小范围查询在所有曾中是相当的。</li>
<li>Space-Amplifcation：空间放大最差的情况就是较低层次的数据被更新到最大层此，因此在最大层中老旧的数据项比例最大。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210112173749.png" alt="20210112173749" loading="lazy"></li>
</ul>
</li>
<li>因为最坏情况下的点查询开销、大范围查询开销、空间放大都主要来源于最大层，LSM-tree 中所有级别的合并操作，除开最大层(即大多数合并操作)在这些指标上几乎没有改进，同时显著增加了更新的平摊成本。这导致了次优的权衡。我们用三个步骤从头开始解决这个问题：
<ul>
<li><strong>Solution 1: Lazy Leveling to Remove Superﬂuous Merging</strong>：
<ul>
<li>我们使用 Lazy level 拓展了 LSM-tree 设计思路，这种新设计除去了 LSM-tree 最大级别之外的所有合并。Lazy Leveling 改进了最坏情况下更新的成本复杂性，同时在点查找成本、大范围查找成本和空间放大上保持相同的限制，同时在小范围查找成本上提供具有竞争力的限制。我们证明改进的更新开销可以用来降低点查找开销和空间放大。这生成了 图1 中的底部曲线，它提供了更丰富的时空权衡，这是迄今为止最先进的设计无法实现的。</li>
</ul>
</li>
<li><strong>Solution 2: Fluid LSM-Tree for Design Space Fluidity.</strong>
<ul>
<li>我们引入了 Fluid LSM-tree 作为新一代的 LSM Tree 支持在整个 LSM-tree 设计思路中流畅地切换。Fluid LSM-tree 通过分别控制最大级别和所有其他级别合并操作的频率，相对于 Lazy leveling, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。</li>
</ul>
</li>
<li><strong>Solution 3: Dostoevsky to Navigate the Design Space</strong>
<ul>
<li>Dostoevsky: Space-Time Optimized Evolvable Scalable Key-Value Store。Dostoevsky 分析地找到了Fluid LSM-tree 的调优方法，以最大限度地提高特定应用程序工作负载和硬件在空间放大方面的用户约束，通过精简搜索空间来快速找到最佳调优，并在运行时对其进行物理调整。因为 Dostoevsky 跨越了所有现有的设计，并能够针对给定的应用程序 navigate 到最佳的设计，因此它在性能和空间放大方面严格控制了现有的键值存储。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="background">BACKGROUND</h2>
<ul>
<li>如下图所示，为了优化写操作，LSM-Tree 初始时缓冲了所有的更新、插入和删除操作到内存中，当 Buffer 满了之后，LSM Tree 将 Buffer 以 Sorted Run 刷回到了第二层存储，LSM Tree 归并排序 runs 是为了：限制查询操作必须访问的 runs 的数量，以及删除老旧的数据项以回收空间。runs 被组织成 L 个呈指数级增长的层此，Level 0 是主存中的 Buffer，其他层次都位于二级存储。</li>
<li>在合并的 I/O 开销和查询 I/O 开销以及空间放大之前的权衡可以由两个参数来控制，第一个是相邻两个层次之间的比例 T，T 控制了层级的个数因此决定了一个数据能够在层级之间合并多少次。第二个参数是合并策略，决定了数据项在一个 level 内的合并次数。所有的现有设计都使用了 tiering 或 leveling 两种策略。
<ul>
<li>tiering：当一个 level 到达容量时合并该 level 内的 runs</li>
<li>leveling: 当一个新的 run 出现，就会在 level 中执行合并</li>
</ul>
</li>
<li>如下图所示，size ratio 为 4，buffer 大小为一个 entry 的大小。在两种策略中，当 buffer flushing 造成 Level 1 到达容量时触发合并操作。对于 tiering，Level 1 的所有 runs 都被合并成同一个新的 run 放置在 Level 2。而对于 Leveling，合并操作还会包含 Level2 原有的 run。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210114224107.png" alt="20210114224107" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210115172909.png" alt="20210115172909" loading="lazy"></li>
<li><strong>Number of Levels</strong>：Level 0 拥有的数据项个数 <em>B * P</em>。$$L = [log_T(\frac{N}{B<em>P}</em>\frac{T-1}{T})]$$。层级之间的大小比例 T 被限制到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>≤</mo><mi>T</mi><mo>≤</mo><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">2 ≤ T ≤ T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 被定义成 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>N</mi><mrow><mi>B</mi><mo>∗</mo><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{N}{B*P}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，当 size ratio 达到上界时，levels 的数量减少到接近 1，超过上界之后将无结构性的变化。大于下界就表明 第 i 级合并操作的结果运行永远不会大到超过第 i + 1 级。换句话说就是确保了 runs 不会跨多个 levels。</li>
<li><strong>Finding Entries</strong>：因为数据项是异地更新，相同 key 的数据的多个版本可能出现在多个 level 中，甚至对于 tiering 策略可能存在于一个 level 的多个 runs 中，为了确保查询操作总是能够找到最新版本的数据，LSM Tree 采用了如下措施：1. 当数据项被插入到 buffer 中且 buffer 中包含相同 key 对应的数据时，新的数据项将代替老的数据；2. 当两个包含相同 key 的数据项的 runs 被合并的时候，只有最新版本的数据将被保留；3. 为了能够获取到来自不同 runs 的相同 key 的不同数据项的插入顺序，一个单独的 run 只能够和相邻时间的 run 进行合并。从而保证当有两个 runs 包含不同版本的相同 key 对应的数据时，younger run 包含的是最新版本的数据。</li>
<li><strong>Point Lookups</strong>：点查询通过从最小到最大层次进行遍历来查询最新版本的数据，对于 tiering 则是在一个 level 中从最新到最老的 runs 中遍历进行查询。当找到一个匹配当前 key 的数据时则终止。</li>
<li><strong>Range Lookups</strong>：范围查询需要查找指定范围的键对应的所有最新的数据，通过对所有 levels 所有 runs 的相关键范围进行排序合并。当 sort-merging 时，识别出来自不同 runs 具有相同 key 的数据，然后丢弃掉老版本的数据。</li>
<li><strong>Deletes</strong>：通过给每个数据项添加一位 flag 来实现。如果查询操作找到了该数据想的最新版本，且该数据项上有该 flag 那么将不会返回对应的 value 给应用。当一个删除的数据项和 最老的 run 合并的时候，该数据将被删除，因为该数据项已经代替了之前所有插入的具有当前 key 的数据。</li>
<li><strong>Fragmented Merging</strong>：为了换接较大级别上由于长时间合并操作而导致的性能下降，主流设计把 runs 分区成了文件，也叫 Sorted String Tables，然后一次合并一个 SSTable 和下一个 older run 中具有重叠键范围的多个 SSTables，该技术不会影响最坏情况下的合并 I/O 开销，而只会影响这种开销如何调度。在整篇文章中，为了便于阅读，我们将合并操作讨论为具有 runs 的粒度，尽管它们也可以具有 sstables 的粒度。</li>
<li><strong>Space-Amplifcation</strong>：过时条目的存在使存储空间增大的因素称为空间放大。由于磁盘的可承受性，空间放大传统上并不是数据结构设计的主要关注点。然而，SSD 的出现使空间放大成为一个重要的成本问题。我们将空间放大作为成本指标，以提供我们所引入和评估的设计的完整描述。</li>
<li><strong>Fence Pointers</strong>：所有主要的基于 LSM 树的键值存储都在主存中对每次运行的每个块的第一个键建立索引，也就是图 2 所示的 fence pointer，通常这些指针占据内存空间大小为 O(N/B)，但是让查询操作中找到每个 runs 的 key 范围变成了只需要一次 I/O。</li>
<li><strong>Bloom Filters</strong>：为了加速点查找，只需要在主存中为每个 run 维护一个 BloomFilter，点查找在访问存储中相应的 runs 之前首先检查 Bloom flter。如果 filter 返回 true positive，那么查询操作配合 fence pointer 只需要一次 I/O 就能访问对应的 run，从而找到对应的数据项并终止。如果返回 negative，那么将跳过该 run 并节省一次 I/O 操作。但还有 false positive 的情况，浪费一次 I/O 然后再去下一个 run 继续查找该 key。</li>
<li>Bloom flter 有一个有用的特性，如果它被分割成较小的等大小的 Bloom flter，其中的条目也被等分，每一个新的分区布隆滤片的 FPR 渐近与原滤片的 FPR 相同(虽然实际略高)。为了便于讨论，我们将Bloom flters称为非分区的，尽管它们也可以按照工业中的某些设计进行分区（比如每个 run 的每个 block），从而为空间管理提供更大的灵活性。(例如，对于那些不经常被点查询读取的块，可以将其 offload 到存储器中以节省内存)</li>
<li><strong>Applicability Beyond Key-Value Stores</strong>：根据工业上的设计，我们的讨论假设一个键在运行过程中与它的值相邻存储。为了便于阅读，本文中的所有图形都将条目描述为键，但它们表示键-值对。我们的工作也适用于没有 value 的应用程序(例如，LSM-tree 被用来回答关于键的集合成员查询)，其中的值是指向存储在 LSM-tree 之外的数据对象的指针，或者 LSM-tree 被用作解决更复杂算法问题的构建块(例如，图分析)， FTL 设计等)。我们将分析的范围限制在基本操作和 LSM-tree 的大小上，以便它可以很容易地应用于这些其他情况。</li>
</ul>
<h2 id="design-space-and-problem-analysis">DESIGN SPACE AND PROBLEM ANALYSIS</h2>
<ul>
<li>现在，我们分析更新和查找的最差情况下的空间放大和 I/O 成本是如何从不同的级别派生出与合并策略和大小比例相关的。为了分析更新和查找，我们使用磁盘访问模型来计算每个操作的 I/O 数量，其中 I/O 是从二级存储读取或写入一个块。</li>
<li>分析结果如下所示：
<ul>
<li><strong>Updates</strong>：更新成本通常都是由更新条目参与的后续合并操作产生的，分析假设最坏情况的工作负载，其中所有更新的目标条目都在最大级别。这意味着一个过时的条目不会被删除，直到它相应的更新的条目达到最大级别。因此，每个条目都会在所有级别上合并(即，而不是在某个更小的级别上被最近的条目丢弃，从而减少以后合并操作的开销)。
<ul>
<li>tiering：每层合并 O(1) 次，每个合并过程中的 I/O 操作从原始的 run 中拷贝 B 个数据项到新的 run，因此每个数据项平均的更新操作成本开销如图所示。填满 level i，需要 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">B · P · T^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 次更新，导致合并操作拷贝 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">B · P · T^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 个数据项。</li>
<li>leveling：到达 level i 的第 j 个 run 触发了一个合并操作，合并操作包括 level i 现有的 runs，这些 runs 是自上次 level i 为空以来到达的前 T−j 个 runs 的合并操作产生的。因此平均每个数据项在该层数据到达容量之前合并了 T/2 次，可以表示为 O(T)，同样需要除以一个块对应的 B 个数据项。每 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">B · P · T^{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 次更新（每次有一个新的 run come in）之后执行一次合并操作，然后拷贝平均 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{B · P · T^i}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3704599999999998em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0254599999999998em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mpunct mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9020857142857143em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 项数据，通过将复制的条目数除以级别 i 的合并操作的频率，<strong>我们观察到，在长期运行中，每个级别上的合并操作所做的工作量是相同的，直觉是，虽然合并操作在更大的级别上以指数方式完成更多的工作，但它们的频率也以指数方式降低</strong></li>
</ul>
</li>
<li><strong>Analyzing Point Lookups</strong>：为了分析最坏情况下的点查找代价，我们将重点放在 zero-result 点查找（例如查询不存在的 Key）上，因为它们最大化了浪费的 I/O 的平均值。这种分析对于插入前判断是否存在的操作就很有用。开销最大的情况即为所有的 BloomFilter 返回 false positive，此时点查询操作会对每一个 run 发起一次 I/O，对于 leveling 浪费的 I/O 为 O(L)，对于 tiering 浪费的 I/O 为 O(T · L) 。但实际上，Bloom flters 对于不存在的 key 能节省很大一部分 I/O，在工业中，键值存储对每一个Bloom flters使用 10 位，这会导致误报率(FPR)为每个过滤器约为 1%，出于这个原因，我们将重点放在预期的最坏情况点查找成本上，它将点查找发出的 I/O 数量作为关于 Bloom flters FPRs 的长期平均值进行估计。我们估计这个成本为所有Bloom flters的FPRs之和。原因是，查询单个 run 的 I/O 成本是一个独立的随机变量，其期望值等于相应的 Bloom flter 的 FPR，多个独立随机变量的期望值之和等于它们各自的期望值之和。在工业界的键值存储中，所有级别的 BloomFilter 的每个条目的比特数是相同的。因此，最大级别的 Bloom flter(s) 比所有较小级别的 filter 的总和要大，因为它们以指数形式表示更多的条目。根据公式 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mo>(</mo><mi>b</mi><mi>i</mi><mi>t</mi><mi>s</mi><mi mathvariant="normal">/</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>s</mi><mo>)</mo><mo separator="true">⋅</mo><mi>l</mi><mi>n</mi><mo>(</mo><mn>2</mn><msup><mo>)</mo><mn>2</mn></msup></mrow></msup></mrow><annotation encoding="application/x-tex">FPR = e^{−(bits/entries)·ln(2)^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9869199999999998em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869199999999998em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">s</span><span class="mord mtight">/</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">s</span><span class="mclose mtight">)</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">n</span><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，最大层的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><msub><mi>R</mi><mrow><mi>p</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">FPR_{pL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 上界被限制在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，所以对于 leveling，即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span>，对于 tiering，即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo separator="true">⋅</mo><mi>T</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L · T )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span>.
<ul>
<li>关于这个问题的最新论文 Monkey 表明，为所有级别的过滤器设置相同的每个条目的比特数并不能最小化浪费的I/O 的预期数量。相反，Monkey 在最大级别上对 filter 中的每个条目重新分配≈1比特，它使用这些比特来设置较小级别上每个条目的比特数，作为不断增加的等差数列：即 Level i 的每个数据项为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo separator="true">⋅</mo><mo>(</mo><mi>L</mi><mo>−</mo><mi>i</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">a + b · (L - i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span>，a 和 b 都是比较小的常数，这导致 FPR 在最大水平上有一个小的、渐近恒定的增加，在较小的水平上有一个指数下降，因为它们包含较少的条目。由于 FPRs 在较小的级别是指数递减的，所以 FPRs 的总和收敛于一个与级别数无关的乘法常数。Monkey 从点查找的复杂性中去掉了一个 L 的因素，这种复杂性导致了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span> I/O (level)和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span> I/O (tiering)，如图3 (B)所示。对于 zero and non-zero result 的结果点查找以及任何类型的偏差，使用 Monkey 总是有益的。</li>
<li>总的来说，我们观察到使用 Monkey 的<strong>点查找成本主要来自于最大的 level，因为较小的 level 的 FPRs 呈指数级下降，所以访问它们的可能性也呈指数级下降</strong>。</li>
</ul>
</li>
<li><strong>Analyzing Range Lookups</strong>：我们将范围查找的 selectivity 表示为在目标键范围内的所有 run 的唯一条目的数量。范围查找在所有 runs 中扫描和排序合并目标键范围，并从结果集中删除过时的条目。范围查询扫描并排序合并所有 runs 的目标键范围，从结果集中消除老数据。为了分析，如果访问的块数至少是可能的最大级别数的两倍，那么就认为范围查询的范围很大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>s</mi><mi>B</mi></mfrac><mo>&gt;</mo><mn>2</mn><mo separator="true">⋅</mo><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{s}{B} &gt; 2 · L_{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，在均匀随机分布的更新下，这个条件意味着在目标键范围内的大多数条目都有很高的概率处于最大级别。
<ul>
<li>小范围查询对每个 run 发起近一个 I/O，叠加起来就是 leveling o(L)，tiering 就是 O(L·T)。对于长范围查询，在消除过时条目之前的结果集的大小平均是其 selectivity 和空间放大的乘积。我们用这个乘积除以块大小来得到 I/O 成本，tiering 即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mfrac><mrow><mi>T</mi><mo separator="true">⋅</mo><mi>s</mi></mrow><mi>B</mi></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\frac{T·s}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>，leveling 为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mfrac><mi>s</mi><mi>B</mi></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\frac{s}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.095em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></li>
<li><strong>一个关键的区别是，短范围查找从所有级别获得的开销大致相同，而长范围查找的大部分开销来自访问最大级别</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210116202849.png" alt="20210116202849" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><strong>Analyzing Space-Amplifcation</strong>：我们将空间放大定义为条目总数 N 除以唯一条目数 unq，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>=</mo><mfrac><mi>N</mi><mrow><mi>u</mi><mi>n</mi><mi>q</mi></mrow></mfrac><mi mathvariant="normal">−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">amp = \frac{N}{unq} − 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3534389999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">−</span><span class="mord">1</span></span></span></span>。为了分析最坏情况的空间放大，我们观察到 LSM-tree 的 1 到 L−1 级包含其容量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的一部分，而 L 级包含其容量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>T</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{T−1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的剩余部分。使用 leveing，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，从而导致 Level L 的最多有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 是过时的。空间放大因此是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>T</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(1/T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span>。对于 tiering，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，且 Level L 的每个 run 包含相同的数据项集时，Level L 完全由过时的条目组成，所以空间放大是 O(T)，因为 Level L 比所有其他 Level 加起来要大 T−1 倍。<strong>总的来说，在最坏的情况下，带有 leveling 和 tiering 的空间放大主要是由于在最大级别上存在过时的条目</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118112648.png" alt="20210118112648" loading="lazy"></li>
<li><strong>Mapping the Design Space to the Trade-Oﬀ Space</strong>：更新成本与查找和空间放大成本之间存在一种内在的权衡。如下图实线绘制了在y轴上查找和空间放大的不同成本，以及在x轴上更新的成本(当我们改变大小比例时)。当大小比例设置为其限制值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>(意味着存储中只有一个级别)时，tiered 的 LSM-tree 退化为日志，而 leveled 的 LSM-tree 退化为排序的数组。当尺寸比设置为其下限2时，随着 level 和 tiering 的行为趋于一致，性能特征逐渐收敛：级别的数量是相同的，当第二个 run come in 时，每个级别都会触发合并操作。一般来说，随着 leveling/tiering 大小比例的增加，查找成本和空间放大相应 减少/增加，更新成本相应 增加/减少。因此，对权衡空间进行了分区:与分层相比，level 相比于 tiering 具有更好的查找成本和空间放大，更糟糕的更新成本。</li>
<li><strong>The Holy Grail</strong>：图5中的实线反映了Monkey的属性，即当前的最先进的设计。图5 还显示了标记为“难以捉摸的最佳”的虚线。指导我们研究的问题是，其他设计是否可能通过时空权衡更接近甚至达到难以捉摸的最佳设计。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118143538.png" alt="20210118143538" loading="lazy"></li>
<li><strong>The Opportunity: Removing Superﬂuous Merging</strong>：我们已经确定了<strong>不对称性:点查找成本、长范围查找成本和空间放大主要来自最大的级别，而更新成本来自所有级别</strong>。这意味着在更小的级别上合并操作显著地放大了更新成本，同时为空间放大、点查找和远程查找带来的好处相对较小。因此，有一个合并策略的启发，在较小的层次上合并较少次数。</li>
</ul>
<h2 id="lazy-leveling-fluid-lsm-tree-and-dostoevsky">LAZY LEVELING, FLUID LSM-TREE, AND DOSTOEVSKY</h2>
<h3 id="lazy-leveling">Lazy Leveling</h3>
<ul>
<li>Lazy Leveling 一种合并策略，除了LSM-tree的最大级别之外，它完全消除了合并。其动机是，在这些更小的级别上合并会显著增加更新成本，同时对点查找、远程查找和空间放大产生的改进相对较小。相比于 Leveling，Lazy Leveling：
<ul>
<li>improves the cost complexity of updates</li>
<li>maintains the same complexity for point lookups, long range lookups, and space-amplifcation</li>
<li>provides competitive performance for short range lookups.<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118160921.png" alt="20210118160921" loading="lazy"></li>
</ul>
</li>
<li><strong>Basic Structure</strong>：Lazy Leveling 结构如下所示，其核心类似于缓和 tiering 和 leveling 两种结构，它在最大 level 上应用 leveling，在所有其他 level 上应用 tiering。结果，最大 level 的 runs 数量为 1，其他 level 的 runs 数量最多为 T−1 (即，合并操作在第 T 个 run 到达时发生)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118161152.png" alt="20210118161152" loading="lazy"></li>
<li><strong>Bloom Filters Allocation</strong>：如何保持点查找的成本复杂性不变，尽管有更多的 rims 在较小的级别上被检索。我们通过优化不同级别之间的 BloomFilter 内存预算来做到这一点。我们开始建模点查找成本和 filter 的总体内存占用与 FPRs 有关。最坏情况下，每次查找的预期浪费I/O 数由零结果点查询造成，等于每次运行的 Bloom flters 的误阳性率之和。</li>
</ul>
<h3 id="fluid-lsm-tree">Fluid LSM-Tree</h3>
<h2 id="references">References</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/129355502">[1] 知乎 - 叶提：SIGMOD'18|Dostoevsky</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost]]></title>
        <id>https://blog.shunzi.tech/post/CRaft/</id>
        <link href="https://blog.shunzi.tech/post/CRaft/">
        </link>
        <updated>2020-12-16T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>一致性协议主要是在分布式系统中用于保证可靠性和可用性的，现有的一致性协议大多都是要将日志项给备份到所有的服务器中，这种全量的副本的策略在存储和网络上的开销都很大，严重影响性能，所以后来出现了纠删码，即在保证相同的容错能力的条件下减少存储和网络的开销。</li>
<li>RS-Paxos 是第一个支持 EC 数据的一致性协议，但是比起通用的一致性协议，如 Paxos/Raft，可用性都相对更差。我们指出了RSPaxos的活性问题，并试图解决，基于 Raft 提出了 CRaft，既能使用 EC 码像 RS-Paxos 一样降低存储和网络开销，也能保证如 Raft 一样的 liveness。</li>
<li>基于 CRaft 实现了一个 KVs，实验表明相对于 Raft 节省了 66% 的存储空间，写吞吐量提升了 250%，写延迟减少了 60.8%</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>共识算法介绍</strong>：共识协议协议通常保证安全性和活动性，这意味着它们总是返回正确的结果，并且在大多数服务器都没有发生故障的情况下可以完全正常工作。
<ul>
<li>Google’s Chubby 会使用 Paxos 对 metadata 做副本</li>
<li>Gaios(NSDI2011) 表明一致性协议可以被用于所有数据的 replicated</li>
<li>现如今大量应用如 etcd, TinyKV, FSS 等大规模系统都使用了 Raft/Paxos 来 replicated TB 数量级的数据，并提供更好的可用性</li>
</ul>
</li>
<li><strong>多副本介绍</strong>：数据操作通常在分布式系统中被转换为一系列的日志指令，然后使用一致性协议在所有的服务器之间进行备份，所以数据需要经过网络传输到所有的服务器，然后还要刷会到磁盘持久化保存。一致性问题中，容错率如果为 F，那么则至少需要 N = (2F + 1) 的服务器，否则就可能因为分组的原因出现不一致的情况。因此传统的副本策略往往就意味值原始数据量的 N 倍的网络和存储开销，而且随着这些协议在大规模存储系统中得到了越来越多的应用，N 倍的网络和存储开销带来的则是延迟的增加和吞吐量的下降。<strong>所以出现了 Erasure Coding</strong></li>
<li><strong>纠删码介绍</strong>：纠删码相比于全量拷贝的副本策略，极大地减小了存储和网络的开销。通过将数据进行分片，编码分片后的数据并生成一些校验的分片，原始的数据就能从足够数量的分片子集中恢复出来，这时候每个服务器只存储一个分片，而不是数据的全量拷贝，开销极大减小。FSS 中就使用了纠删码来减少存储开销，但是 FSS 在编码之前使用了一个 5 way 流水线 Paxos 来备份完整的用户数据和元数据，因此额外的网络开销还是有 4 倍数据量大小。</li>
<li><strong>RS-Paxos</strong> 是第一个结合了 Paxos 和 EC 的共识协议，虽然减少了存储和网络的开销，但是在可用性上比 Paxos 还是更差，RA-Paxos 牺牲了 liveness 来使用 EC 提升性能，换句话说就是 RS-Paoxs 如果有 N = (2F + 1) 的服务器不再能容忍 F 个错误，即容错率下降了，主要是因为 RS-Paxos 中的提交要求越来越严格。</li>
<li>作者提出了 erasure-coding-supported version of Raft <strong>CRaft</strong> (Coded Raft)。该方案中，一个 leader 有两种方法备份日志项到 followers，如果 leader 能够和足够数量的 followers 通信，那么 leader 将使用分片后的日志项进行备份，即传统纠删码的方式，否则将备份完整的数据以保证可用性。相比于 RS-Paxos，CRaft 最大的不同是拥有和 Paxos/Raft 相同级别的 liveness，而 RS-Paxos 没有，但是两个方案都节省了网络和存储的成本。</li>
</ul>
<h2 id="background">Background</h2>
<h3 id="raft">Raft</h3>
<ul>
<li>https://raft.github.io/</li>
<li>Raft 原始论文：https://raft.github.io/raft.pdf</li>
<li>Raft 中主要有三个角色/三种状态。Candidate 收到了来自大多数 servers 的选票后成为 Leader，一个 Server 只会给 和该 Server 日志同步的 Candidate 投票。每个 Server 每一轮最多投一次，所以 Raft 保证每一轮最多就一个 leader。
<ul>
<li>Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader。</li>
<li>Follower: 类似选民，完全被动</li>
<li>Candidate候选人: 类似Proposer，可以被选为一个新的 Leader</li>
</ul>
</li>
<li>leader 从客户端接收日志条目，并试图将它们复制到其他服务器，迫使其他服务器的日志与自己的日志一致。当 leader 发现这一轮中有日志被被分到了大多数 servers，该日志项和之前的日志将被安全地应用到状态机中。Leader 将提交并应用这些日志项，然后告诉 followers 也 apply 他们。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216220445.png" alt="20201216220445" loading="lazy"></li>
<li>用于实际系统的共识协议通常具有以下特性：
<ul>
<li>Safety：它们不会在所有非拜占庭条件下返回错误的结果</li>
<li>Liveness：只要大多数服务器都处于活动状态，并且能够相互通信和与客户端通信，它们就能完全发挥作用。我们称这组服务器是健康的</li>
</ul>
</li>
<li>Raft 中的 Safety 是由 Leader Completeness Property 来保证的， 如果在给定 term 提交了日志条目，那么该条目将出现在所有编号较高的 term 的 leader 日志中。</li>
<li>Liveness 由 Raft 规则保证，通常使用了一致性协议的系统的服务器的数量常常为奇数，假设 N = 2F + 1，Raft 可以容忍 F 个错误，我们定义一个一致性协议可以容忍的失败数量作为 liveness level，所以此时的 liveness level 为 F，更高的 liveness level 意味着更好的 liveness，没有一个协议的 liveness level 可以达到 F+1，因为如果存在这样的协议，则可能存在两个分裂的 F 个健康服务器组，这两个组可以分别就不同的内容达成一致，这是违反安全特性的。</li>
</ul>
<h3 id="erasure-coding">Erasure Coding</h3>
<ul>
<li>擦除编码是存储系统和网络传输中容忍错误的常用技术。们已经提出了大量的编码，其中最常用的是Reed-Solomon (RS)编码。RS 码中有两个可配置的正整数参数 k 和 m，数据被分成了相同大小的 k 个分片，然后使用这 k 个原始的数据分片计算出 m 个类似的校验分片，也就是编码过程，此时总共将有 k+m 个分片，(k,m)-RS 码就意味着所有分片中的任意 k 个分片就能恢复出原始数据，这就是 RS 码的容错原理。（类似于解方程的过程）</li>
<li>当引入一致性协议，k + m = N，N 为服务器的总数量，存储和网络开销将被见效的全拷贝的 1/k，然而如何保证 safety 和 liveness 不容忽视</li>
</ul>
<h3 id="rs-paxos">RS-Paxos</h3>
<ul>
<li>RS-Paxos 是将纠删编码与 Paxos 相结合的一种 Paxos 的改革版本，可以节省存储和网络成本。在 Paxos 中，命令被完全传输。然而，在 RS-Paxos 中，命令是通过代码片段传输的。根据这一变化，服务器在 RS-Paxos 中只能存储和传输片段，从而降低了存储和网络成本。</li>
<li>为了保证安全性和活动性，Paxos和Raft基于以下包容-排斥原则。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>A</mi><mo>∪</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi>A</mi><mi mathvariant="normal">∣</mi><mo>+</mo><mi mathvariant="normal">∣</mi><mi>B</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">−</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>∩</mo><mi>B</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|A∪B| = |A|+|B| −|A∩B|
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord">−</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span></span></span></span></span></p>
<p>包含排除原则保证在两个不同的服务器组合中至少有一个服务器的数量差距，这样安全性就可以得到保证。</p>
<ul>
<li>RS-Paxos 的想法是增加交集集的大小。具体来说，在选择了一个 (k,m)-RS 代码后，读quorum QR、写 quorum QW 和服务器数量 N 应该符合以下公式。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>R</mi></msub><mo>+</mo><msub><mi>Q</mi><mi>W</mi></msub><mi mathvariant="normal">−</mi><mi>N</mi><mo>≥</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">Q_R +Q_W −N ≥ k
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span></span></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alluxio]]></title>
        <id>https://blog.shunzi.tech/post/Alluxio/</id>
        <link href="https://blog.shunzi.tech/post/Alluxio/">
        </link>
        <updated>2020-12-10T02:21:17.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Alluxio 简单介绍，测试报告，然后会结合一些实际体验。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Alluxio 简单介绍，测试报告，然后会结合一些实际体验。</li>
</ul>
</blockquote>
<!-- more -->
<h1 id="alluxio">Alluxio</h1>
<ul>
<li>Alluxio（之前名为 Tachyon），是一个开源的具有内存级速度的虚拟分布式存储系统， 使得应用程序可以以内存级速度与任何存储系统中的数据进行交互。</li>
<li>源码：https://github.com/Alluxio/alluxio</li>
<li>论文：https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-29.pdf</li>
</ul>
<h2 id="架构">架构</h2>
<ul>
<li>文档：https://docs.alluxio.io/os/user/stable/cn/Overview.html</li>
<li>初衷：建立底层存储和大数据计算框架之间的存储系统，为大数据应用提供一个数量级的加速，同时它还提供了通用的数据访问接口。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201210191906.png" alt="20201210191906" loading="lazy"></li>
<li>主要分为两层：UFS 和 Alluxio
<ul>
<li>UFS：底层文件存储，该存储空间代表不受Alluxio管理的空间。
<ul>
<li>UFS存储可能来自外部文件系统，包括如HDFS或S3。 Alluxio可能连接到一个或多个UFS并在一个命名空间中统一呈现这类底层存储。</li>
<li>通常，UFS存储旨在相当长一段时间持久存储大量数据。</li>
</ul>
</li>
<li>Alluxio 存储：
<ul>
<li>Alluxio 做为一个分布式缓存来管理 Alluxio workers 本地存储，包括内存。这个在用户应用程序与各种底层存储之间的快速数据层带来的是显著提高的I/O性能。</li>
<li>Alluxio存储主要用于存储热的、暂时的数据，而不关注长期的持久性。</li>
<li>要管理的每个Alluxio工作节点的存储数量和类型由用户配置决定。</li>
<li><strong>即使数据当前不在Alluxio存储中，通过Alluxio连接的UFS​​中的文件仍然 对Alluxio客户可见。当客户端尝试读取仅可从UFS获得的文件时数据将被复制到Alluxio存储中。</strong></li>
</ul>
</li>
</ul>
</li>
<li>和其他常见的分布式文件系统对比：<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211095549.png" alt="20201211095549" loading="lazy"></li>
</ul>
<h3 id="角色">角色</h3>
<ul>
<li>Alluxio的设计使用了单Master和多Worker的架构。从高层的概念理解，Alluxio可以被分为三个部分，Master，Worker和Client。
<ul>
<li>Master和Worker一起组成了Alluxio的服务端，它们是系统管理员维护和管理的组件。</li>
<li>Client通常是应用程序，如Spark或MapReduce作业，或者Alluxio的命令行用户。</li>
</ul>
</li>
<li>以前的版本需要借助 ZooKeeper 进行高可用选主，后续的 Alluxio 自己实现了高可用机制。（注：Tachyon 为 Alluxio 旧称）<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211093448.png" alt="20201211093448" loading="lazy"></li>
</ul>
<h4 id="master">Master</h4>
<ul>
<li>
<p>主从模式：主Master主要负责处理全局的系统元数据，从Master不断的读取并处理主Master写的日志。同时从Master会周期性的把所有的状态写入日志。从Master不处理任何请求。</p>
<ul>
<li>主从之间心跳检测</li>
<li>主Master不会主动发起与其他组件的通信，它只是以回复请求的方式与其他组件进行通信。一个Alluxio集群只有一个主Master。</li>
</ul>
</li>
<li>
<p>简单模式：最多只会有一个从Master，而且这个从Master不会被转换为主Maste。</p>
</li>
<li>
<p>高可用模式：可以有零个或者多个从Master。 当主Master异常的时候，系统会选一个从Master担任新的主Master。</p>
</li>
</ul>
<h4 id="worker">Worker</h4>
<ul>
<li>类似于 OSD</li>
<li>Alluxio的Worker负责管理分配给Alluxio的本地资源。这些资源可以是本地内存，SSD 或者硬盘，其可以由用户配置。</li>
<li>Alluxio的Worker以块的形式存储数据，并通过读或创建数据块的方式处理来自Client读写数据的请求。但Worker只负责这些数据块上的数据；文件到块的实际映射只会存储在Master上。</li>
</ul>
<h2 id="features">Features</h2>
<h3 id="全局命名空间">全局命名空间</h3>
<ul>
<li>Alluxio通过使用透明的命名机制和挂载API来实现有效的跨不同底层存储系统的数据管理。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201210220101.png" alt="20201210220101" loading="lazy"></li>
<li>https://www.alluxio.io/resources/whitepapers/unified-namespace-allowing-applications-to-access-data-anywhere/</li>
</ul>
<h3 id="智能多层级缓存">智能多层级缓存</h3>
<ul>
<li>Alluxio支持分层存储，以便管理内存之外的其它存储类型。目前Alluxio支持这些存储类型(存储层)：MEM (内存)，SSD (固态硬盘)，HDD (硬盘驱动器)</li>
<li><strong>单层/多层 区别？</strong></li>
</ul>
<h4 id="单层存储">单层存储</h4>
<ul>
<li>启动时默认分配一个 ramdisk，Alluxio将在每个worker节点上默认发放一个ramdisk并占用一定比例的系统的总内存。 此ramdisk将用作分配给每个Alluxio worker的唯一存储介质。</li>
<li>可以显示地设置每个 Worker 的 ramdisk 大小</li>
</ul>
<pre><code class="language-shell">alluxio.worker.ramdisk.size=16GB
</code></pre>
<ul>
<li>可以指定多个存储介质共同组成一个 level，也可以自定义添加存储介质类型</li>
</ul>
<pre><code class="language-shell">alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd1,/mnt/ssd2
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD,SSD
</code></pre>
<ul>
<li>所提供的路径应该指向安装适当存储介质的本地文件系统中的路径。要启用短路操作，这些路径的权限应该允许客户端用户对该路径进行读、写和执行。例如，启动Alluxio服务的同一用户组中的客户端用户需要770权限。</li>
<li>在更新存储媒体之后，我们需要指出为每个存储目录分配了多少存储空间。例如，如果我们想在ramdisk上使用 16GB，在每个 SSD 上使用 100GB:</li>
</ul>
<pre><code class="language-shell">alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB,100GB
</code></pre>
<h4 id="多层存储">多层存储</h4>
<ul>
<li>通常建议使用具有异构存储介质的单一存储层。在某些环境中，工作负载将受益于基于I/O速度的存储介质显式排序。Alluxio假设层是根据I/O性能从上到下排序的。例如，用户经常指定以下层:
<ul>
<li>MEM</li>
<li>SSD</li>
<li>HDD</li>
</ul>
</li>
<li><strong>写策略</strong>：用户写新的数据块时，默认情况下会将其写入顶层存储。如果顶层没有足够的可用空间， 则会尝试下一层促成。如果在所有层上均未找到存储空间，因Alluxio的设计是易失性存储，Alluxio会释放空间来存储新写入的数据块。会基于  block annotation policies 尝试从 worker 中驱逐数据，如果不能释放出新的空间，那么该写入将会失败。
<ul>
<li>eviction model 是同步的且是代表客户端来执行空间的释放的，主要是为要写入的客户端的数据腾出一块空闲空间，这种同步模式预计不会导致性能下降，因为在 block annotation policies 下有序的一组数据块通常都是可用的。</li>
</ul>
</li>
<li><strong>读策略</strong>：如果数据已经存在于Alluxio中，则客户端将简单地从已存储的数据块读取数据。 如果将Alluxio配置为多层，则不一定是从顶层读取数据块， 因为数据可能已经透明地挪到更低的存储层。有两种数据读取策略：<code>ReadType.CACHE</code> and <code>ReadType.CACHE_PROMOTE</code>。
<ul>
<li>用 <code>ReadType.CACHE_PROMOTE</code> 读取数据将在从worker读取数据前尝试首先将数据块挪到 顶层存储。也可以将其用作为一种数据管理策略 显式地将热数据移动到更高层存储读取。</li>
<li><code>ReadType.CACHE</code> Alluxio将块缓存到有可用空间的最高层。因此，如果该块当前位于磁盘(SSD/HDD)上，您将以磁盘速度读取该缓存块。</li>
</ul>
</li>
</ul>
<pre><code class="language-shell"># configure 2 tiers in Alluxio
alluxio.worker.tieredstore.levels=2
# the first (top) tier to be a memory tier
alluxio.worker.tieredstore.level0.alias=MEM
# defined `/mnt/ramdisk` to be the file path to the first tier
alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk
# defined MEM to be the medium type of the ramdisk directory
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM
# set the quota for the ramdisk to be `100GB`
alluxio.worker.tieredstore.level0.dirs.quota=100GB
# configure the second tier to be a hard disk tier
alluxio.worker.tieredstore.level1.alias=HDD
# configured 3 separate file paths for the second tier
alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3
# defined HDD to be the medium type of the second tier
alluxio.worker.tieredstore.level1.dirs.mediumtype=HDD,HDD,HDD
# define the quota for each of the 3 file paths of the second tier
alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB
</code></pre>
<h5 id="block-allocation-policies">Block Allocation Policies</h5>
<ul>
<li>Alluxio使用块分配策略来定义如何跨多个存储目录(在同一层或不同层中)分配新块。分配策略定义将新块分配到哪个存储目录中。这是通过 worker 属性 <code>alluxio.worker.allocate.class</code> 配置的。
<ul>
<li><code>MaxFreeAllocator</code>：从 0 层开始尝试到最低层，尝试将块分配到当前最具有可用性的存储目录。这是默认行为。</li>
<li><code>RoundRobinAllocator</code>：从 0 层到最低层开始尝试。在每一层上，维护存储目录的循环顺序。尝试按照轮询顺序将新块分配到一个目录中，如果这不起作用，就转到下一层。</li>
<li><code>GreedyAllocator</code>：这是 Allocator 接口的一个示例实现。它从顶层循环到最低层，尝试将新块放入可以包含该块的第一个目录中。</li>
</ul>
</li>
</ul>
<h5 id="experimental-block-allocation-review-policies">[Experimental] Block Allocation Review Policies</h5>
<ul>
<li>这是在Alluxio 2.4.1中增加的一个实验特性。在未来的版本中，接口可能会发生变化。</li>
<li>Alluxio 使用块分配审查策略来补充分配策略。与定义分配应该是什么样子的分配策略相比，分配审查过程验证分配决策，并防止那些不够好的分配决策。评审者与分配器一起工作</li>
<li>这是由worker属性 <code>alluxio.worker.review.class</code> 配置的。
<ul>
<li><code>ProbabilisticBufferReviewer</code>：基于每个存储目录对应的可用的空间，概率性低拒绝把新的数据块写入对应目录的请求。这个概率由 <code>alluxio.worker.reviewer.probabilistic.hardlimit.bytes</code> 和 <code>alluxio.worker.reviewer.probabilistic.softlimit.bytes</code> 来决定。
<ul>
<li>当可用空间低于 hardlimit，默认是 64MB，新的块将被拒绝</li>
<li>当可用空间大于 softlimit，默认 256MB，新的数据块将不会被拒绝</li>
<li>当可用空间介于上下限之间时，接受新的块的写入的概率将会随着可用容量的下降而线性低下降，我们选择在目录被填满之前尽早拒绝新的块，因为当我们读取块中的新数据时，目录中的现有块会扩大大小。在每个目录中留下缓冲区可以减少 eviction 的机会。</li>
</ul>
</li>
<li><code>AcceptingReviewer</code>：此审阅者接受每个块分配。和 v2.4.1 之前的行为完全一样</li>
</ul>
</li>
</ul>
<h5 id="block-annotation-policies">Block Annotation Policies</h5>
<ul>
<li>Alluxio使用块注释策略(从v2.3开始)来保持存储中块的严格顺序。Annotation策略定义了跨层块的顺序，并在以下过程中被参考:
<ul>
<li>Eviction</li>
<li>Dynamic Block Placement.</li>
</ul>
</li>
<li>与写操作一起发生的 Eviction 操作将尝试根据块注释策略执行的顺序删除块。按注释顺序排列的最后一个块是驱逐的第一个候选者，无论它位于哪一层。</li>
<li>可配置对应的 Anotator 类型，<code>alluxio.worker.block.annotator.class</code>。有如下 annotation 实现：
<ul>
<li><code>LRUAnnotator</code>：根据最近最少使用的顺序注释块。这是Alluxio的默认注释器。</li>
<li><code>LRFUAnnotator</code>：使用可配置的权重，根据最近最不常用和最不常用的顺序注释块。
<ul>
<li>如果权重完全偏向最近最少使用的，行为将与LRUAnnotator相同。</li>
<li>使用 <code>alluxio.worker.block.annotator.lrfu.step.factor</code> 和 <code>alluxio.worker.block.annotator.lrfu.attenuation.factor</code> 来配置。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="managing-data-replication-in-alluxio">Managing Data Replication in Alluxio</h4>
<h5 id="passive-replication">Passive Replication</h5>
<ul>
<li>与许多分布式文件系统一样，Alluxio中的每个文件都包含一个或多个分布在集群中存储的存储块。默认情况下，Alluxio可以根据工作负载和存储容量自动调整不同块的复制级别。例如，当更多的客户以类型CACHE或CACHE_PROMOTE请求来读取此块时Alluxio可能会创建此特定块更多副本。当较少使用现有副本时，Alluxio可能会删除一些不常用现有副本 来为经常访问的数据征回空间(块注释策略)。 在同一文件中不同的块可能根据访问频率不同而具有不同数量副本。</li>
<li>默认情况下，此复制或征回决定以及相应的数据传输 对访问存储在Alluxio中数据的用户和应用程序完全透明。</li>
</ul>
<h5 id="active-replication">Active Replication</h5>
<ul>
<li>除了动态复制调整之外，Alluxio还提供API和命令行 界面供用户明确设置文件的复制级别目标范围。 尤其是，用户可以在Alluxio中为文件配置以下两个属性:
<ul>
<li><code>alluxio.user.file.replication.min</code> 是此文件的最小副本数。 默认值为0，即在默认情况下，Alluxio可能会在文件变冷后从Alluxio管理空间完全删除该文件。 通过将此属性设置为正整数，Alluxio 将定期检查此文件中所有块的复制级别。当某些块 的复制数不足时，Alluxio不会删除这些块中的任何一个，而是主动创建更多 副本以恢复其复制级别。</li>
<li><code>alluxio.user.file.replication.max</code> 是最大副本数。一旦文件该属性 设置为正整数，Alluxio将检查复制级别并删除多余的 副本。将此属性设置为-1为不设上限(默认情况)，设置为0以防止 在Alluxio中存储此文件的任何数据。注意，<code>alluxio.user.file.replication.max</code> 的值 必须不少于 <code>alluxio.user.file.replication.min</code>。</li>
</ul>
</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<h3 id="testing-alluxio-for-memory-speed-computation-on-ceph-objects">Testing Alluxio for Memory Speed Computation on Ceph Objects</h3>
<ul>
<li>https://blog.zhaw.ch/icclab/testing-alluxio-for-memory-speed-computation-on-ceph-objects/#more-12747</li>
<li>4th SEPTEMBER 2020</li>
</ul>
<h4 id="环境介绍">环境介绍</h4>
<ul>
<li>底层存储：Ceph mimic
<ul>
<li>6 OpenStack VMs
<ul>
<li>one Ceph monitor</li>
<li>three storage devices running Object Storage Devices (OSDs)</li>
<li>one Ceph RADOS Gateway (RGW) node</li>
<li>one administration node</li>
</ul>
</li>
<li>total storage size of 420GiB was spread over 7 OSD volumes attached to the three OSD nodes</li>
</ul>
</li>
<li>Alluxio 2.3， Java8 (换成 Java11 即升级 Alluxio 后会有后续提升)</li>
<li>Spark 3.0.0</li>
<li>两种模式：
<ul>
<li>单 VM 运行 Alluxio 和 Spark （16vCPU，40GB of memory）</li>
<li>集群模式：two additional Spark and Alluxio worker nodes are configured (with 16vCPUs and 40GB of memory).</li>
</ul>
</li>
<li>对比测试：
<ul>
<li>直接访问 Ceph RGW 和 通过 Alluxio 访问
<ul>
<li>通过 Alluxio 访问时，第一次访问文件的话，Alluxio 会将文件上传到内存中，后续的文件访问将直接命中内存，从而带来显著的性能提升。</li>
</ul>
</li>
<li>不同文件大小： 1GB, 5GB and 10GB，记录第一层和第二次访问文件需要的时间。
<ul>
<li>平均会运行超过 10 次</li>
<li>然后再次启动相同的应用程序，以再次测量相同的文件访问时间。这样做的目的是展示内存中的 Alluxio 缓存如何为以后访问相同数据的应用程序带来好处。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="测试结果">测试结果：</h4>
<ul>
<li>如下为单节点测试测试结果，Ceph 上第二次访问该文件相比于 Alluxio 在 1GB,5GB,10GB 时的执行时间分别为 75x，111x，107x<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211113435.png" alt="20201211113435" loading="lazy"></li>
<li>如下为集群模式下的测试结果，所有情况的整体时间比单机的时候少了很多，Ceph 相比于 Alluxio 的第二次访问时间为 35x, 57x, 65x<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211114027.png" alt="20201211114027" loading="lazy"></li>
</ul>
<h3 id="accelerate-and-scale-big-data-analytics-with-alluxio-and-inteloptanetm-persistent-memory">Accelerate And Scale Big Data AnAlytics with Alluxio And intel®optane™ persistent Memory</h3>
<ul>
<li>https://www.alluxio.io/app/uploads/2020/05/Intel-Alluxio-DCPMM-Whitepaper-200507.pdf</li>
</ul>
<h3 id="reliability-testing">Reliability Testing</h3>
<ul>
<li>TODO</li>
</ul>
<h2 id="install-deploy">Install &amp; Deploy</h2>
<h3 id="single-server">Single Server</h3>
<h4 id="download">Download</h4>
<ul>
<li>Download Binary: https://www.alluxio.io/download/</li>
<li>Choose Version. (eg. Alluxio 2.4.1 Release. 1.4GB)</li>
<li>Tar file: <code>tar -xzf alluxio-2.4.1-bin.tar.gz</code></li>
</ul>
<h4 id="initial-config">Initial Config</h4>
<ul>
<li><code>cd alluxio-2.4.1/conf &amp;&amp; cp alluxio-site.properties.template alluxio-site.properties</code></li>
<li><code>echo &quot;alluxio.master.hostname=localhost&quot; &gt;&gt; conf/alluxio-site.properties</code></li>
<li>[Optional] If use local file system, you can specific configuration in conf files like this: <code>echo &quot;alluxio.master.mount.table.root.ufs=/root/shunzi/Alluxio/tmp&quot; &gt;&gt; conf/alluxio-site.properties</code></li>
<li>Validate env: <code>./bin/alluxio validateEnv local</code></li>
</ul>
<pre><code class="language-cmd">2 Errors:
ValidateRamDiskMountPrivilege
ValidateHdfsVersion
</code></pre>
<h4 id="start-alluxio">Start Alluxio</h4>
<ul>
<li>Format journal and storage directory: <code>./bin/alluxio format</code>
<ul>
<li>It may throw exceptions <code>java.nio.file.NoSuchFileException: /mnt/ramdisk/alluxioworker</code> in <code>log/task.log</code>. So you need to <code>mkdir -p /mnt/ramdisk/alluxioworker</code></li>
</ul>
</li>
<li>Start alluxio (with a master and a worker): <code>./bin/alluxio-start.sh local SudoMount</code></li>
<li>Stop local server: <code>./bin/alluxio-stop.sh local</code>
<ul>
<li><code>./bin/alluxio-stop.sh all</code></li>
</ul>
</li>
</ul>
<h5 id="verify">Verify</h5>
<ul>
<li>Access website <code>http://localhost:19999</code> to check the master server status.</li>
<li>Access website <code>http://localhost:30000</code> to check the worker server status.</li>
<li>For internal network, you can use reverse proxy like this: (And you can access website <strong>master</strong> <code>http://114.116.234.136:19999</code> and <strong>worker</strong> <code>http://114.116.234.136:30000</code>)
<ul>
<li><code>autossh -M 1999 -fNR 19999:localhost:19999 root@114.116.234.136</code></li>
<li><code>autossh -M 3000 -fNR 30000:localhost:30000 root@114.116.234.136</code></li>
</ul>
</li>
</ul>
<h5 id="run-tests">Run tests</h5>
<ul>
<li>Verify run status and run test cases: <code>./bin/alluxio runTests</code></li>
<li>The runTests command runs end-to-end tests on an Alluxio cluster to provide a comprehensive sanity check.</li>
<li>It will generate directory <code>/default_tests_files</code> and use different cache policy to upload files.
<ul>
<li>BASIC_CACHE_ASYNC_THROUGH</li>
<li>BASIC_CACHE_CACHE_THROUGH</li>
<li>BASIC_CACHE_MUST_CACHE</li>
<li>BASIC_CACHE_PROMOTE_ASYNC_THROUGH</li>
<li>BASIC_CACHE_PROMOTE_CACHE_THROUGH</li>
<li>BASIC_CACHE_PROMOTE_MUST_CACHE</li>
<li>BASIC_CACHE_PROMOTE_THROUGH</li>
<li>BASIC_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_ASYNC_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_MUST_CACHE</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_ASYNC_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_MUST_CACHE</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_ASYNC_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_MUST_CACHE</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_THROUGH</li>
<li>BASIC_NO_CACHE_ASYNC_THROUGH</li>
<li>BASIC_NO_CACHE_CACHE_THROUGH</li>
<li>BASIC_NO_CACHE_MUST_CACHE</li>
<li>BASIC_NO_CACHE_THROUGH</li>
</ul>
</li>
</ul>
<h4 id="simple-example">Simple Example</h4>
<h5 id="upload-files-from-local-server">Upload files from local server</h5>
<ul>
<li>Show fs command help: <code>./bin/alluxio fs</code></li>
<li>List the files in Alluxio: <code>./bin/alluxio fs ls /</code></li>
<li>Copy files from local server: <code>./bin/alluxio fs copyFromLocal LICENSE /LICENSE</code></li>
<li>List again: <code>./bin/alluxio fs ls /</code></li>
<li>Cat the file: <code>./bin/alluxio fs cat /LICENSE</code></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="https://docs.alluxio.io/os/user/stable/cn/overview/Getting-Started.html">[1] Alluxio 快速上手指南</a></li>
<li><a href="https://blog.csdn.net/baichoufei90/article/details/107322069">[2] CSDN - Alluxio学习</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/127118960">[3] 知乎 - 路云飞：Alluxio 技术分析</a></li>
<li><a href="https://www.zhihu.com/column/alluxio">[4] 知乎 - Alluxio 专栏</a></li>
<li><a href="https://www.jianshu.com/p/481675971727">[5] 简书 - Alluxio：架构及数据流</a></li>
<li><a href="https://www.infoq.cn/article/q58xagobiioimqeem-a9">[6] InfoQ - Alluxio在多级分布式缓存系统中的应用</a></li>
<li><a href="https://pdf.dfcfw.com/pdf/H3_AP201912181371929557_1.pdf?1596649255000.pdf">[7] Alluxio 开源 AI 和大数据存储编排平台-顾荣</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees]]></title>
        <id>https://blog.shunzi.tech/post/osdi-Bourbon/</id>
        <link href="https://blog.shunzi.tech/post/osdi-Bourbon/">
        </link>
        <updated>2020-11-06T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2020 From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2020 From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees</li>
</ul>
</blockquote>
<!--more-->
<blockquote>
<ul>
<li>很久没看 LSM tree 的文章了，恰好这几天 OSDI2020 开了，果不其然还是有 LSM-tree 相关的，而且是和学习索引结合的，之前对  Learned Index 也是一知半解，所以拜读了一下。</li>
<li>这篇文章从写作和行文的角度讲写的特别的清楚，实验做的可能是我见过的最充分的，从一开始发现问题就开始用了大量实验来证明，后续的负载测试也极其丰富，回答了读者可能会有的各种各样的问题。值得一读。</li>
</ul>
</blockquote>
<h2 id="abstract">Abstract</h2>
<ul>
<li>Bourbon，一个利用了机器学习来提供快速查询的 LSM-tree</li>
<li>Bourbon 使用贪婪的分段线性回归来学习 key 分布，以最小的计算代价实现快速查找，并根据成本和收益来决定何时学习是值得的。实验表明，查询性能相比于最先进的 LSMs 提升了 1.23x-1.78x</li>
</ul>
<h2 id="introduction">Introduction</h2>
<h4 id="学习索引是个啥">学习索引是个啥？</h4>
<ul>
<li>作者上来先简要介绍了一波机器学习，当然是为了引出重点 <strong>学习索引</strong>，简单地讲，学习索引就是指当你查询一个 key 的时候，系统使用该索引（或者该函数）预测出你要查询的 key 所对应的位置，相比于传统的数据结构中的查找性能有比较大的提升，某些场景下可能提升可能更为明显，同时一定程度上因为不直接构建具体的数据结构节省了空间开销。基于这项工作，很多人提出了更好的模型、更好的树结构来减少对基于树的索引结构的访问和开销。</li>
</ul>
<h4 id="学习索引和-lsms-能擦出啥火花">学习索引和 LSMs 能擦出啥火花？</h4>
<h5 id="两者理论上的矛盾">两者理论上的矛盾</h5>
<ul>
<li>现有的学习索引大多是基于数据库场景中的 B 树来做的，很少有人提出说将学习索引应用到 LSM-tree 上，所以作者就尝试着把学习索引的想法应用到 LSM-tree 上（LSM-tree 的应用就不再具体介绍）。那么问题来了，为什么其他人没想到说把学习索引用到 LSM-Tree 上呢？<strong>主要是因为学习索引主要针对只读设置而量身定做的，而 LSMs 则主要是针对写进行了优化。</strong></li>
<li>听着很抽象？那先简单解释一下。LSM tree 是对写比较友好的，但是写操作会影响学习索引，因为学习索引通常是基于原有的数据学习出来的，现在数据都变了，那你索引肯定得需要做出相应的改变才能保证你索引的准确性，最直接的办法当然是直接再学习。</li>
<li><strong>然而，作者发现 LSMs 非常适合用于学习索引</strong>，虽然写操作修改了 LSM，但树的大多数部分是不可变的；因此，学习一个预测键/值位置的函数只需要完成一次，并且只要不可变数据存在就可以使用它。然而也有别的问题，可变的键或值大小使学习预测位置的函数变得更加困难，过早地执行模型构建可能导致大量的资源浪费。</li>
</ul>
<h5 id="bourbon-做了啥">Bourbon 做了啥</h5>
<ul>
<li>作者研究了 WiscKey，得出了几条 guidelines
<ul>
<li>虽然学习 LSM 中稳定的低级别是有用的，但是学习更高级别也会带来好处，因为查找必须始终先搜索更高级别</li>
<li>并不是所有的文件都是相同的：一些文件即使在较低的级别也是非常短暂的：系统必须避免学习这些文件，否则会浪费资源</li>
<li>工作负载和数据感知非常重要：根据工作负载和数据加载方式，了解树的某些部分可能比了解其他部分更有益</li>
</ul>
</li>
<li>Bourbon 基于 WiscKey 实现，WiscKey 原本大约 20K 行代码，Bourbon 增加了大约 5K 行，使用分段线性回归，这是一种简单但有效的模型，能够在很小的空间开销下实现快速训练(即学习)和推理(即查找)，使用文件学习:模型建立在文件之上，假设一个LSM文件一旦创建，就不会被修改。实现了一个成本效益分析器，它动态地决定是否学习一个文件，在最大化收益的同时减少了不必要的学习。</li>
</ul>
<h2 id="background">Background</h2>
<h3 id="lsm-leveldb">LSM &amp; LevelDB</h3>
<ul>
<li>如下图所示是 LevelDB 和 WiscKey 的原理示意图。具体的介绍请参考其他资料，此处不再详细展开。本文中提到的 higher level 是指存放了更新的数据的 level，lower level 是指存放了更老的数据的 level。</li>
<li>简要介绍查询步骤，如图所示，便于后文引入学习索引：
<ul>
<li>step1. FindFiles：如果 key 在内存中的 tables 中没有找到，LevelDB 将会查找到一组候选的来自磁盘的可能包含键 k 的 sstables。最坏的情况是 k 可能出现在所有 L0 文件中(因为重叠的范围)，并在每个连续级别的一个文件中</li>
<li>step2. LoadIB+FB：对于每一个候选的 SSTable，其索引块和布隆过滤器块首先被加载</li>
<li>step3. SearchIB：对索引块进行二分查找，从而找到可能包含 k 的数据块</li>
<li>step4. SearchFB：查询过滤器判断 k 是否存在对应的 datablock 中</li>
<li>step5. ReadValue：如果 k 在对应的 datablock 中，相应地读取出对应的 value，然后查询结束。如果上一步的 filter 显示该 key 不存在或者对 datablock 查询时没有找到相应的 key，搜索操作将继续在下一个候选文件中执行</li>
<li>NOTICE：blocks 不一定总是从磁盘中加载出来的，index block 和 filter block，以及经常访问的 data blocks 很有可能就在内存中可以被直接访问（文件系统缓存）。</li>
</ul>
</li>
<li>作者对于索引的步骤和数据访问的步骤做了简单区分。 本文目标即为减少索引过程中的开销。
<ul>
<li>FindFiles, SearchIB, SearchFB, and SearchDB 都是通过文件和 blocks 找到对应的键，也就是所谓的 indexing steps</li>
<li>LoadIB+FB, LoadDB, and ReadValue 从存储中读取数据就是所谓的 data-access steps<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201106165224.png" alt="20201106165224" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="wisckey">WiscKey</h3>
<ul>
<li>WiscKey 是为了解决 LSM-Tree 中比较严峻的写放大问题而提出的，架构如上图所示，主要是讲 Key Value 分离，只是对 Key 使用 LSM 存储，Value 直接使用 Value Log 进行存储，因为数据量小了，写放大也就得到了缓解，同时因为比较小就可以缓存在内存中，因此一个查询操作可能最终只涉及到一次 IO 操作来读取 Value Log 上指定位置的 Value</li>
</ul>
<h3 id="optimizing-lookups-in-lsms">Optimizing Lookups in LSMs</h3>
<ul>
<li>因为 LSM Tree 本身结构的原因，对于 LSM Tree 的查询可能需要对多个 level 进行查询，而且 LSM Tree 本能就是以写性能见长的，在读性能方面表现较差，所以对 LSM Tree 的读操作进行优化就很有必要。</li>
<li>受学习索引的启发，现在有很多工作考虑使用机器学习模型来替代传统的索引结构，核心思想是针对输入训练一个模型（使用如线性回归或者神经网络的方法）从而预测出输入对应的记录子啊排序好了的数据集中的具体地址。模型可能有误差，因此预测有一个相关的误差界。在查找过程中，如果模型预测的键的位置是正确的，则返回记录;如果错误，则在错误范围内执行本地搜索。例如，如果预测的位置是 pos，而最小和最大的误差范围是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>i</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">α_min</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">α_max</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span></span></span></span>，那么根据错误的预测，在 pos - <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>m</mi><mi>i</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">α_mmin</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span></span></span> 和 pos + <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">α_max</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span></span></span></span> 之间进行局部搜索。</li>
<li>学习的索引可以大大加快查找速度。直观地说，一个学习过的索引将 b 树的 O(log-n) 查找变成 O(1) 操作。根据实际的经验表明，学习索引提升 B 树的查询性能约 1.5x-3x</li>
<li>传统的学习索引不支持更新，因为在现有数据上学习的模型会随着修改而改变。但是，LSMs 在写密集型工作负载中的高性能很有吸引力，因为它们只按顺序执行写操作。因此提出了关键问题给：如何实现学习索引同时保证 LSM 对写性能带来的提升？</li>
</ul>
<h2 id="learned-indexes-a-good-match-for-lsms">Learned Indexes: a Good Match for LSMs?</h2>
<h3 id="learned-indexes-beneficial-regimes">Learned Indexes: Beneficial Regimes</h3>
<ul>
<li>LSM-Trees 中的查询操作包含索引和数据访问两个方面的操作，如前面章节所述。优化后的索引如学习索引可以减少索引的一些步骤的开销，但是对于数据访问的开销没什么影响。在 WiscKey 中，学习索引可以减少如 FindFiles, SearchIB, and SearchDB 的开销。因此如果索引在总查找延迟中占相当大的比例，学习索引就可以显著提升查询的性能。</li>
<li>首先，当数据集或它的一部分缓存在内存中时，数据访问成本很低，因此索引成本就变得很重要。下图展示了在 WiscKey 中的延迟分解情况。柱状图的第一条显示了全部缓存在内存中的情况，第二条显示了数据存储在 SATA SSD 上的情况。其实第一条就相当于有缓存的情况，数据访问和索引成本对延迟的贡献几乎是相等的，优化索引部分可以将查找延迟降低约 2 倍，当不缓存数据集时，数据访问成本占主导地位，因此优化索引可能产生较小的好处，大约只有 20%。</li>
<li>然而，学习索引并不局限于数据缓存在内存中的场景。它们为当前流行的快速存储设备提供了优势，并且可以在正在出现的更快的设备上发挥更大的作用，如图所示随着设备的升级，即便延迟大大降低，但是索引结构的操作所占的延迟比重越来越大，如在 Optane SSD 中，索引结构的操作占据了大约 44% 的比例，因此优化索引结构的相关操作可以将性能提升约 1.8x。<strong>随着存储器件的发展，学习索引能够发挥的效果也越来越显著</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201106175207.png" alt="20201106175207" loading="lazy"></li>
</ul>
<h3 id="learned-indexes-with-writes">Learned Indexes with Writes</h3>
<ul>
<li>与传统索引相比，学习索引为只读分析工作负载提供了更高的查找性能。：然而，学习索引的一个主要缺点是它们不支持诸如插入和更新之类的修改，因为修改操作改变了数据的分布，所以模型就必须重新学习，对于写密集型的负载就常常需要重建模型，频繁的重建就会导致比较高的开销。</li>
<li>乍一看，学习索引似乎并不适合那些 LSMs 优化的写密集的负载，然而，我们观察到 LSMs 的设计很适合学习索引。我们的关键认识是，<strong>尽管更新可以改变 LSM 树的一部分，但大部分仍然是不变的</strong>。具体来说，新修改的项缓冲在内存结构中，或者存在于树的较高级别中，而稳定的数据驻留在较低级别。考虑到数据集的很大一部分位于稳定的、较低的级别，对这一部分的查找可以更快，而无需或只需进行少量的重新学习。相比之下，在更高层次的学习可能没有那么有效果，它们变化的速度更快，因此必须经常重新学习。</li>
<li>我们还认识到，<strong>SStable 文件的不可变特性使它们成为理想的学习单元</strong>。一旦学习之后，这些文件就再也不会被更新，因此一个模型可以一直被使用，除非该文件被替换。除此以外，SSTable 内的数据还是有序的，有序的数据就可以采用更简单的模型学习，一个级别是许多不可变文件的集合，也可以使用简单的模型作为一个整体来学习。一个级别中的数据也进行了排序：对各个sstable进行了排序，并且在sstable之间不存在重叠的键范围。</li>
<li>进行了一些实验来证明上述结论，实验的目标是确定一个模型在多长时间内是有用的，以及模型使用的频率。只要SSTable 文件存在，为该文件建立的模型就有用，因此，我们首先测量和分析 SSTable 的寿命。一个模型被使用的额频率将由内部查询的次数决定，因此只需要测试每个文件的内部查询次数即可，因为模型也可以基于整个 level 构建，所以作者也测试了 level 的 lifetimes，实验是基于 WiscKey 做的，但作者认为对应的实验结论也应该适用于大多数 LSM 的实现。</li>
</ul>
<h4 id="sstable-lifetimes">SSTable Lifetimes</h4>
<ul>
<li>下图 a 显示了不同层级的 SSTables 文件的平均寿命（寿命通过使用文件的创建时间和删除时间来衡量）。
<ul>
<li>较低级别的 SSTable 文件的平均寿命大于较高级别的。</li>
<li>在较低的写比例的负载下，即使是较高级别的文件也有相当长的生存期，但较低级别的寿命此时更长</li>
<li>即便随着更高的写比例导致文件的寿命下降，但是对于低级别的文件而言，寿命还是很长</li>
</ul>
</li>
<li>图 b 展示了 5% 的写比例的情况下 L1 和 L4 的寿命的分布情况， 可以发现有的文件寿命非常短，有的文件寿命非常长。如 L1 的大约 50% 的寿命只有 2.5s，如果过了该临界值，寿命就会变得特别长，超过五分钟。而对于 L4，只有很少的文件寿命很短，大约有 2% 的寿命不超过 1s，造成该现象的原因可能是：
<ul>
<li>压缩一个 Li 层的文件会在 Li+1 中创建一个新文件，该文件会被立即选择用于压缩到下一个级别</li>
<li>压缩一个 Li 层的文件会在 Li+1 中创建一个新文件，该文件与从 Li 压缩的下一个文件有重叠的键范围</li>
</ul>
</li>
<li>图 c 展示了不同写请求比例下 L1 和 L4 的寿命分布，规律和 5% 时大体相同。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107111926.png" alt="20201107111926" loading="lazy"></li>
<li>于是乎总结出了两条 guidelines：
<ul>
<li>Favor learning files at lower levels：学习索引最好用于较低层次的文件，因为这些文件的寿命通常比较长</li>
<li>Wait before learning a file：学习索引最好在某个文件存活的时间达到一定阈值之后才开始学习，因为有的文件寿命可能很短，即便是在一些较低层次的文件，因为存活持续了一段时间之后该文件才可能存活的比较长时间。</li>
</ul>
</li>
</ul>
<h4 id="number-of-internal-lookups-per-file">Number of Internal Lookups Per File</h4>
<ul>
<li>为了测试模型的使用频率，就分析了 SSTables 对应的内部查询次数。如下图所示，图 a 显示了数据集以一个随机顺序加载的情况，较高层次对应的总的内部查询次数更多，即便是很大一部分数据驻留在较低的级别上（图 (a)(ii) 则显示了查询不命中的情况），而如图(a)(iii)所示的查询命中的情况时候，低级别的文件的查询次数更多。结果表明更高级别的文件通常服务于一些不命中也就是 negative 的查询操作，虽然采用了 BloomFilter 来尽可能加速 negative lookup 的过程，但是 index block 在查询 filter block 之前还是会被查询。</li>
<li>同时还针对 zipfian 的负载（大多数请求都是针对一小组键的）下进行了同样的测试，结果表明大多数情况下都和随机加载的负载是相似的，除了 positive lookup，如图 (a)(iv)，在 zipfian 负载下，更高层级的文件处理更多的 positive lookups，因为负载经常访问一小组常被更新的键，因此这组键被常常存储在更高的 level 上。</li>
<li>图 b 显示了数据集被顺序加载（keys 按照升序或者降序的方式被插入）的情况，相比于随机加载的情况，就没有了 negative lookups，因为不同 SSTable 的键不会重叠，即便是在不同的 level 上也不会重叠，FindFiles 步骤可以直接找到可能包含该 key 的唯一的文件。因此，较低的级别提供更多的查找，并可以从学习索引中获得更多好处。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107113529.png" alt="20201107113529" loading="lazy"></li>
<li>从上述的实验观察中也得到了两个 guidelines：
<ul>
<li>Do not neglect files at higher levels：即便更底层次的文件寿命更长，处理的查询也更多，但是更高层次的文件在某些负载下也是可能处理很多 negative 甚至 positive 的查询请求的，因此学习索引在高层次的文件中也能让内部查询更快</li>
<li>Be workload- and data-aware：尽管大多数数据位于较低的级别，但如果工作负载不查找这些数据，那么学习这些级别带来的收益就很悠闲；因此，学习索引必须能够感知工作负载的情况。除此以外，数据被加载的顺序性也会影响那些层处理更多的内部查询请求，即会影响请求处理的层级分布情况，因此学习索引还需要感知数据的情况。内部查询请求次数可以同时代表工作负载和数据加载顺序，所以基于请求次数就可以动态地决定是否要学习某个文件。</li>
</ul>
</li>
</ul>
<h4 id="lifetime-of-levels">Lifetime of Levels</h4>
<ul>
<li>前面章节有描述过一整个层级也是可以被学习的，所以作者测试分析了整个层级的寿命。</li>
<li>因为 L0 层无序，L0 层的文件可能有重叠的键范围，所以不能应用层级学习的策略。一旦一个层级被学习了之后，任何对于该层级的更新都可能导致重学习，而层级更新则是指新的 SSTable 文件在该层次被创建，或者一个已经存在的被删除，因此，一个层级的的寿命与单个 SSTable 相同或更短。在层级的粒度上进行学习的好处是不需要在单独的步骤中找到候选 SSTables，而是在查询时候模型直接输出对应的 SSTable 和文件内的偏移。</li>
<li>下图 a 展示了在 5% 的写比例下不同层级的文件变化情况，纵轴上的 0 表示当前时间该层级没有发生变化，此时则可以进行学习。如果大于 0 则表示该层级发生了变化，因此就需要进行重新学习。更高层次对应的文件变化频率更高。随着级别的下降，更改的文件的比例会减少，因为较低的级别在许多文件中包含大量数据</li>
<li>对层级文件的更改通常是突发性的。该突发通常是由压缩引起的该层级中的很多文件同时被修改，因此这些突发的时机在不同层次表现的时间基本相同。这背后的原因是，对于我们使用的数据集，级别 L0 到 L3 已经满了，因此任何在一层上的压缩都会导致<strong>级联压缩</strong>，最终在未满的 L4 级别上存放。在这些突发情况之间层级的文件基本会保持平稳不会发生变化。</li>
<li>但是随着写请求比例的上升，突发间隔会逐渐减小，如图 b 所示，层级的平稳周期将大幅减小。如图所示 5% 时，大约有 5 分钟的周期，在 50% 的时候，就只有大约 25s 了。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107115847.png" alt="20201107115847" loading="lazy"></li>
<li>基于上述实验，又总结了一条 guidelines
<ul>
<li>Do not learn levels for write-heavy workloads：当写请求比例较低的时候，学习一整个层级还算比较合适，但是写密集型的负载，因为层级寿命变得很短就可能导致很频繁的重新学习。</li>
</ul>
</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>通过对 WiscKey 的实验分析，总结了五条 guidelines:
<ul>
<li>Favor learning files at lower levels</li>
<li>Wait before learning a file</li>
<li>Do not neglect files at higher levels</li>
<li>Be workload- and data-aware</li>
<li>Do not learn levels for write-heavy workloads</li>
</ul>
</li>
</ul>
<h2 id="bourbon-design">Bourbon Design</h2>
<h3 id="learning-the-data">Learning the Data</h3>
<ul>
<li>回顾学习索引的目标：预测 key 在一个有序的数据集中的位置。</li>
<li>本文设计了两种学习索引，对应学习的粒度不同。
<ul>
<li>File Learning：预测 key 对应的在文件内的偏移</li>
<li>Level Learning：预测出对应的 SSTable 文件和文件内的偏移</li>
</ul>
</li>
<li>对于学习索引的要求：无论是学习过程还是查询过程，开销都需要很低才能真正优化整个系统。除此以外，因为优化的是磁盘上的数据结构对应的存储系统，空间的开销也需要尽可能的小。作者发现分段线性回归（PLR）能够同时满足上面的要求。PLR 的本质是用一些线段来表示有序的数据集，PLR 构造了一个有误差限制的模型，每个数据点 d 必须在范围 [<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> − δ, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> + δ] 内，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 数据集中的 d 预测的位置，δ 是提前定义好的误差限制。</li>
<li>为了训练 PLR 模型，Bourbon 方案使用了 Greedy-PLR 算法，一次处理一个数据点，如果数据点不能在不超出定义的误差限制的情况下被添加到当前的线段中，那么将创建一个新的线段，并将数据点添加到其中，最终 Greedy-PLR 生成了一组代表数据的线段。Greedy-PLR 的运行时间与数据点的数量呈线性关系。</li>
<li>一旦模型学习完成，推理就会很快。首先，找到包含键的正确线段(使用二分查找)。在该线段内，目标键的位置是通过将键与直线的斜率相乘并加上截距得到的。如果键不在预测的位置，在误差范围内进行局部查询。因此查询操作除了常数时间做局部搜索，只需要花费 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>s</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logs)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">s</span><span class="mclose">)</span></span></span></span> 的时间，其中 s 是线段的数量。PLR 的空间开销很小：每个线段只有几十个字节。</li>
<li>其他模型诸如 RMI，PGMIndex，splines 等可能更适合 LSMs 且提供比 PLR 更好的表现，未来可以采用这些模型来进行实现。</li>
</ul>
<h3 id="supporting-variable-size-values">Supporting Variable-size Values</h3>
<ul>
<li>如果 KV 对大小相同的话，学习索引预测 KV 对的偏移量将会很容易，模型可以将 key 的预测位置乘以 KV 对的大小，从而产生最终的偏移量。但是对于许多系统而言，往往允许任意大小的 KV 对。</li>
<li>Bourbon 要求 Key 是固定大小的，但是 Value 可以是不固定的。作者认为这是一个合理的设计，因为大多数数据集有确定大小的 key，比如 user-id 通常有 16bytes，但是 value 的大小就不固定了。即使 keys 大小不同，可以填充使所有 keys 的大小相同。Bourbon 通过借鉴WiscKey的键值分离思想来支持可变长度的 value。</li>
<li>Key Value 分离的方式，Bourbon 中的 SSTables 就只会包含 key 和对应的指向 value 的指针，value 被单独维护在 value log 中，在这样的模式下，Bourbon 通过从模型中得到预测的位置，然后乘以对应的记录大小（通常是 keySize  + pointerSize），从而获取要访问的 KV 对的偏移量，Value 指针用作 Value Log 的偏移量，最终从该日志读取值。</li>
</ul>
<h3 id="level-vs-file-learning">Level vs. File Learning</h3>
<ul>
<li>前面的分析表明文件的寿命比层级的寿命通常更长，特别是在写密集的负载下，也就意味着以文件的粒度进行学习可能是更好的选择。作者为了在 file 和 level 之间进行权衡测试了不同负载下对应的性能，初始化的时候都加载一个数据集并构建模型，对于只读负载，模型不需要重新学习，在混合负载中，因为数据的改变模型需要重新学习。</li>
<li>如下表所示，混合负载下，Level 明显不如 File，因为有稳定的写入流，系统无法对 Level 进行学习。只有 1.5% 的内部查找采用模型路径；这些查找是在加载数据之后以及初始的 Level 模型可用时执行的。作者观察到所有尝试的 66 次 level 学习都失败了因为在学习完成之前 level 已经发生了改变。由于重新学习的额外成本，level 学习的性能甚至比 50% 写操作的基线还要差.而使用 file model，大比例的查询操作都能从学习索引中获益，因此 file model 相比于基线性能有所提升。</li>
<li>对于读敏感的负载（ 5% 的写），尽管 level model 相比于基线有一定的提升，但还是比 file model 性能表现要差，原因还是因为重新学习的额外成本和仅作用了有限的查询操作，带来的提升有限。</li>
<li>只有在只读负载下，Level 学习才能带来比较大的提升，甚至比起 file learning 都提升了 10%，因此，只有只读工作负载的部署可以从层级学习中获益。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107155836.png" alt="20201107155836" loading="lazy"></li>
<li>鉴于 Bourbon 的目标是在支持写操作的同时提供更快的查找，对于学习粒度来说，level 并不是一个合适的选择，所以 Bourbon 默认使用文件学习，但同时也会支持 level 学习以便适应只读负载。</li>
</ul>
<h3 id="cost-vs-benefit-analyzer">Cost vs. Benefit Analyzer</h3>
<ul>
<li>因为还是有部分文件的寿命较短，对这类文件的学习可能是对资源的浪费，所以需要开销和收益的分析机制来决定是否要对某一个文件进行学习。</li>
</ul>
<h4 id="wait-before-learning">Wait Before Learning</h4>
<ul>
<li>从前文的 Guidelines 中了解到，需要设定一个等待时间的阈值 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，即在学习一个文件之前，需要等待相应的时间，该系数的具体值体现了开销和收益的权衡。值太小，导致一些寿命较短的文件也被学习，引入了较大的开销，值太大导致执行大量的查询的时候，因为模型还未学习构建，导致大量的查询不能通过学习索引来进行优化，导致性能下降。</li>
<li>BOURBON 将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设置为学习一个文件大概所需要的时间。测试发现学习一个文件（最大 4MB）的最长时间大约为 40ms，作者保守地将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设置为 50ms</li>
</ul>
<h4 id="to-learn-a-file-or-not">To Learn a File or Not</h4>
<ul>
<li>虽然现在有了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，但是一个文件即便是存活了很长时间但是可能也不是特别有益的。作者实验发现更低级别的文件通常寿命更长，对于有的工作负载和数据集，他们服务的查询操作比更高级别的文件要少得多，更高级别的文件尽管寿命较短，但是在有的场景下服务了大量的 negative lookups。因此除了考虑模型对应的开销以外，还需要考虑模型可能带来的收益。如果一个模型的收益（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）大于构建该模型的开销（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）那么该模型就是有利的。</li>
</ul>
<h5 id="estimating-c_model">Estimating <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></h5>
<ul>
<li>评估开销的一种方式是假设学习过程完全是在后台完成的且不会影响系统其他部分，那么开销就为 0，如果有很多空闲的 core，学习线程可以利用它们，这样就不会干扰前台任务（工作负载的处理或者压缩过程等）。但是 Bourbon 采用了一种比较保守的办法并且假设学习线程会干扰和减慢系统的其他部分，所以，Bourbon 假设开销等于为单个文件构建 PLR 模型的时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>b</mi><mi>u</mi><mi>i</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{build}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，我们发现，这个时间与文件中的数据点数量成线性比例，因此可以通过将训练一个数据点的平均时间和该文件中包含的点的数量相乘从而得到该时间。</li>
</ul>
<h5 id="estimating-b_model">Estimating <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></h5>
<ul>
<li>评估模型带来的收益相对比较复杂，直观地说，模型为内部查找提供的好处由 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mi>b</mi></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">T_b−T_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 给出，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">T_b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">T_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 分别是基线和模型路径中查找的平均时间。如果一个文件在生命周期中服务了 N 个查询请求，那么该模型的净收益即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>T</mi><mi>b</mi></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mi>m</mi></msub><mo>)</mo><mo>∗</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">(T_b−T_m) * N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>。作者将查询操作又划分成了 negative 和 positive，因为大多数 negative 的查询操作在 filter 处就终止了，所以最终的收益模型为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mo>(</mo><mo>(</mo><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub><mo>)</mo><mi mathvariant="normal">∗</mi><msub><mi>N</mi><mi>n</mi></msub><mo>)</mo><mo>+</mo><mo>(</mo><mo>(</mo><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub><mo>)</mo><mi mathvariant="normal">∗</mi><msub><mi>N</mi><mi>p</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">B_{model} = ((T_{n.b} −T_{n.m}) ∗N_n)+((T_{p.b} −T_{p.m}) ∗N_p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">N_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">N_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 则是 negative lookup 和 positive lookup 的数量，T 为对应分类下的时间。</li>
<li>如果不知道文件将执行的查找次数或查找将花费的时间，则无法计算文件的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。因此分析器为了预估这些指标，维护了这些文件在他们生命周期内的统计信息，为了估计文件 F 的这些指标，分析器使用与 F 处于同一级别的其他文件的统计数据，我们只在同一层次上考虑统计数据，因为这些统计数据在不同层次上差异很大。</li>
<li>Bourbon 在学习一个文件之前的等待时间里，查询操作可能在 baseline 的路劲中被服务处理，Bourbon 将把该过程的处理时间作为基线的查询处理时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{n.b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{p.b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{n.m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{p.m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 将使用同一层级的其他文件的平均模型查询时间来进行估计。对于 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">N_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">N_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>，分析器首先获取该级别中其他文件的平均 negative 查找和 positive 查找，然后，将其按 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>=</mo><mi>s</mi><mi mathvariant="normal">/</mi><msub><mi>s</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">f = s/s_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的倍数进行缩放，其中 s 是文件的大小，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>s</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">s_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是该层级的文件的平均大小。在估计上述数量的时候，Bourbon 将会过滤掉寿命较短的文件。</li>
<li>当模型开始引导时，分析器可能还没有足够的统计信息，所以，初始化的时候，Bourbon 以 always-learn 的模式来运行，一旦足够的统计信息收集到了之后，分析器就可以开始执行开销和收益的权衡，来判断 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的大小关系来决定是否学习某一个文件。如果同时选中了多个文件进行学习，Bourbon 则把多个文件放在一个最大优先级队列中，按照 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>−</mo><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model} - C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的顺序进行排序，因此优先级最高的文件对应的收益也就最大。</li>
</ul>
<h4 id="bourbon-putting-it-all-together">Bourbon: Putting it All Together</h4>
<ul>
<li>总体的流程如下图所示，主要分为两条路径：model exist 和 no model 对应的 baseline。baseline 和前文描述的 LevelDB 的检索方式基本相同，只是此处采用了 WiscKey 的方式来布局。</li>
<li>对于学习索引的查询方式，步骤如下：
<ul>
<li>step1. FindFiles：因为使用了文件学习，所以该步骤需要执行，即找到候选的 SSTables 文件</li>
<li>step2. LoadIB+FB：BOURBON 加载了 filter 和 index block，这些块可能已经被缓存在内存中了</li>
<li>step3. Model Lookup：FB： BOURBON 在候选的 SSTables 文件中对要查询的 Key 进行检索，模型相应地输出键 k 对应的文件内偏移 pos 和误差边界 δ。然后 BOURBON 计算包含记录 pos−δ 到 pos+δ 的数据块</li>
<li>step4. SearchFB：首先检查该数据块对应的 filter block 来判断 k 是否存在，如果存在，则 BOURBON 计算要加载的块对应的字节范围（因为 keys 和 pointers 大小固定，计算相对简单）</li>
<li>step5. LoadChunk：加载对应的字节范围</li>
<li>step6. LocateKey：键位于加载的块中，那么该 key 将位于预测的位置上（加载的 chunk 的中点）；如果不在，BOURBON 将会对该 chunk 执行二分查找</li>
<li>step7. ReadValue：使用相应的 Value Pointer 从 ValueLog 中读取对应的 Value<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107165512.png" alt="20201107165512" loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="possible-improvements">Possible improvements</h5>
<ul>
<li>BOURBON 少了一些 features，现有的实现中，不支持字符串 keys 以及 keys 的压缩
<ul>
<li>对于字符串键，我们计划探索的一种方法是将字符串视为base-64整数，并将它们转换为64位整数，然后可以采用本文描述的相同学习方法。虽然这种方法可以很好地用于小 keys，但是大 keys 可能需要更大的整数(大于64位)，因此高效的大整数数学可能是必不可少的。</li>
</ul>
</li>
<li>BOURBON 暂时不支持 level 和 file model 的切换，目前只是一个静态配置。</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>测试之前抛出了几个关键问题：
<ul>
<li>BOURBON 究竟优化了查询的哪些部分？</li>
<li>BOURBON 在有模型但没有写的情况下表现如何？性能会随着数据集、加载顺序以及请求分布的变化发生什么样的变化？</li>
<li>BOURBON 范围查询表现如何？</li>
<li>在有写的情况下，BOURBON 的开销-收益分析器和其他从不重新学习的方法相比表现如何？</li>
<li>BOURBON 在真实负载下的性能也能表现得和预期一致吗？</li>
<li>数据存储在存储设备上时（不再在内存） BOURBON 是否有用？</li>
<li>有限的内存的情况下，BOURBON 是否有用？</li>
<li>BOURBON 在误差范围和空间开销上的权衡是怎么样的？</li>
</ul>
</li>
</ul>
<h3 id="测试环境">测试环境</h3>
<h4 id="硬件环境">硬件环境</h4>
<ul>
<li>20-core Intel Xeon CPU E5-2660</li>
<li>160-GB memory</li>
<li>a 480GB SATA SSD</li>
</ul>
<h4 id="系统参数">系统参数</h4>
<ul>
<li>16B integer keys and 64B values</li>
<li>error bound - 8</li>
<li>Unless specified, our workloads perform 10M operations.</li>
</ul>
<h4 id="负载">负载</h4>
<ul>
<li>构造了四个合成数据集，64M key-value pairs
<ul>
<li>linear, 键都是连续的</li>
<li>segmented-1%, 在连续的100个键之后有一个间隙</li>
<li>segmented-10% , 在10个连续的键之后会有一个间隙</li>
<li>normal，从标准正态分布 N(0,1) 中抽样 64M 个唯一值，并按比例缩放到整数</li>
</ul>
</li>
<li>真实负载：Amazon reviews (AR) &amp; New York OpenStreetMaps (OSM)</li>
</ul>
<h3 id="测试结果">测试结果</h3>
<ul>
<li>
<p><strong>BOURBON 究竟优化了查询的哪些部分？</strong></p>
<ul>
<li>减少了索引花费的时间。图中标记为 Search 的部分对应基线中的 SearchIB 和 SearchDB</li>
<li>还降低了数据访问成本，因为 BOURBON 加载的字节范围比基线加载的整个块要小。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107172404.png" alt="20201107172404" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>BOURBON 在有模型但没有写的情况下表现如何？性能会随着数据集、加载顺序以及请求分布的变化发生什么样的变化？</strong> 无论哪种情况 BOURBON 都可以提供显著的加速</p>
<ul>
<li>对于所有数据集， BOURBON 比 WiscKey 更快；根据数据集的不同，提升也不同(1.23倍到1.78倍)。
<ul>
<li>BOURBON 对线性数据集提升最大，因为它有最小的片段数(每个模型一个)；使用更少的段，找到目标需要检索的段也就更少。</li>
<li>延迟随着段数的增加而增加</li>
</ul>
</li>
<li>level learning 适用于只读负载，BOURBON-level 比基线快1.33 - 1.92倍，比 BOURBON 更好，因为 level 学习查找对应的 SSTables 更快。
<ul>
<li>由于 level 模型只对只读工作负载提供好处，并且与文件模型相比最多提高10%，所以后续测试主要针对 file learning。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107172755.png" alt="20201107172755" loading="lazy"></li>
</ul>
</li>
<li>负载加载顺序的差异，使用顺序加载，sstable 甚至在不同的级别上都不会有重叠的键范围;然而，在随机加载的情况下，某个级别的 sstable 可能会与其他级别的 sstable 重叠。
<ul>
<li>无论负载顺序如何，BOURBON 都比基线有显著的优势（1.47× – 1.61×）。</li>
<li>与顺序加载情况相比，随机加载情况下的平均查找延迟有所增加。这是因为，虽然在顺序情况下没有 negative 的内部查找（10M），但在随机情况下有很多 negative 的查找（23M）</li>
<li>在随机情况下，对基线的加速比顺序情况下要小。虽然 BOURBON 同时优化了 positive 查找和 negative 查找，但 negative 查找的收益较小，因为 negative 的查询路径比较短，在 filter 处可能就终止了，没有加载或搜索数据块，而且 negative 的查询比 positive 的查询多。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107173423.png" alt="20201107173423" loading="lazy"></li>
</ul>
</li>
<li>请求的分布情况，测试了六种请求分布下的延迟，sequential, zipfian, hotspot, exponential, uniform, and latest。
<ul>
<li>BOURBON 使查找速度比基线快1.54倍- 1.76倍。总的来说，无论请求分布如何，BOURBON 都减少了延迟。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107174316.png" alt="20201107174316" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>BOURBON 范围查询表现如何？</strong></p>
<ul>
<li>如图展示了 BOURBON 的吞吐量标准化到了 WiscKey 之后的结果。对于较短的范围，索引开销(即查找范围的第一个键的开销)占主导地位，BOURBON 优化效果比较明显，但随着范围的增大，其效果就不再那么明显，这是因为BOURBON 可以加速索引部分，但它遵循与 WiscKey 类似的路径来扫描后续键。因此，在大范围查询时，索引查询占较少的总性能，性能提升就不明显了。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107174537.png" alt="20201107174537" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>在有写的情况下，BOURBON 的开销-收益分析器和其他从不重新学习的方法相比表现如何？</strong></p>
<ul>
<li>首先了解三种策略：
<ul>
<li>BOURBON-offline 在写发生时不执行学习，模型仅针对最初加载的数据而存在</li>
<li>BOURBON-always 只要写发生了就会重新学习，不考虑成本</li>
<li>BOURBON-cba 会使用开销-收益分析器来决定是否学习</li>
</ul>
</li>
<li>图 a 显示了在前端查询和插入花费的时间，图 b 显示了学习过程花费的时间，图 c 显示了总的时间开销，图 d 显示了采用基线路径的内部查找的比例</li>
<li>结果表明：
<ul>
<li>所有的策略都相比于 WiscKey 减少了前台任务的时间开销，随着写比例的增加，查询比例的减小，优化的效果就减弱。offline 的策略表现最差，即便是在只有 1% 的写的情况下，这时候大多都是通过基线的索引查询方式（如图 d 所示的比例）来进行，所以对于数据改变之后的重新学习十分关键</li>
<li>BOURBON-always 在前台任务的时间开销上表现最好，几乎不会退化成基线的查询方式，但是对应的学习时间就特别长，在 50% 时候就大概花费 134s 进行学习，所以总的时间开销当写请求的比例较大的时候，可能比基线的时间开销还大</li>
<li>写请求比例较低的时候，BOURBON-cba 几乎会学习所有的文件，所以此时和 always 的表现比较相近，当写请求比例增大时，BOURBON-cba 不再学习特别多的文件，学习时间开销只有 13.9s，相比于 always 大大减小，因此这时候的很多查询都是按照基线对应的路径，因为此时数据变化迅速，查找次数较少，学习的收益较小。</li>
</ul>
</li>
<li>总结：积极学习策略提供快速查找，但代价高昂；没有 re-learning 的话几乎不能加快速度。也不理想。相比之下，BOURBON 提供了与积极学习类似的高收益，同时显著降低了总成本<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107183741.png" alt="20201107183741" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>BOURBON 在真实负载下的性能也能表现得和预期一致吗？</strong></p>
<ul>
<li>YCSB：BOURBON 提高了读操作的性能;同时，波旁威士忌并不影响写的表现。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107185902.png" alt="20201107185902" loading="lazy"></li>
<li>SOSD<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107185921.png" alt="20201107185921" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>数据存储在存储设备上时（不再在内存） BOURBON 是否有用？</strong></p>
<ul>
<li>即使数据存在存储设备上，BOURBON 也有一定程度的提升(查询速度比WiscKey快1.25倍到1.28倍)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107191601.png" alt="20201107191601" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107191738.png" alt="20201107191738" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>有限的内存的情况下，BOURBON 是否有用？</strong></p>
<ul>
<li>如下表所示，使用了 SATA SSD 和大小只有数据量的 25% 的内存，BOURBON 速度只有 WiscKey 的 1.04 倍，因为大部分时间都花在了将数据加载到内存上。</li>
<li>相比之下，在 zipfian 工作负载中，索引时间（而不是数据访问时间）占主导地位，因为大量请求访问已经缓存在内存中的一小部分数据，所以能够提供 1.25x 的加速和低得多的延迟。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107191910.png" alt="20201107191910" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>BOURBON 在误差范围和空间开销上的权衡是怎么样的？</strong></p>
<ul>
<li>随着误差范围增大，更少的线段被创建，从而需要检索的线段就更少，延迟也就相应减小，然而当 δ 超过 8，尽管需要检索的分段更少，但是延迟增加了，对于其他数据集也是 δ = 8 是转折点。还显示了空间开销，因为创建的线段更少了，空间开销也就更小了。</li>
<li>对于各种数据集，开销与数据集的总大小相比是很小的(0% - 2%)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107192311.png" alt="20201107192311" loading="lazy"></li>
</ul>
</li>
</ul>
<h2 id="related-work">Related Work</h2>
<ul>
<li><strong>Learned Index</strong>:
<ul>
<li>XIndex</li>
<li>FITingTree</li>
<li>AIDEL</li>
<li>Alex</li>
<li>SageDB</li>
</ul>
</li>
<li><strong>LSM optimizations</strong>:
<ul>
<li>Monkey</li>
<li>Dostoevsky</li>
<li>HyperLevelDB</li>
<li>bLSM</li>
<li>cLSM</li>
<li>RocksDB</li>
</ul>
</li>
<li><strong>Model choices</strong>:
<ul>
<li>Greedy-PLR</li>
<li>Neural networks</li>
<li>one-pass learning algorithm based on splines</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[intelligent cache research]]></title>
        <id>https://blog.shunzi.tech/post/intelligent-cache-research/</id>
        <link href="https://blog.shunzi.tech/post/intelligent-cache-research/">
        </link>
        <updated>2020-10-25T15:59:49.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>本篇为智能缓存相关的研究调研，可能涉及 AI for System 以及相关缓存策略的设计</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>本篇为智能缓存相关的研究调研，可能涉及 AI for System 以及相关缓存策略的设计</li>
</ul>
</blockquote>
 <!-- more -->
<h2 id="smart-ssd-cache">Smart SSD Cache</h2>
<blockquote>
<ul>
<li>智能缓存顾名思义为不同的负载下提供相应的缓存策略来保证缓存的高效，主要包含多种缓存策略执行和 IO 负载的捕捉两个方面，同时可能结合不同的缓存层级需要进行动态调整。</li>
<li>缓存策略本身很难有普适的，现有做法和方案常常需要根据 IO 负载来进行决策，而对于 IO 负载的建模往往采用统计或者机器学习的方法</li>
</ul>
</blockquote>
<h3 id="oceanstor-v5-系列-v500r007-smartcache">OceanStor V5 系列 V500R007 SmartCache</h3>
<ul>
<li>参考 https://support.huawei.com/enterprise/zh/doc/EDOC1000181455/1064ba78</li>
</ul>
<h4 id="定义">定义</h4>
<ul>
<li>华为技术有限公司开发的 SmartCache 特性又叫智能数据缓存特性。</li>
<li><strong>缓存池化</strong>：利用 SSD 盘对随机小I/O读取速度快的特点，将 SSD 盘组成智能缓存池，将访问频率高的随机小I/O读热点数据从传统的机械硬盘移动到由 SSD 盘组成的高速智能缓存池中。由于 SSD 盘的数据读取速度远远高于机械硬盘，所以 SmartCache 特性可以缩短热点数据的响应时间，从而提升系统的性能。</li>
<li><strong>多粒度</strong>：SmartCache 将智能缓存池划分成多个分区，为业务提供细粒度的SSD缓存资源，不同的业务可以共享同一个分区，也可以分别使用不同的分区，各个分区之间互不影响。从而向关键应用提供更多的缓存资源，保障关键应用的性能。</li>
</ul>
<h4 id="场景">场景</h4>
<ul>
<li>SmartCache特性对LUN（块业务）和文件系统（文件业务）均有效。</li>
<li>SmartCache特性可以提高业务的<strong>读性能</strong>。尤其是存在热点数据，且读操作多于写操作的随机小I/O业务场景。例如：OLTP（Online Transaction Processing ）应用、数据库、Web服务、文件服务应用等。</li>
</ul>
<h4 id="原理">原理</h4>
<ul>
<li>SmartCache特性在对SSD盘资源进行管理上，分为智能缓存池和SmartCache分区两部分。</li>
</ul>
<h5 id="智能缓存池">智能缓存池</h5>
<ul>
<li>智能缓存池管理本控制器的所有SSD盘，用以保证每个智能缓存分区的资源来自不同SSD盘，从而避免不同SSD盘负载不均衡。</li>
<li>存储系统默认在每个控制器上生成一个智能缓存池。</li>
</ul>
<h5 id="读流程">读流程</h5>
<ul>
<li>SSD Cache 缓存命中<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021162117.png" alt="20201021162117" loading="lazy"></li>
<li>SmartCache读未命中
<ul>
<li>当从 HDD 读取到 RAM Cache 后，RAM Cache 将数据返回给应用服务器，同时 RAM Cache 将该数据同步到智能缓存池中。当智能缓存池容量不够时，则智能缓存池根据时间顺序释放旧数据，释放数据内存，完成旧数据的淘汰。</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021162235.png" alt="20201021162235" loading="lazy"></figure>
<h5 id="smartcache分区">SmartCache分区</h5>
<ul>
<li>SmartCache分区负责为业务提供细粒度（4KB、8KB、16KB、32KB、64KB、128KB，与前端I/O自适应，即根据前端下发的I/O大小申请不同粒度的SSD缓存资源）的SSD缓存资源。</li>
<li>每两个控制器创建一个默认的SmartCache分区。除了默认分区外，每两个控制器最多支持创建8个用户自定义分区。</li>
<li>通过SmartCache分区调控，各业务独立使用所分配的SmartCache分区，避免不同类型应用之间的相互影响，保障存储系统整体的服务质量。可以通过设置SmartCache大小，实现不同业务与性能的最佳匹配。通过限制非关键应用的缓存资源，向关键应用提供更多的缓存资源，保障关键应用的性能。</li>
<li>SmartCache分区负责为业务提供细粒度的SSD盘缓存资源，不同的业务可以共享同一个分区，也可以分别使用不同的分区，各个分区之间互不影响。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021162705.png" alt="20201021162705" loading="lazy"></figure>
<h3 id="machine-learning">Machine Learning</h3>
<h4 id="date20-a-machine-learning-based-write-policy-for-ssd-cache-in-cloud-block-storage">DATE20 - A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage</h4>
<ul>
<li><strong>发现</strong>：作者在腾讯云的云存储环境中（主要是块存储系统），测试统计发现大约有 47.9% 的写操作是 write-only 的，即在某一个确定的时间窗口内所写的这些块不会再次被访问。那么这些 write-only 的写操作对应的数据放置在缓存中其实并不会带来性能的提升，反而会占据缓存容量，影响其他真正需要缓存的数据来进行缓存。</li>
<li><strong>问题</strong>：现有的写策略大致分为两种：
<ul>
<li>将所有要写的数据加载到缓存中（write-back 和 write through）
<ul>
<li>write-back：在数据更新时只写入缓存Cache。只在数据被替换出缓存时，被修改的缓存数据才会被写到后端存储。此模式的优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回。</li>
<li>Write-through（直写模式）在数据更新时，同时写入缓存Cache和后端存储。此模式的优点是操作简单；缺点是因为数据修改需要同时写入存储，数据写入速度较慢。</li>
</ul>
</li>
<li>不加载数据到缓存，直接写存储设备 （write-around）</li>
</ul>
</li>
<li><strong>方案</strong>：提出了一种基于机器学习的方法来识别 write-only 数据和 normal 数据，针对不同类型的数据动态应用不同的写策略。最大的挑战是怎么样才能够实时地区分数据类型<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200714201124.png" alt="image" loading="lazy"></li>
<li>使用了五种监督学习方法：Naive Bayes, Logistic Regression, Decision Tree, AdaBoost, and Random Forest。随机森林准确率最高，但是耗时最长，不满足实时性，朴素贝叶斯在准确率、召回率和预测时间上基本做到了 trade-off，所以最终选择了朴素贝叶斯。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master/img/blog/20200714201440.png" alt="image" loading="lazy"></li>
<li><strong>数据指标的选择</strong>：不同于在文件级别或者片上系统的基于机器学习的优化，在块级系统中部署机器学习算法来分类不同的数据面临很大的障碍，因为块级能提供的信息是有限的，常用的信息包括时间特征和空间特征（如最近访问时间和最近访问的地址）。本文中我们扩展了数据指标到 IO 请求，收集了如 average write size, write request ratio 等，这是由于具有类似请求级别的数据往往表现出类似的访问特征
<ul>
<li>Temporal features：这些特性包括数据块的访问近因和时间间隔。
<ul>
<li>Last Access Timestamp 在本文中被定义成了当前时间和最近访问该块的时间间隔。</li>
<li>Average Re-access Time Difference：对一个数据块进行两个相邻访问的时间间隔</li>
</ul>
</li>
<li>Spatial features：这些特性包含地址信息，例如卷 ID、偏移量等
<ul>
<li>因为地址会随着时间不断变化，对算法的效果会有显著影响，在作者实现的算法中其实没有使用空间特征。</li>
</ul>
</li>
<li>Request-level features
<ul>
<li>Average Request Size 平均请求大小，单位 KB，上界为 100KB</li>
<li>Big Request Ratio 大请求比例，请求大小大于 64KB 即为大请求</li>
<li>Small Request Ratio 小请求比例，请求小于 8KB 即为小请求</li>
<li>Write Request Ratio 写请求的比例</li>
</ul>
</li>
</ul>
</li>
<li><strong>统计粒度</strong>： 块设备的最小粒度为一个块，假设一个块为 8KB，如果我们为每一个块都统计对应的数据特征，因为块设备容量很大，那么统计这些特征就会造成巨大的开销，同时太大可能影响算法的准确率，所以实际应用过程中，作者采用了 1MB 为统计粒度（称之为 tablet），1MB 连续的数据块中所包含的最小物理块对应的数据特征相同，从而减少统计的开销。</li>
<li>整个模型的工作流程如下：
<ul>
<li>写请求的所有数据被首先被写入到内存中并顺序地记录在日志文件中，然后当脏数据的比例达到阈值则刷回到后端存储服务器，当在内存写缓冲中要执行对应的刷回操作时，进行下一步</li>
<li>分类器获取最近刷回的数据块的 tablet 特征并预测其写请求类型，如果是 write-only 跳转到下一步，如果不是（即为 normal data）跳转到第四步</li>
<li>如果该数据位于 SSD Cache 中且为脏数据，那么首先将该脏数据刷回到 HDD。对应的数据块将直接在 HDD 上进行写</li>
<li>数据块首先写入到 SSD 缓存然后使用 write-back 的策略异步刷回 HDD</li>
</ul>
</li>
<li>分类器每天训练一次用于第二天的预测，系统运行过程中收集样本数据，当 SSD cache 发生 eviction 的时候将增加一个样本，如果 evicted-data 有一次或者多次读命中，该样本将被设置为 0，也就是 normal data，否则设置为 1，write-only data。在作者的想法中，真实的实现里，write-only 的数据块是指一个在从缓存中 evict 之前不会被读的块</li>
<li><strong>效果</strong>：实验结果表明，与业界广泛部署的回写策略相比，ML-WP 减少了对 SSD 缓存的写流量 41.52%，同时提高了 2.61% 的命中率，降低了 37.52% 的平均读延迟</li>
</ul>
<h5 id="related-work">Related Work</h5>
<ul>
<li>
<p><strong>写策略优化</strong></p>
<ul>
<li>ATC15 - Request-oriented durable write caching for application performance
<ul>
<li>分析IO工作负载（判断是否会发生争用），然后部署最合适的写策略，使用了固定的写策略</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Cache Optimization Based on Machine Learning</strong>:</p>
<ul>
<li>MICRO16 Perceptron learning for reuse prediction
<ul>
<li>感知器学习用于判断 last-level cache 的重用预测 （神经网络做预测）</li>
<li>通过使用多个能够展示程序和存储器行为的特征（七个参数化特征）来从多个视角对LLC中的缓存块的未来重用进行预测。利用预测的结果，设计三种cache的管理策略：block placement，replacement，bypass</li>
<li>PC, address, reference count, etc</li>
</ul>
</li>
<li>ICPP18 Efficient ssd caching by avoiding unnecessary writes using machine learning
<ul>
<li>使用机器学习（决策树）来进行一次访问排除，能够准确地识别和过滤一次访问的照片，并阻止它们写入cache，提高社交网络照片缓存服务的效率。更多的使用照片的相关数据以及用户的登录请求数据来作为数据集</li>
<li>使用 reaccess distance 来定义是否为只访问一次的图像，被定义为从该图像进入cache 开始到下一次被访问之间的总的图像访问量</li>
<li>命中率提高了17%，缓存写操作降低了79%，平均访问延迟降低了 7.5%</li>
</ul>
</li>
<li>NSDI19 Flashield: a hybrid key-value cache that controls flash write amplification
<ul>
<li>由于键值更新会对 SSD 造成严重的有害的小随机I/O写操作，因此使用机器学习（支持向量机 SVM）的方法来选择期望读取频率更高的对象</li>
<li>Flashield这里引入了Flashiness的概念，作为一个对象被Cache价值的一个评价指标，一个高Cache价值的对象要满足两点条件
<ul>
<li>一个对象在访问之后会在不远的将来访问n次(及以上)，这里的n作为参数定义，</li>
<li>在将来的一段时间内不会被修改<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021172230.png" alt="20201021172230" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="new-cache-policy">New Cache Policy</h3>
<h4 id="tpds20-efficient-ssd-cache-for-cloud-block-storage-via-leveraging-block-reuse-distances">TPDS20 - Efficient SSD Cache for Cloud Block Storage via Leveraging Block Reuse Distances</h4>
<ul>
<li>
<p><strong>发现</strong>：我们在一个典型云块存储的服务器端发现了这一点，有很大比例的块具有很大的重用距离，这意味着很多块只有在遥远的将来才会被重新引用。在这样的场景中，如果在每次 miss 时进行简单的替换，新访问的块会污染缓存，而不会给命中率带来任何好处，从而降低缓存效率。现有的缓存算法在存在较大的重用距离时，缓存效率往往不理想</p>
<ul>
<li>如下图所示，A 点意味着 CBS 访问中，有大约 50% 的块的重用距离大于 32GB，而对于内存和主机上的 IO 则要小得多</li>
<li>因此，如果我们使用像LRU这样的缓存算法，当缓存大小等于或小于 32gb 时，50% 的重用块不会被命中<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021174825.png" alt="20201021174825" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>现象分析</strong>：</p>
<ul>
<li>多数具有小重用距离的数据块已经被客户端缓存设备缓存和过滤。</li>
<li>单个云磁盘的容量可以达到几十TB，这比传统磁盘高出几个数量级。因此，数据规模较大的云应用程序可能导致更大的重用距离</li>
<li>在多租户共享云环境中，来自不同租户的访问会混合在一起，导致重用距离增加</li>
</ul>
</li>
<li>
<p><strong>贡献</strong>：我们提出了一种新的缓存算法，专为 CBS 和其他类似场景设计的 LEA，LEA在缓存未命中时默认不进行替换，除非满足某些(惰性)条件。条件包含两个方面：新访问块的频率 和 缓存中的候选逐出块的值（该值是指该块对缓存的重要性或有用性）。这样，块的缓存持续时间可以大大延长。更重要的是，可以大大减少对 SSD 的写操作，延长 SSD 的生命周期</p>
</li>
<li>
<p><strong>Reuse Distance</strong>：数据块的重用距离定义为对同一块的两个连续引用之间的唯一数据量。例如 1-2-4-5-4-3-3-2 的块访问序列中，Block 2 的 reuse distance 就是三个块的大小，在有的研究中，Block 2 的 reuse distance 为 5 个块的大小。本文采用了第二种定义。</p>
</li>
<li>
<p>传统的 LRU 当缓存空间满时，对每次miss进行替换，主要遵循时间局部性原则，这些缓存替换算法的主要目标是保留重用距离较小的块，驱逐重用距离较大的块。当大部分数据重用距离小于缓存大小时，这些算法可以有效地工作。</p>
</li>
</ul>
<h5 id="lea-algorithm">LEA Algorithm</h5>
<ul>
<li>维护两个队列，Lazy Eviction List (LEL) 和 Block Identity List (BIL)。每个队列有两个端点 Insertion Point (IP) 和 Eviction Point (EP)
<ul>
<li>标识只包括块的卷号和地址信息(偏移量)。一个条目包含额外的块信息，如 last_access, reuse_distance, age, flag，等等。条目中的信息用于计算条目块的值。</li>
<li>age 表示块的当前时间和最后一次访问时间(last_access)之间的时间差，如果 age 小于 reuse_distance，它表明该块对缓存具有较高的价值。这里的时间是逻辑时间，当一个新的块请求到来时，逻辑时间增加1。这里，重用距离是最后两次访问之间的时间(而不是访问之间的平均时间)。</li>
<li>flag 也用来判断块的值，当块在 LEL 列表中命中时增加 1，当块被视为块驱逐候选，但由于懒惰替换还没有被驱逐时减少一半，如果 flag 大于 0，它表明该块对于缓存是有价值的。</li>
</ul>
</li>
<li>当发生缓存 miss 时，LEA 不进行替换，只在满足延迟条件时将丢失的块标识插入 BIL 列表，否则，它将执行替换并将丢失的块条目插入到 LEL 列表中。如果在 BIL 列表中命中了一个块，那么它进入LEL列表的概率更高。通过这样做，间接地考虑了访问频率<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021191310.png" alt="20201021191310" loading="lazy"></li>
</ul>
<h4 id="atc20-osca-an-online-model-based-cache-allocation-scheme-in-cloud-block-storage-systems">ATC20 - OSCA: An Online-Model Based Cache Allocation Scheme in Cloud Block Storage Systems</h4>
<ul>
<li>OSCA 可以在非常低的复杂度下找到接近最佳的配置方案，从而提高缓存服务器的总体效率
<ul>
<li>部署了一个新的缓存模型来获得云基础设施块存储系统中每个存储节点的 miss ratio curve (MRC)。使用一种低开销的方法来获得一个时间窗口内从重新访问流量与总流量之比的数据重用距离。然后将得到的重用距离分布转化为 miss ratio curve (MRC)。</li>
<li>通过了解存储节点的缓存需求，将总命中流量指标定义为优化目标</li>
<li>使用动态规划方法搜索接近最优的配置，并基于此解进行缓存重新分配</li>
</ul>
</li>
<li>在实际工作负载下的实验结果表明，模型达到了一个平均值绝对误差(MAE)，可与现有的最先进的技术相媲美，但同时可以做到没有跟踪收集和处理的开销。由于命中率的提高，相对于在相同缓存内存的情况下对所有实例的等分配策略，OSCA 减少了到后端存储服务器的 IO 流量 13.2%<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021194032.png" alt="20201021194032" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ceph FS 介绍和使用]]></title>
        <id>https://blog.shunzi.tech/post/CephFS/</id>
        <link href="https://blog.shunzi.tech/post/CephFS/">
        </link>
        <updated>2020-10-14T12:32:20.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>一个项目测试使用到了 CephFS，故简要整理 CephFS 资料和相关文档</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>一个项目测试使用到了 CephFS，故简要整理 CephFS 资料和相关文档</li>
</ul>
</blockquote>
<!--more-->
<h2 id="cephfs">CephFS</h2>
<ul>
<li><a href="https://github.com/zjs1224522500/BlogIssue/issues/18">Ceph 常用命令</a></li>
<li><a href="https://chaosd.github.io/2020/08/17/Deploy%20a%20Ceph%20Cluster%20Manually/">Deploy a Ceph Cluster Manually</a></li>
</ul>
<h3 id="overview">Overview</h3>
<ul>
<li>CephFS 应用相比于 RBD/RGW 不够广泛主要是因为文件系统采用树状结构管理数据（文件和目录）、基于查表寻址的设计理念与 Ceph 扁平化的数据管理方式、基于计算进行寻址的设计理念有些违背；其次文件系统的支持常常需要集中的元数据管理服务器来作为树状结构的统一入口，这又与 Ceph 去中心化、追求近乎无限的横向扩展能力的设计思想冲突。</li>
<li>由于分布式文件系统的需求仍旧很大，应用场景尤为广泛，在 Ceph 不断的版本迭代中，CephFS 也取得了越来越好的支持。</li>
</ul>
<h3 id="背景">背景</h3>
<ul>
<li>要想实现分布式文件系统，那么就必须实现分布式文件系统的特点，即具有良好的横向扩展性，性能能够随着存储规模呈线性增长，为了实现这样的目标则需要对文件系统命名空间分而治之，即实现相应的负荷分担和负载均衡，采用相应的数据路由算法。</li>
</ul>
<h4 id="文件系统数据负载均衡分区">文件系统数据负载均衡分区</h4>
<h5 id="静态子树分区">静态子树分区</h5>
<ul>
<li>手工分区，数据直接分配到某个固定的服务节点，负载不均衡时再手动调整。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201013150928.png" alt="20201013150928" loading="lazy"></li>
</ul>
<h5 id="hash-计算分区">HASH 计算分区</h5>
<ul>
<li>HASH 计算数据的存储位置，保证了数据分布的均衡，但如果环境变化（集群规模变化）此时需要固定原有的数据分区而减少数据的迁移，或者根据元数据的访问频率，要想保证 MDS 负载均衡，需要重新决定元数据的分布，此时则不适合使用 HASH</li>
</ul>
<h5 id="动态子树分区">动态子树分区</h5>
<ul>
<li>通过实时监控集群节点的负载，动态调整子树分布于不同的节点。这种方式适合各种异常场景，能根据负载的情况，动态的调整数据分布，不过如果大量数据的迁移肯定会导致业务抖动，影响性能。在元数据存储、流量控制和灵活的资源利用策略方面，动态分区比其他技术有许多优势。</li>
<li>https://ceph.com/wp-content/uploads/2016/08/weil-mds-sc04.pdf<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201013151347.png" alt="20201013151347" loading="lazy"></li>
<li>动态子树分区方法的核心是将文件系统作为层次结构处理。通过将层次结构的子树的权限委托给不同的元数据服务器，对文件系统进行分区。委托可以嵌套:例如，/usr可以分配给一个MDS，而/usr/local可以分配给另一个MDS。但是，在没有显式分配子树的情况下，嵌套在某个点下的整个目录树被假定驻留在同一台服务器上。</li>
<li>这个结构中隐含着层次结构遍历的过程，以便找到并打开嵌套的索引节点，以便随后下降到文件层次结构中。这样的路径遍历对于验证POSIX语义所要求的嵌套项的用户访问权限也是必要的，对于在目录层次结构深处定位一个文件来说，这个过程可能代价很高。</li>
<li>为了允许有效地处理客户机请求(以及正确响应它们所需的路径遍历)，每个MDS都缓存缓存中所有项的前缀索引节点，以便在任何时候缓存的层次结构子集仍然是树结构。也就是说，只有叶子项可以从缓存中过期;在目录中包含的项首先过期之前，不能删除目录。这允许对所有已知项进行权限验证，而不需要任何额外的I/O成本，并保持层次一致性。</li>
<li>为了适应文件系统发展和工作负载变化的要求，MDS集群必须调整目录分区，以保持工作负载的最佳分布。动态分布是必要的，因为层次结构部分的大小和流行度都以一种不均匀和不可预测的方式随时间变化。通过允许MDS节点传输目录层次结构的子树的权限，元数据分区会随着时间的推移进行修改。MDS节点定期交换心跳消息，其中包括对其当前负载级别的描述。此时，忙碌的节点可以识别层次结构中适当流行的部分，并发起一个双重提交事务，将权限传递给非繁忙节点。在此交换过程中，所有活动状态和缓存的元数据都被转移到新的权威节点，这既是为了保持一致性，也是为了避免磁盘I/O，否则，新权威节点将需要磁盘I/O来重新读取它，而磁盘I/O会慢上几个数量级。</li>
</ul>
<h4 id="cephfs-mds-特点">CephFS MDS 特点</h4>
<ul>
<li>采用多实例消除性能瓶颈并提升可靠性</li>
<li>采用大型日志文件和延迟删除日志机制提升元数据读写性能</li>
<li>讲 Inode 内嵌至 Dentry 中来提升文件索引率</li>
<li>采用目录分片重新定义命名空间层次结构，并且目录分片可以在 MDS 实例之间动态迁移，从而实现细粒度的流控和负载均衡机制</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200831094239.png" alt="20200831094239" loading="lazy"></figure>
<h3 id="架构">架构</h3>
<ul>
<li>
<p>虽然 Ceph 文件系统中的 inode 数据存储在 RADOS 中并由客户端直接访问，但是 inode 元数据和目录信息由Ceph metadata server (MDS)管理。MDS 充当所有与元数据相关的活动的中介，将结果信息存储在与文件数据不同的RADOS 池中。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200830135706.png" alt="20200830135706" loading="lazy"></p>
</li>
<li>
<p>CephFS中的所有文件数据都存储为RADOS对象。CephFS客户端可以直接访问RADOS对文件数据进行操作。MDS只处理元数据操作。</p>
</li>
<li>
<p>要读/写CephFS文件，客户端需要有相应inode的“文件读/写”功能。如果客户端没有需要的功能 caps，它发送一个“cap消息”给MDS，告诉MDS它想要什么。MDS将在可能的情况下向客户发布功能 caps。一旦客户端有了“文件读/写”功能，它就可以直接访问 RADOS 来读/写文件数据。文件数据以 <inode number>, <object index> 的形式存储为RADOS对象。如果文件只由一个客户端打开，MDS还会向唯一的客户端提供“文件缓存/缓冲区”功能。“文件缓存”功能意味着客户端缓存可以满足文件读取要求。“文件缓冲区”功能意味着可以在客户端缓存中缓冲文件写。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200831094239.png" alt="20200831094239" loading="lazy"></p>
</li>
</ul>
<h4 id="cepgfs-client-访问示例">CepgFS Client 访问示例</h4>
<ul>
<li>Client 发送 open file 请求给 MDS</li>
<li>MDS 返回 file node, file size, capability 和 stripe 信息</li>
<li>Client 直接 READ/WRITE 数据到 OSDs（如果无 caps 信息需要先向 MDS 请求 caps）</li>
<li>MDS 管理 Client 对该 file 的 capabilities</li>
<li>Client 发送 close file 请求给 MDS，释放 file 的 capabilities，更新 file 的详细信息</li>
</ul>
<h4 id="mds-文件锁">MDS 文件锁</h4>
<ul>
<li>当客户机希望在 inode 上操作时，它将以各种方式查询 MDS，然后授予客户机一组功能。它们授予客户端以各种方式操作 inode 的权限。与其他网络文件系统(例如 NFS 或 SMB)的主要区别之一是，所授予的功能非常细粒度，多个客户机可能在同一个 inode 上拥有不同的功能。</li>
<li>CephFS 客户机可以请求MDS代表它获取或更改 inode 元数据，但是MDS还可以为每个 inode 授予客户机功能 (caps)</li>
</ul>
<pre><code class="language-C">/* generic cap bits */
#define CEPH_CAP_GSHARED     1  /* client can reads (s) */
#define CEPH_CAP_GEXCL       2  /* client can read and update (x) */
#define CEPH_CAP_GCACHE      4  /* (file) client can cache reads (c) */
#define CEPH_CAP_GRD         8  /* (file) client can read (r) */
#define CEPH_CAP_GWR        16  /* (file) client can write (w) */
#define CEPH_CAP_GBUFFER    32  /* (file) client can buffer writes (b) */
#define CEPH_CAP_GWREXTEND  64  /* (file) client can extend EOF (a) */
#define CEPH_CAP_GLAZYIO   128  /* (file) client can perform lazy io (l) */
</code></pre>
<ul>
<li>然后通过特定数量的位进行移位。这些表示 inode 的数据或元数据的一部分，在这些数据或元数据上被授予能力:</li>
</ul>
<pre><code class="language-C">/* per-lock shift */
#define CEPH_CAP_SAUTH      2 /* A */
#define CEPH_CAP_SLINK      4 /* L */
#define CEPH_CAP_SXATTR     6 /* X */
#define CEPH_CAP_SFILE      8 /* F */
</code></pre>
<ul>
<li>一个 Cap 授予客户端 缓存和操作与 inode 关联的部分数据或元数据的能力。当另一个客户机需要访问相同的信息时，MDS 将撤销该 cap，而客户机最终将返回该功能，以及 inode 元数据的更新版本(如果它在保留功能时对其进行了更改)。</li>
<li>客户机可以请求 cap，并且通常会获得这些 cap，但是当 MDS 面临竞争访问或内存压力时，这些 cap 可能会被 revoke。当一个 cap 被 revoke 时，客户端负责尽快返回它。未能及时这样做的客户端可能最终被阻塞并无法与集群通信。</li>
<li>由于缓存是分布式的，所以 MDS 必须非常小心，以确保没有客户机拥有可能与其他客户机的 cap 或它自己执行的操作发生冲突的 cap。这使得 cephfs 客户机比 NFS 这样的文件系统依赖于更大的缓存一致性，在 NFS 中，客户机可以缓存数据和元数据，而这些数据和元数据在服务器上已经更改了。</li>
<li>基于 caps，构建了 ceph 的分布式文件锁，分布式文件锁保证了多个客户端并发且细粒度访问同一文件、目录、文件系统，同时保证一致性、可靠性。ceph 实现的分布式文件系统锁，客户端可见部分是 caps，服务端可见部分包括 caps和各种 lock，每个类型的 lock 又有多种状态，根据客户端的请求、持有、释放情况，lock 转换自身状态，并和客户端同步 caps 信息。最终实现分布式锁的访问。</li>
</ul>
<h3 id="参考链接">参考链接</h3>
<ul>
<li><a href="https://www.jianshu.com/p/e7d79a0d5314">[1] CephFS 介绍及使用经验分享</a></li>
</ul>
]]></content>
    </entry>
</feed>