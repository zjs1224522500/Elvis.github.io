<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.shunzi.tech</id>
    <title>Elvis Zhang</title>
    <updated>2021-02-26T09:17:16.677Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.shunzi.tech"/>
    <link rel="self" href="https://blog.shunzi.tech/atom.xml"/>
    <subtitle>The easy way or the right way.</subtitle>
    <logo>https://blog.shunzi.tech/images/avatar.png</logo>
    <icon>https://blog.shunzi.tech/favicon.ico</icon>
    <rights>All rights reserved 2021, Elvis Zhang</rights>
    <entry>
        <title type="html"><![CDATA[Reading Group Notes]]></title>
        <id>https://blog.shunzi.tech/post/ReadingGroup/</id>
        <link href="https://blog.shunzi.tech/post/ReadingGroup/">
        </link>
        <updated>2021-02-19T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>参加 Systems Reading Group 记录的论文笔记。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>参加 Systems Reading Group 记录的论文笔记。</li>
</ul>
</blockquote>
<!--more-->
<h1 id="system-reading-group">System Reading Group</h1>
<ul>
<li>Group 介绍以及 Presentation 安排： https://learn-sys.github.io/cn/reading/</li>
<li>THU AOS 2020: http://os.cs.tsinghua.edu.cn/oscourse/AOS2020</li>
</ul>
<h2 id="week-1-operating-system">Week 1: Operating System</h2>
<h3 id="course-notes">Course Notes</h3>
<ul>
<li>Video: THU AOS P7 - P11</li>
</ul>
<h4 id="os-architecture-structure">OS Architecture &amp; Structure</h4>
<ul>
<li><strong>OS Structure</strong>:
<ul>
<li>Simple kernel</li>
<li>Monolithic kernel</li>
<li>Micro kernel</li>
<li>Exokernel</li>
<li>VMM(Virtual Machine Monitor), etc...</li>
</ul>
</li>
<li><strong>Monolithic kernel</strong>
<ul>
<li>UNIX Arch<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219163131.png" alt="20210219163131" loading="lazy"></li>
<li>Linux Arch<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219163153.png" alt="20210219163153" loading="lazy"></li>
</ul>
</li>
<li><strong>Micro kernel</strong>
<ul>
<li><strong>微内核：功能相对较少的内核，只提供某些核心功能。从而相比于单体内核，把很多单体内核中的事情放到用户空间去做，解耦了内核的各个 features，让整个系统的稳定性和灵活性得到了提升。但也就因为 IPC 的开销导致性能表现不尽如人意。</strong></li>
<li>Kernel with minimal features</li>
<li>Moves as much from the kernel into user space
<ul>
<li>Address spaces</li>
<li>Interprocess communication (IPC)</li>
<li>Scheduling</li>
</ul>
</li>
<li>Benefits
<ul>
<li>Flexibility</li>
<li>Safety</li>
<li>Modularity</li>
</ul>
</li>
<li>Detriments (Poor Performance)
<ul>
<li>Address spaces</li>
<li>Interprocess communication (IPC)</li>
<li>Scheduling</li>
</ul>
</li>
<li><strong>Mach</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165141.png" alt="20210219165141" loading="lazy"></li>
<li><strong>L4</strong> - Microkernel– L4Second generation microkernel
<ul>
<li>synchronous IPCs –&gt; async IPCs (like epoll in Linux)</li>
<li>smaller, Mach 3(330 KB) –&gt; L4 (12KB)</li>
<li>IPC security checks moved to user process</li>
<li>IPC is hardware dependent</li>
</ul>
</li>
</ul>
</li>
<li><strong>Exokernel</strong>
<ul>
<li><strong>Exokernel 要做的事情其实是把内核也近乎给 PASS 掉，尽可能减少抽象层次，允许应用程序直接访问硬件，而ExoKernel只负责保护和分配系统资源。说白了就是把硬件资源都直接交给应用程序自己来组织了，因为有大量的应用程序想要自己独立控制可管理硬件，而不需要你操作系统层面的过多干涉。</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165849.png" alt="20210219165849" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165610.png" alt="20210219165610" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="paper-1-the-multikernel-a-new-os-architecture-for-scalable-multicore-systems">Paper 1: The Multikernel: A new OS architecture for scalable multicore systems</h3>
<ul>
<li><strong>SOSP09</strong></li>
<li><strong>SIGOPS Hall of Fame Award 2020</strong></li>
</ul>
<h4 id="abstract">Abstract</h4>
<ul>
<li>普通计算机系统包含越来越多的处理器核心，并呈现出越来越多的架构权衡，包括内存层次结构、互连、指令集和变体，以及IO配置。 以前的高性能计算系统在特定情况下进行了扩展，但是现代客户机和服务器工作负载的动态特性，加上不可能针对所有工作负载和硬件变体静态地优化操作系统，对操作系统结构构成了严重的挑战。</li>
<li>我们认为，迎接未来多核硬件挑战的最好方法是拥抱机器的网络化本质，重新思考使用来自分布式系统的思想的操作系统架构。我们研究了一种新的操作系统结构，即 Multikernel，它将机器视为一个由独立核心组成的网络，假定在最低层次上没有核间共享，并将传统的操作系统功能转移到一个通过消息传递进行通信的分布式进程系统。</li>
<li>我们已经实现了一个多内核操作系统来证明这种方法是有前途的，并且我们描述了操作系统的传统的可伸缩性问题(如内存管理)是如何通过消息有效地重新解决的，以及如何利用分布式系统和网络的洞察力。在多核系统上对我们的原型的评估表明，即使在现在的机器上，多内核的性能也可以与传统的相媲美，并且可以更好地扩展以支持未来的硬件。</li>
</ul>
<h4 id="problems">Problems</h4>
<ul>
<li>随着不断变化的技术对摩尔定律的限制，处理器架构变得越来越多样化，且逐渐转向异构化，并向可扩展的架构发展，以适应高性能的应用。传统的单体操作系统在解决可伸缩性问题和针对不同硬件结构进行优化方面面临着巨大的挑战。
<ul>
<li>Systems are increasingly diverse</li>
<li>Cores are increasingly diverse</li>
<li>The interconnect matters</li>
<li>Messages cost less than shared memory</li>
<li>Cache coherence is not a panacea</li>
<li>Messages are getting easier</li>
</ul>
</li>
<li>本文作者尝试通过在内核之间使用显式消息传递和在内核之间复制内核状态来解决这个问题，而不是使用共享内存模型。他们的另一个主要目标是使这个操作系统与硬件无关，不针对任何机器架构。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210220221342.png" alt="20210220221342" loading="lazy"></li>
</ul>
<h4 id="contributions">Contributions</h4>
<ul>
<li>多内核操作系统的主要贡献嵌入在它们的三个设计原则中：
<ul>
<li>通过消息传递显式地实现内核间通信</li>
<li>使操作系统结构与硬件无关</li>
<li>在内核之间复制内核状态</li>
</ul>
</li>
<li>该系统侧重于非共享内存模型，通过消息的显式通信来维护缓存的一致性。基于以上原则，设计实现了 MultiKernel 原型 Barrelfish<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210224113033.png" alt="20210224113033" loading="lazy"></li>
</ul>
<h2 id="week-2-virtualization">Week 2: Virtualization</h2>
<h3 id="course-notes-2">Course Notes</h3>
<ul>
<li>Video
<ul>
<li>THU AOS P12 - P21</li>
<li>IPADS MOS P70 - P87</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MapReduce: Simplified Data Processing on Large Clusters]]></title>
        <id>https://blog.shunzi.tech/post/MapReduce/</id>
        <link href="https://blog.shunzi.tech/post/MapReduce/">
        </link>
        <updated>2021-02-08T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。</li>
</ul>
</blockquote>
<!--more-->
<h2 id="before-beginning">Before Beginning</h2>
<ul>
<li>为什么要读这篇论文呢？其实这篇文章之前也已经简单看过了，只是最近开始刷 MIT6.824，本来是想直接做相关 Lab 的，但是发现还是有整理不清楚的思路，觉得还是有必要回顾一下，那就多花点时间继续研读吧~</li>
<li>网上关于 6.824 以及 MapReduce 的资料很多了，我在这里只是做一些简单的记录，如果有发现其他大佬做的比较好的笔记，也会贴在这里，以供膜拜学习。我的 6.824 系列的的博客介绍大抵都是如此。</li>
<li>话不多说，学习开始~</li>
</ul>
<h2 id="abstract">Abstract</h2>
<ul>
<li><strong>大致流程</strong>: 用户指定一个 map 函数来处理键/值对以生成一组中间键/值对，以及一个 reduce 函数来合并与同一中间键相关的所有中间值</li>
<li><strong>为什么这么做？</strong> 是想充分利用不同机器的并行性来处理大量的数据，分别执行 map 和 reduce 任务来完成大数据任务，提高每个 host 的利用率。（也就是分布式系统的原型）</li>
<li><strong>需要解决的问题：</strong>
<ul>
<li>数据输入的切分</li>
<li>不同机器上执行的任务的调度</li>
<li>机器故障处理</li>
<li>机器间通信的管理</li>
</ul>
</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>根本矛盾</strong>：少数据量时单机能够直接运行简单的任务来完成相关计算，但面对大数据量的情况下，引入多计算实例组成的系统的复杂性和本身计算任务的简单性之间的矛盾。</li>
<li><strong>思想起源</strong>：来自于 Lisp 语言的函数式编程思想中的 map/reduce 函数。
<ul>
<li>在 lisp 语言中，map 作为一个输入函数接受一个序列，然后处理每个序列中 value 值，然后 reduce 将最终的 map 计算出来的结果整理成最终程序输出。</li>
</ul>
</li>
</ul>
<h2 id="programming-model">Programming Model</h2>
<ul>
<li>MapReduce 本质是一种编程模型
<ul>
<li>Map: 由用户编写，接受一个输入对并生成一组中间键/值对</li>
</ul>
</li>
</ul>
<pre><code class="language-Java">// map (k1,v1) → list(k2,v2)
map(String key, String value): 
    // key: document name 
    // value: document contents 
    for each word w in value: 
        EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<ul>
<li>Reduce: 也由用户编写，接受一个中间键和该键的一组值。它将这些值合并在一起，形成一个可能更小的值集</li>
</ul>
<pre><code class="language-Java">// reduce (k2,list(v2)) → list(v2)
reduce(String key, Iterator values): 
    // key: a word 
    // values: a list of counts 
    int result = 0; 
    for each v in values: 
        result += ParseInt(v); 
    Emit(AsString(result));
</code></pre>
<ul>
<li><strong>应用实例</strong>：
<ul>
<li>Distributed Grep</li>
<li>Count of URL Access Frequency：map &lt;URL, 1&gt;, reduce &lt;URL, total count&gt;</li>
<li>Reverse Web-Link Graph: map &lt;target, source&gt;, reduce &lt;target, list(source)&gt;</li>
<li>Term-Vector per Host</li>
<li>Inverted Index: map &lt;word, document ID&gt;, reduce &lt;word, list(document ID)&gt;</li>
<li>Distributed Sort</li>
</ul>
</li>
</ul>
<h2 id="implementation">Implementation</h2>
<ul>
<li>Map 函数分布在多个机器上，相应地自动将输入数据划分为 M 份，然后可以由分布了 Map 函数的机器并行处理这些数据。而对于 Reduce 则是将中间数据划分为 R 份，通常需要使用一个分割函数，常见的就是 <code>hash(key) mod R</code> 来将中间 Key 进行区分。</li>
<li>下图演示了整个 MapReduce 的流程，当用户程序调用 MapReduce 函数时将按照以下顺序执行：
<ul>
<li>
<ol>
<li>MapReduce Library 首先将输入文件划分为 M 个分片，每个分片大小通常为 16MB or 64MB，可以由用户控制，然后开始将程序拷贝到各个机器上，也就是图中的 <strong>fork</strong> 过程</li>
</ol>
</li>
<li>
<ol start="2">
<li>fork 的过程中会有一个特殊的情况，即 master 节点上运行的程序。剩下的 worker 对应执行的任务都是由 master 分配的，有 M 个 map task 和 R 个 reduce task 需要分配，master 选择空闲的 worker 来执行 map 或者 reduce task。</li>
</ol>
</li>
<li>
<ol start="3">
<li>被分配到 map task 的 worker 首先读取分片的数据内容，它从输入数据中解析键/值对，并将每对键/值传递给用户定义的 Map 函数，然后由 Map 产生的中间键值对将被缓冲在内存中；</li>
</ol>
</li>
<li>
<ol start="4">
<li>缓冲在内存中的中间数据将定期执行刷回操作写到磁盘，然后再由用户定义的分割函数执行将中间数据分割为 R 个区域，这些原本缓冲在内存中的数据持久化到磁盘之后的地址将传递给 master，然后 master 负责告诉 reduce task worker 这些数据在哪里。</li>
</ol>
</li>
<li>
<ol start="5">
<li>执行 reduce task 的 worker 在接收到来自 master 的数据地址的通知之后，使用 RPC 来从 map worker 的本地磁盘中读取数据，当一个 reducer 读取到了所有的中间数据之后，就可以根据中间键对它进行排序，以便将所有出现的相同键组合在一起。之所以需要排序，是因为通常有许多不同的键映射到同一个reduce任务。如果中间数据量太大，无法装入内存，则使用外部排序。</li>
</ol>
</li>
<li>
<ol start="6">
<li>reduce worker 迭代已排序的中间数据，对于遇到的每个惟一的中间键，它将键和相应的中间值集传递给用户的 reduce 函数。Reduce 函数的输出被追加到这个 Reduce 分区的最终输出文件中；</li>
</ol>
</li>
<li>
<ol start="7">
<li>所有的 map 任务和 reduce 任务都完成后，master 唤醒用户程序。此时，用户程序中的MapReduce 调用返回到用户程序。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210208135434.png" alt="20210208135434" loading="lazy"></li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="master-data-structures">Master Data Structures</h3>
<ul>
<li>master 节点保存了几个数据结构：
<ul>
<li>对于每个 map 和 reduce task，它存储了 task 对应的状态（idle, in-progress, completed）</li>
<li>worker machine 的标识（非空闲任务运行所在的机器）</li>
</ul>
</li>
<li>master 是一个管道，通过它将中间文件区域的位置从 map task 传播到 reduce task。因此，对于每个完成的 map task，master 存储了 map task 产生的 R 个中间文件区域的位置和大小，当 map task 完成时，master 将接收对该位置和大小信息的更新。信息被递增地推送给正在进行 reduce task 的 worker。</li>
</ul>
<h3 id="fault-tolerance">Fault Tolerance</h3>
<ul>
<li>由于 MapReduce 库被设计用来帮助处理使用成百上千台机器的大量数据，所以这个库必须能够优雅地容忍机器故障。</li>
</ul>
<h4 id="worker-failure">Worker Failure</h4>
<ul>
<li>master 周期地 ping 每个 worker，如果在确定时间内未收到对应的响应，则认为该 worker 宕机，标记该 worker 为 failed，由 worker <strong>已经完成</strong>的任何 map task 都将被重置回初始的空闲 <em>idle</em> 状态，因此有资格对其他 worker 进行重新调度，类似地，在一个失败的 worker 上<strong>正在进行</strong>的任何 map task 或reduce task 也会被重置为空闲，并可以重新调度。</li>
<li>在发生故障时，完成的 map task 将被重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。<strong>已完成的 reduce 任务</strong>不需要重新执行，因为它们的输出存储在全局文件系统中。</li>
<li>假设一个 map task A 一开始由 A 执行，之后由 B 执行（因为 A failed），所有正在执行 reduce task 的 workers 将被通知重新执行，任何尚未从 worker A 读取数据的 reduce task 都将从 worker B 读取数据。</li>
</ul>
<h4 id="master-failure">Master Failure</h4>
<ul>
<li>让 master 定期对上面描述的 master 节点存取的数据结构做 checkpoint 很容易。如果 master task 失效，可以从最后一个检查点状态启动一个新的副本。然而，考虑到只有一个主机，它的失败是不太多见，因此，如果 master 失败，我们当前的实现将终止 MapReduce 计算。客户端可以检查这种情况，如果他们愿意，可以重试 MapReduce 操作。</li>
</ul>
<h4 id="semantics-in-the-presence-of-failures">Semantics in the Presence of Failures</h4>
<ul>
<li>当用户提供的 map 和 reduce 操作符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的非故障顺序执行产生的输出相同。</li>
<li>我们依赖 map 和 reduce task 输出的原子提交来实现该属性。每个正在进行的任务都将其输出写入私有临时文件。一个 reduce task 生成一个这样的文件，map task 生成 R 个这样的文件(每个 reduce task 一个)。当 map task 完成时，worker 向 master 发送一条消息，并在消息中包含 R 临时文件的名称，如果 master 接收到一个<strong>已经完成</strong>的 map task 的完成消息，它将忽略该消息。否则，它将在主数据结构中记录 R 文件的名称。</li>
<li>当一个 reduce task 完成之后，reduce worker 自动地将其临时输出文件重命名为最终输出文件。如果在多台机器上执行相同的 reduce 任务，那么将对相同的最终输出文件执行多个 rename 调用。我们依赖于底层文件系统提供的原子重命名操作，以确保最终文件系统状态只包含一次执行 reduce 任务所产生的数据。</li>
<li>我们的 map 和 reduce 操作符绝大多数都是确定性的，在这种情况下，我们的语义等价于顺序执行，这使得程序员可以很容易地推断他们的程序行为。当 map 或 reduce 操作符是不确定的时，我们提供较弱但仍然合理的语义。在存在非确定性操作符的情况下，特定 reduce 任务 R1 的输出等价于由非确定性程序的顺序执行产生的 R1 的输出。然而，不同 reduce 任务 R2 的输出可能对应于非确定性程序的不同顺序执行产生的 R2 输出。</li>
<li>考虑 map 任务 M 和 reduce 任务 R1 和 R2。设 e(Ri) 是所承诺的 Ri 的执行(只有一个这样的执行)。由于 e(R1) 可能读取了 M 的一次执行产生的输出，而 e(R2) 可能读取了 M 的另一次执行产生的输出，所以语义较弱。</li>
</ul>
<h3 id="locality">Locality</h3>
<ul>
<li>在我们的计算环境中，网络带宽是一个相对稀缺的资源。通过利用输入数据(由 GFS 管理)存储在组成集群的机器的本地磁盘这一事实，我们节约了网络带宽。GFS 将每个文件划分为64 MB的块，并在不同的机器上存储每个块的多个副本(通常是3个副本)，MapReduce master 将输入文件的位置信息考虑在内，并尝试在包含相应输入数据副本的机器上调度map任务。如果失败，它将尝试调度靠近该任务输入数据副本的 map 任务(例如，在与包含数据的机器在同一网络交换机上的工作机器上)。当在集群中相当一部分 worker 上运行大型MapReduce 操作时，大部分输入数据都是在本地读取的，不会消耗网络带宽</li>
</ul>
<h3 id="task-granularity">Task Granularity</h3>
<ul>
<li>如上所述，我们将 map 阶段细分为 M 个部分，将 reduce 阶段细分为 R 个部分。理想情况下，M 和 R 应该远远大于工作机器的数量。让每个 worker 执行许多不同的任务可以改善动态负载平衡，并在 worker 失败时加快恢复速度:它完成的许多 map 任务可以分散到所有其他 worker 机器上。</li>
<li>在我们的实现中，M 和 R 的大小最多有多大是有实际限制的，因为如上所述，master 必须做出 O(M + R) 调度决策，并在内存中保持 O(M*R) 状态。(内存使用的常量是很小的:状态的 O(M∗R) 部分由每个 map任务/reduce任务对大约一个字节的数据组成。)</li>
<li>此外，R 常常受到用户的限制，因为每个 reduce 任务的输出都以单独的输出文件结束。在实践中，我们倾向于选择 M，以便每个单独的任务大约有 16MB 到 64MB 的输入数据(以便上面描述的局部性优化最有效)，并且我们将 R 设为预期使用的工作机器数量的小倍数。我们经常使用 2000 台 worker 机器进行 M = 200000 和 R = 5000 的 MapReduce 计算。</li>
</ul>
<h3 id="backup-tasks">Backup Tasks</h3>
<ul>
<li>导致 MapReduce 操作总时间延长的一个常见原因是“掉线”(straggler)。一种需要异常长时间才能完成计算过程中最后几个 map 或 reduce 任务之一的机器。掉队者出现的原因有很多。例如，磁盘有问题的机器可能会经常出现可纠正错误，导致读性能从 30MB/s 降至 1MB/s。集群调度系统可能已经调度了机器上的其他任务，由于 CPU、内存、本地磁盘或网络带宽的竞争，导致它执行 MapReduce 代码的速度变慢。我们最近遇到的一个问题是，机器初始化代码中的一个bug导致了处理器缓存被禁用:受影响机器的计算速度降低了 100 倍以上。</li>
<li>我们有一个一般性的机制来缓解掉队者的问题。当 MapReduce 操作接近完成时，master 会对剩余的正在执行的任务进行备份。只要主执行或备份执行完成，任务就被标记为完成。我们已经调优了这种机制，因此它通常不会增加操作使用的计算资源超过几个百分点。我们发现，这大大减少了完成大型 MapReduce 操作的时间。以5.3中所述的排序程序为例，关闭备份机制后，排序程序完成的时间会增加 44%。</li>
</ul>
<h2 id="refinements">Refinements</h2>
<h3 id="partitioning-function">Partitioning Function</h3>
<ul>
<li>MapReduce 的用户指定他们想要的 reduce 任务/输出文件的数量(R)，使用中间键上的分区函数在这些任务之间对数据进行分区。提供了一个默认的分区函数，使用哈希(例如&quot; hash(key) mod R &quot;)。这往往会导致相当平衡的分区。然而，在某些情况下，通过键的其他函数来分区数据是有用的。例如，有时输出键是url，我们希望单个主机的所有条目都在同一个输出文件中结束。为了支持这种情况，MapReduce 库的用户可以提供一个特殊的分区函数。例如，使用&quot; hash(Hostname(urlkey)) mod R &quot;作为分区函数会导致来自同一主机的所有 url 最终出现在同一个输出文件中。</li>
</ul>
<h3 id="ordering-guarantees">Ordering Guarantees</h3>
<ul>
<li>我们保证在给定的分区中，中间键/值对按键的递增顺序进行处理。这种排序保证使得为每个分区生成有序的输出文件变得很容易，当输出文件格式需要支持按键进行有效的随机访问查找，或者输出的用户发现对数据进行排序很方便时，这很有用。</li>
</ul>
<h3 id="combiner-function">Combiner Function</h3>
<ul>
<li>在某些情况下，每个 map 任务产生的中间键有显著的重复，并且用户指定的 Reduce 函数是可交换的和关联的。单词计数示例就是一个很好的例子。由于单词频率倾向于遵循 Zipf 分布，每个 map 任务将产生数百或数千个 &lt;the, 1&gt; 形式的记录。所有这些计数将通过网络发送到一个 reduce 任务，然后由 reduce 函数相加产生一个数字。我们允许用户指定一个可选的Combiner函数，该函数在通过网络发送数据之前对数据进行部分合并。</li>
<li>Combiner 函数在每一个执行 map task 上的机器执行，通常使用相同的代码来实现 combiner 和 reduce 函数，reduce 函数和 combiner 函数之间的唯一区别是 MapReduce 库如何处理函数的输出。reduce 函数的输出被写入最终的输出文件。combiner 函数的输出被写入一个中间文件，该文件将被发送到reduce 任务。</li>
<li>部分 Combine 大大加快了 MapReduce 操作的某些类。</li>
</ul>
<h3 id="input-and-output-types">Input and Output Types</h3>
<ul>
<li>MapReduce 库支持以几种不同的格式读取输入数据。text 模式的输入将每一行视为键/值对：键是文件中的偏移量，值是行内容。另一种常见的支持格式存储按键排序的键/值对序列。每个输入类型实现都知道如何将自己分割成有意义的范围，以便作为单独的 map 任务进行处理(例如，文本模式的范围分割确保范围分割只发生在行边界)。用户可以通过提供一个简单的 <em>reader</em> 接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义输入类型中的一种。</li>
<li><em>reader</em> 并不一定需要提供从文件中读取的数据。例如，很容易定义从数据库或映射在内存中的数据结构中读取记录的 <em>reader</em>。</li>
<li>以类似的方式，我们支持一组输出类型来生成不同格式的数据，并且用户代码很容易添加对新输出类型的支持。</li>
</ul>
<h3 id="side-effects">Side-effects</h3>
<ul>
<li>在某些情况下，MapReduce 的用户发现从他们的 map 或 reduce 操作生成辅助文件作为额外的输出是很方便的。我们依靠应用程序 writer 使这些副作用具有原子性和幂等性。通常，应用程序会写入一个临时文件，并在完全生成该文件后自动重命名该文件。</li>
<li>我们不支持单个任务生成的多个输出文件的原子两阶段提交。因此，产生具有跨文件一致性要求的多个输出文件的任务应该是确定的。这种限制在实践中从来就不是问题。</li>
</ul>
<h3 id="skipping-bad-records">Skipping Bad Records</h3>
<ul>
<li>有时，用户代码中的错误会导致 Map 或 Reduce 函数在特定记录上崩溃。此类 bug 会导致 MapReduce 操作无法完成。通常的做法是修复 bug，但有时这是不可行的;这个 bug 可能存在于源代码不可用的第三方库中。此外，有时忽略一些记录是可以接受的，例如在对一个大数据集进行统计分析时。我们提供了一个可选的执行模式，MapReduce 库会检测哪些记录导致确定性崩溃，并跳过这些记录以便继续前进。</li>
<li>每个工作进程都安装一个信号处理程序来捕获分割违规和总线错误。在调用 Map 或 Reduce 操作之前，MapReduce 库会将参数的序号存储在全局变量中。如果用户代码产生信号，信号处理器发送一个包含序列号的“最后一口气” UDP 包给 MapReduce master。当主服务器在一个特定的记录上看到多个失败时，它指示在下一次重新执行对应的 Map 或 Reduce 任务时应该跳过该记录。</li>
</ul>
<h3 id="local-execution">Local Execution</h3>
<ul>
<li>在 Map 或 Reduce 函数中调试问题可能会很棘手，因为实际的计算发生在分布式系统中，通常在几千台机器上，由 master 动态地做出工作分配决策。为了方便调试、分析和小规模测试，我们开发了 MapReduce 库的替代实现，在本地机器上顺序执行 MapReduce 操作的所有工作。控件提供给用户，以便计算可以限制到特定的映射任务。用户可以用一个特殊的标志来调用他们的程序，然后可以很容易地使用任何他们认为有用的调试或测试工具(例如gdb)。</li>
</ul>
<h3 id="status-information">Status Information</h3>
<ul>
<li>主服务器运行一个内部HTTP服务器，并导出一组状态页面供人们使用。状态页面显示了计算的进度，例如有多少任务已经完成，有多少任务正在进行，输入字节数，中间数据字节数，输出字节数，处理速率等。这些页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算将花费多长时间，以及是否应该向计算中添加更多的资源。这些页面还可以用于计算何时会比预期的慢得多。</li>
<li>此外，顶级状态页面显示哪些 worker 失败了，以及当他们失败时正在处理哪些 map 和 reduce 任务。当试图诊断用户代码中的错误时，此信息非常有用。</li>
</ul>
<h3 id="counters">Counters</h3>
<ul>
<li>MapReduce 库提供了一个计数器来计算各种事件的发生次数。例如，用户代码可能需要计算已处理的字的总数或索引的德文文档的数量，等等。</li>
<li>要使用这个功能，用户代码创建一个命名的计数器对象，然后在 Map 或 Reduce 函数中适当地增加计数器。例如:</li>
</ul>
<pre><code class="language-C++">Counter* uppercase; 
uppercase = GetCounter(&quot;uppercase&quot;);
map(String name, String contents): 
  for each word w in contents: 
    if (IsCapitalized(w)): 
      uppercase-&gt;Increment(); 
    EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<ul>
<li>来自各个 worker 机器的计数器值定期传播到 master (在 ping 响应中附带)。master 聚合成功的 map 和 reduce 任务的计数器值，并在 MapReduce 操作完成时将它们返回给用户代码。当前计数器值也显示在 master 状态页面上，以便人们可以观看实时计算的进度。在聚合计数器值时，master 消除了重复执行同一个 map 或 reduce 任务的影响，以避免重复计算。(重复执行可能是由于我们使用了备份任务以及由于失败而重新执行任务引起的。)</li>
<li>一些计数器值由 MapReduce 库自动维护，例如处理的输入键/值对的数量和产生的输出键/值对的数量。</li>
<li>用户已经发现 counter 工具对于检查 MapReduce 操作的行为是非常有用的。例如，在一些 MapReduce 操作中，用户代码可能希望确保生成的输出对的数量恰好等于处理的输入对的数量，或者确保处理的德文文档的比例在处理的文档总数的某个可容忍的比例内。</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>MapReduce编程模型已经在谷歌上成功地用于许多不同的目的。我们认为这种成功有几个原因。首先，该模型易于使用，即使对于没有并行和分布式系统经验的程序员也是如此，因为它隐藏了并行化、容错、局部性优化和负载平衡的细节。其次，大量的问题可以通过MapReduce计算很容易地表达出来。例如，MapReduce被用于谷歌生产web搜索服务的数据生成、排序、数据挖掘、机器学习以及许多其他系统。第三，我们开发了一个MapReduce的实现，它可以扩展到由数千台机器组成的大型机器集群。该实现有效地利用了这些机器资源，因此适合用于在谷歌中遇到的许多大型计算问题。</li>
<li>我们从这项工作中学到了一些东西。首先，对编程模型的限制使得并行化和分布式计算变得容易，并使这些计算具有容错性。其次，网络带宽是一种稀缺资源。因此，我们系统中的许多优化都旨在减少通过网络发送的数据量:局部性优化允许我们从本地磁盘读取数据，将中间数据的单个副本写入本地磁盘可以节省网络带宽。第三，冗余执行可以用来减少慢速机器的影响，并处理机器故障和数据丢失。</li>
</ul>
<pre><code class="language-C++">#include &quot;mapreduce/mapreduce.h&quot;
// 用户实现map函数
class WordCounter : public Mapper {
 public:
    virtual void Map(const MapInput&amp; input) {
      const string&amp; text = input.value();
      const int n = text.size();
      for (int i = 0; i &lt; n; ) {
        // 跳过前导空格
        while ((i &lt; n) &amp;&amp; isspace(text[i]))
             i++;
         // 查找单词的结束位置
         int start = i;
         while ((i &lt; n) &amp;&amp; !isspace(text[i]))
              i++;
         if (start &lt; i)
            Emit(text.substr(start,i-start),&quot;1&quot;);
        }
 
     }
 
};
 
REGISTER_MAPPER(WordCounter);
// 用户实现reduce函数
class Adder : public Reducer {
    virtual void Reduce(ReduceInput* input) {
              // 迭代具有相同key的所有条目,并且累加它们的value
              int64 value = 0;
              while (!input-&gt;done()) {
                     value += StringToInt(input-&gt;value());
                     input-&gt;NextValue();
              }
              // 提交这个输入key的综合
              Emit(IntToString(value));
       }
 
};
REGISTER_REDUCER(Adder);
int main(int argc, char** argv) {
       ParseCommandLineFlags(argc, argv);
       MapReduceSpecification spec;
       // 把输入文件列表存入&quot;spec&quot;
       for (int i = 1; i &lt; argc; i++) {
              MapReduceInput* input = spec.add_input();
              input-&gt;set_format(&quot;text&quot;);
              input-&gt;set_filepattern(argv[i]);
              input-&gt;set_mapper_class(&quot;WordCounter&quot;);
       }
        //指定输出文件:
       // /gfs/test/freq-00000-of-00100
       // /gfs/test/freq-00001-of-00100
      // ...
       MapReduceOutput* out = spec.output();
       out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);
       out-&gt;set_num_tasks(100);
       out-&gt;set_format(&quot;text&quot;);
       out-&gt;set_reducer_class(&quot;Adder&quot;);
       // 可选操作:在map任务中做部分累加工作,以便节省带宽
       out-&gt;set_combiner_class(&quot;Adder&quot;);
       // 调整参数: 使用2000台机器,每个任务100MB内存
       spec.set_machines(2000);
       spec.set_map_megabytes(100);
       spec.set_reduce_megabytes(100);
       // 运行它
       MapReduceResult result;
       if (!MapReduce(spec, &amp;result)) abort();
       // 完成: 'result'结构包含计数,花费时间,和使用机器的信息
       return 0;
</code></pre>
<hr>
<ul>
<li>论文的部分到此结束，后面展开讲一下 MapReduce 的其他东西。</li>
</ul>
<h2 id="other">Other</h2>
<ul>
<li>MapReduce 最重要的贡献：MR takes care of, and hides, all aspects of distribution!</li>
</ul>
<h3 id="problems">Problems</h3>
<ul>
<li><strong>What if the master gives two workers the same Map() task?</strong>
<ul>
<li>Perhaps the master incorrectly thinks one worker died. it will tell Reduce workers about only one of them.</li>
</ul>
</li>
<li><strong>What if the master gives two workers the same Reduce() task?</strong>
<ul>
<li>they will both try to write the same output file on GFS! atomic GFS rename prevents mixing; one complete file will be visible.</li>
</ul>
</li>
<li><strong>What if a single worker is very slow -- a &quot;straggler&quot;?</strong>
<ul>
<li>perhaps due to flakey hardware. master starts a second copy of last few tasks.</li>
</ul>
</li>
<li><strong>What if a worker computes incorrect output, due to broken h/w or s/w?</strong>
<ul>
<li>too bad! MR assumes &quot;fail-stop&quot; CPUs and software.</li>
</ul>
</li>
</ul>
<h3 id="current-status">Current status</h3>
<ul>
<li>Hugely influential (Hadoop, Spark, &amp;c).</li>
<li>Probably no longer in use at Google.
<ul>
<li>Replaced by Flume / FlumeJava (see paper by Chambers et al).</li>
<li>GFS replaced by Colossus (no good description), and BigTable.</li>
</ul>
</li>
</ul>
<h3 id="conclusion-2">Conclusion</h3>
<ul>
<li>MapReduce single-handedly made big cluster computation popular.
<ul>
<li>-Not the most efficient or flexible.</li>
<li>+Scales well.</li>
<li>+Easy to program -- failures and data movement are hidden.</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is license for source code?]]></title>
        <id>https://blog.shunzi.tech/post/license/</id>
        <link href="https://blog.shunzi.tech/post/license/">
        </link>
        <updated>2021-02-06T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。</li>
</ul>
</blockquote>
<!--more-->
<h3 id="开源许可长啥样">开源许可长啥样？</h3>
<ul>
<li>我们常常在 Github 上看到关于 License 的信息，仿佛 NB 点的项目都挂了个 License（啊没有不挂就不 NB 的意思），一般都长成下面这样，只是可能协议啥的会有区别。如 RocksDB 的 <a href="https://github.com/facebook/rocksdb">repo</a>，使用了 <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GPLv2</a> 和 <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a> 协议，然后 repo 内也有相应的协议文件与之对应，COPYING 和 LICENSE.Apache<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206203249.png" alt="20210206203249" loading="lazy"></li>
</ul>
<h3 id="到底啥是开源许可">到底啥是开源许可？</h3>
<ul>
<li>License 可能大家听说的相对于 Copyright 少一点。那么不妨先说啥是 Copyright。</li>
</ul>
<h4 id="copyrightc">Copyright(C)</h4>
<ul>
<li><strong>Copyright</strong>：中文译作版权，大家在一些数字媒体软件上可能感受的真切一点，比如某首歌只有部分音乐公司拥有其版权，那为什么有的公司没有版权就不能提供相关音乐的播放和下载服务呢，不妨拆词解义 copy + right，即复制的权利，说的简单点就是没有某个产品 Copyright 的公司就无法对该产品进行复制，就更别说进行修改发布了。</li>
</ul>
<blockquote>
<p>百度百科：版权是对计算机程序、文学著作、音乐作品、照片、游戏，电影等的复制权利的合法所有权。除非转让给另一方，版权通常被认为是属于作者的。大多数计算机程序不仅受到版权的保护，还受软件许可证的保护。版权只保护思想的表达形式，而不保护思想本身。算法、数学方法、技术或机器的设计均不在版权的保护之列。</p>
</blockquote>
<ul>
<li>如果有去公司实习或者工作过的同学应该就知道，往往在公司的项目里写相关代码的时候往往会有一条编程规范的限制，即 Copyright 的声明，许多 IDE 也有相关 Copyright 模板和自动生成插件的提供。如下为 RocksDB 源代码中关于 CopyRight 的声明。Copyright 约定了版权归属谁，并归定了这个软件的使用许可证方式。</li>
</ul>
<pre><code class="language-Java">// Copyright (c) 2011-present, Facebook, Inc.  All rights reserved.
//  This source code is licensed under both the GPLv2 (found in the
//  COPYING file in the root directory) and Apache 2.0 License
//  (found in the LICENSE.Apache file in the root directory).

package org.rocksdb;

public abstract class Cache extends RocksObject {
  protected Cache(final long nativeHandle) {
    super(nativeHandle);
  }
}
</code></pre>
<ul>
<li>Copyright 是作者或者创建者因为其原创性的工作，所拥有的复制，分发，出售以及其他一系列的排他性权利，如上所示代码声明了“这段代码的版权归属于 Facebook”，拥有一切版权保护的相关权利。</li>
</ul>
<h4 id="copyleftɔ">Copyleft(Ɔ)</h4>
<ul>
<li>其实还有个东西叫 <strong>Copyleft(Ɔ)</strong>，
<ul>
<li>“Copyleft”最初是为反对商业软件而生，但它并不是放弃版权。反对软件一切权利归作者私有，保护知识共享、权利共享。</li>
<li>软件的版权归原作者所有，其它一切权利归任何人所有。用户和软件的作者享有除版权外的完全同等的权利，包括复制软件和重新发布修改过的软件的权利。</li>
<li>自由软件在承认著作权的基础上，可以通过许可协议，与公众共享作品的其它权利</li>
</ul>
</li>
</ul>
<blockquote>
<p>百度百科：著佐权（Copyleft）是一个由自由软件运动所发展的概念，是一种利用现有著作权体制来保护所有用户和二次开发者的自由的授权方式。在自由软件授权方式中增加著佐权条款之后，该自由软件除了允许使用者自由使用、散布、修改之外，著佐权许可证更要求使用者修改后的衍生作品必须要以同等的授权方式（除非许可证或者版权声明里面例外条款所规定的外）释出以回馈社会。</p>
</blockquote>
<ul>
<li>所以正是因为 <strong>Copyleft(Ɔ)</strong> 的思想，才逐渐衍生出后来的 <strong>License</strong>。可以简单理解为 <code>Copyleft = Copyright+GPL</code></li>
</ul>
<h4 id="license">License</h4>
<ul>
<li>版权法默认禁止共享，也就是说，没有许可证的软件，就等同于保留版权，虽然开源了，用户只能看看源码，不能用，一用就会侵犯版权。所以软件开源的话，必须明确地授予用户开源许可证。</li>
<li>License 是 Copyright 拥有者授予其他人处置其原创性成果的权利，如上代码版权声明所示，“Facebook 授予了这段代码 GPLv2 和 Apache2.0 的许可”。</li>
<li>开放源码许可证是符合开放源码定义的许可证——简而言之，它们允许“<strong>自由</strong>”地使用、修改和共享软件。这里的自由其实是相对的，相应地需要遵守对应 License 下的规定。</li>
<li>软件许可是告诉其他人，他们能够对您的代码做什么，不能做什么。</li>
<li>大多数人将其许可文件放在仓库根目录的文件 <code>LICENSE.txt</code>（或 <code>LICENSE.md</code>）中，如 RocksDB 中的 COPYING 和 LICENSE.Apache。</li>
<li>GPLv2 COPYING</li>
</ul>
<pre><code class="language-txt">                    GNU GENERAL PUBLIC LICENSE
                       Version 2, June 1991

 Copyright (C) 1989, 1991 Free Software Foundation, Inc.,
 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.
 ...
</code></pre>
<ul>
<li>Apache2.0 LICENSE.Apache</li>
</ul>
<pre><code class="language-txt">

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.
   ...
</code></pre>
<ul>
<li>上面其实介绍的都是开源 license，但其实在软件市场中还是有大量的<strong>商业 license</strong> 的，毕竟也是要恰饭的嘛。主要就是一些商业软件的使用，常常需要大家购买对应软件的 license 才能使用，而且很多 license 大多都是有时间期限的，例如 IDEA 可能还有一些 license server 的机制，但总体思想都是通过销售 license 来获取盈利。
<ul>
<li>BTW，很多盗版软件其实就是尝试着去碰撞出一个可能有效的 license，甚至有人共享对应的 license 来进行多用户使用。</li>
</ul>
</li>
</ul>
<h3 id="有哪些开源许可">有哪些开源许可</h3>
<ul>
<li>https://www.gnu.org/licenses/license-list.zh-cn.html</li>
<li>几种常见的开源许可：
<ul>
<li><a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GPL（GNU General Public License）</a>：GNU 通用公共许可协议，免费使用、引用、修改代码，但不能用在闭源软件中发布及销售。“传染性” 表示如果一个软件使用了 GPL 协议的开源代码，那么这个软件也必须开源，仍然免费使用。不能用于商业产品。</li>
<li><a href="https://www.gnu.org/licenses/lgpl-3.0.html">LGPL（GNU Lesser General Public License）</a>：宽松GPL，规定：如果A项目采用LGPL许可证，那么基于A开发出来的B项目也必须采用LGPL，即必须也开源，但是，如果B项目不是基于A开发出来的，而仅仅调用了A的接口，那么B项目可不必开源，倘若换做GPL的话，那么B项目也是要开源的（所以叫做宽松的GPL）。</li>
<li><a href="https://en.wikipedia.org/wiki/BSD_licenses">BSD License（original BSD license、FreeBSD license、Original BSD license）</a>：伯克利软件套装，规定：如果A项目采用BSD许可证，那么基于A开发出来的B项目可以选择闭源，即私有化、商业化，但是必须注明B项目采用了A这个开源项目。<strong>主要限制在于不能用开源代码的作者或机构进行商品推广。</strong></li>
<li><a href="https://en.wikipedia.org/wiki/MIT_License">MIT(The MIT License)</a>：麻省理工学院许可证，规定：这是一个自由度很高的开源许可证，几乎同意了可以随意使用一个开源项目（使用、复制、修改、合并、出版发行、散布、再授权、贩售软件及软件的副本），只要在你的项目中包含或提及原开源项目的MIT许可证。<strong>至于你会不会通过它进行商品推广，作者并不关心，只想保留版权。</strong></li>
<li><a href="https://www.apache.org/licenses/">Apache Licence</a>：Apache软件基金会，规定：大致上和BSD许可证类似，只是有一点细微差别，它除了需要注明B项目源于开源项目A，也要在每个修改过的A项目的文件注明此文件已被修改，并且原文件是A开源项目中的哪个文件。<strong>相对于 MIT，如果修改了源代码，需要进行说明</strong>。</li>
</ul>
</li>
<li>不推荐用于商业产品的协议：GPL (eg. Linux), LGPL, MPL</li>
<li>适用于商业产品的协议：BSD, MIT, Apache (eg. RocksDB)</li>
<li><strong>Dual-Licensed</strong>: 但是我们在如上的 RocksDB 中的例子观察到，RocksDB 中包含了两个开源许可，一个是不推荐用于商业产品的 GPLv2，一个是推荐用于商业产品的 Apache2.0，而在 RocksDB 关于 License 的介绍中我们发现本身该项目就是基于两个开源协议的，而其他软件开发者可以根据自己的实际需求来决定使用哪一个 License。</li>
</ul>
<h4 id="other">Other</h4>
<ul>
<li>可能还会有小伙伴看过这样的版权例子，但大多都是一些知识产品，比如博客、slides、文档以及网页等等。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206230938.png" alt="20210206230938" loading="lazy"></li>
<li><strong>CC License</strong>: Creative Commons license，简称CC许可，是一种公共版权许可协议，知识共享许可协议，其允许分发受版权保护的作品。一个创作共用许可，用于一个作者想给他人分享、使用、甚至创作派生作品的权利。创作共用提供给作者灵活性（例如，他们可以选择允许非商业用途使用他们的作品），保护使用或重新分配他人作品的人，所以他们只要遵守由作者指定的条件，不必担心侵犯版权。</li>
</ul>
<blockquote>
<p>百度百科：知识共享（Creative Commons，简称CC，台湾译名创用CC）是一个非营利组织，也用是一种创作的授权方式。此组织的主要宗旨是增加创意作品的流通可及性，作为其它人据以创作及共享的基础，并寻找适当的法律以确保上述理念。</p>
</blockquote>
<ul>
<li><strong>CC-BY-NC-SA</strong> 本质是几种权利的组合：
<ul>
<li><strong>CC</strong>：创作共用</li>
<li><strong>BY</strong>：署名：您（用户）可以复制、发行、展览、表演、放映、广播或通过信息网络传播本作品；您必须按照作者或者许可人指定的方式对作品进行署名。</li>
<li><strong>NC</strong>：非商业性使用（英语：Noncommercial，nc）您可以自由复制、散布、展示及演出本作品；您不得为商业目的而使用本作品。</li>
<li><strong>SA</strong>：相同方式共享（英语：ShareAlike，sa）您可以自由复制、散布、展示及演出本作品；若您改变、转变或更改本作品，仅在遵守与本作品相同的许可条款下，您才能散布由本作品产生的派生作品。（参见copyleft。）</li>
</ul>
</li>
<li>除此以外还包含一种权利：
<ul>
<li><strong>ND</strong>：禁止演绎（英语：No Derivative Works，nd)，您可以自由复制、散布、展示及演出本作品；您不得改变、转变或更改本作品。</li>
</ul>
</li>
</ul>
<h3 id="怎么选开源许可">怎么选开源许可</h3>
<ul>
<li>如何选 License: https://www.gnu.org/licenses/license-recommendations.html</li>
<li>千言万语一大堆，不如一张图。图源阮一峰博客 http://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206213620.png" alt="20210206213620" loading="lazy"></li>
<li>https://choosealicense.com/<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206220305.png" alt="20210206220305" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous Merging]]></title>
        <id>https://blog.shunzi.tech/post/Dostoevsky/</id>
        <link href="https://blog.shunzi.tech/post/Dostoevsky/">
        </link>
        <updated>2021-01-12T14:50:15.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based<br>
Key-Value Stores via Adaptive Removal of Superfluous Merging</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based<br>
Key-Value Stores via Adaptive Removal of Superfluous Merging</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>无论是学术界还是工业界，所有主流的基于 LSM 树的键值存储都在更新的 I/O 成本和查询和存储空间的 I/O 成本之间进行了权衡。因为在所有的 LSM Tree 的级别上都需要执行 Compaction 操作来限制查询遍历的 runs，并删除 obsolete 的数据项来腾出存储空间。即便是最先进的 LSM Tree 设计，来自 LSM Tree 所有层此的合并操作（除了最大的层此）减少的点查询成本、大范围查询成本和存储空间，减少的效果可以忽略不计；与此同时还增加了更新操作的平摊开销。</li>
<li>为了解决这个问题，我们提出了 Lazy Leveling，一种新的设计，从除开最大层以外的所有 level 中删除合并操作。同时提出了 Fluid LSM-tree，一种可以涵盖整个 LSM-tree 设计领域的通用设计，可以参数化以假设任何现有的设计。相对于 Lazy level, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。</li>
<li>Dostoevsky，一种键值存储，通过基于应用程序工作负载和硬件来动态调整的弹性的 LSM-tree 设计，自适应地消除多余的合并。基于 RocksDB 实现，测试表明无论是性能还是存储空间方面都优于目前最先进的设计。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>LSM-tree 将要插入/更新的条目缓冲在内存中，并在缓冲区填满时将缓冲区作为 sorted run 刷新到次要存储。LSM-tree 稍后对这些 runs 进行排序合并，以限制查找必须扫描的run 数量，并删除过时的条目。LSM-tree 将运行组织成指数级增长的容量，更大的级别包含更老的运行。当条目被替换更新时，点查找通过从最小到最大的级别查找条目的最新版本，并在查找目标键时终止查找。另一方面，范围查找必须在所有级别的所有 run 中访问相关的键范围，并从结果集中删除过时的条目。为了提高单个 run 的查询速度，设计中常常包含了两个额外的内存中的数据结构。首先，对于每个 run，都有一组包含每个 run 块的第一个键的 fence 指针，这允许查找在一个 run 中只使用一个 I/O 就可以访问特定的键；第二，每个 run 会有一个 BoolmFilter，这允许点查询跳过不包含目标键的 runs。这个设计被应用到了大量的现代 KV 存储中，如 LevelDB、BigTable 等。</li>
<li><strong>问题</strong>：LSM-tree 中的合并操作的频率控制了在 更新的 I/O 成本 和 查询和存储空间放大的 I/O 成本之间的 trade-off，另外的问题就是现有的设计在这些指标之间的 trade-off 并不理想。下图表示了指标之间的权衡关系，虽然这些 y 轴指标具有不同的单位，但它们相对于 x 轴的权衡曲线具有相同的形状。两个极端分别是 log 和 sorted array。LSM-tree在完全不合并或尽可能多地合并时，分别退化为这些边缘点。我们将主流系统放置在这些边缘点之间的顶部曲线上，基于它们的默认合并频率，我们为 Monkey 绘制了一个优越的权衡曲线，我们证明了存在一个甚至比Monkey更好的权衡曲线。现有的设计放弃了大量的性能和/或存储空间，因为没有沿着这条底部曲线设计。</li>
<li><strong>问题来源</strong>：通过分析最先进的 LSM 树的设计空间，我们指出了问题的根源，即最坏情况下的更新代价、点查询代价、范围查询代价和空间放大在不同的层次上产生不同的结果。
<ul>
<li>Update: 更新的 I/O 成本稍后通过更新条目参与的合并操作来分担。虽然较大级别的合并操作需要成倍地增加工作，但它们发生的频率却成倍地减少。因此，更新从所有级别的合并操作中同等地获得它们的 I/O 成本。</li>
<li>Point lookups：虽然 图1 中沿顶部曲线的主流设计将跨 LSM-tree 所有级别的 Bloom flters 假阳性率设置为相同，但目前最先进的 Monkey 为更小的层此设置更低的假阳性率。他被证明可以最小化所有筛选器的误报率之和，从而最小化点查找的 I/O。与此同时，这意味着进入较小层此的可能性呈指数级下降，因此大多数点查询 I/O 将直接命中最大的层次。</li>
<li>Long range lookup：因为 LSM Tree 的容量呈指数级增长，最大曾通常包含绝大部分数据，所以该层更可能包含给定键范围内的数据，因此大多数由大范围查询引起的 I/O 都将对最大层进行操作。</li>
<li>Short range lookup： 使用极小键范围的范围查找在每次 run 中只能访问大约一个块，而不管 run 的大小，因为每一层 run 的最大个数是固定的，因此小范围查询在所有曾中是相当的。</li>
<li>Space-Amplifcation：空间放大最差的情况就是较低层次的数据被更新到最大层此，因此在最大层中老旧的数据项比例最大。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210112173749.png" alt="20210112173749" loading="lazy"></li>
</ul>
</li>
<li>因为最坏情况下的点查询开销、大范围查询开销、空间放大都主要来源于最大层，LSM-tree 中所有级别的合并操作，除开最大层(即大多数合并操作)在这些指标上几乎没有改进，同时显著增加了更新的平摊成本。这导致了次优的权衡。我们用三个步骤从头开始解决这个问题：
<ul>
<li><strong>Solution 1: Lazy Leveling to Remove Superﬂuous Merging</strong>：
<ul>
<li>我们使用 Lazy level 拓展了 LSM-tree 设计思路，这种新设计除去了 LSM-tree 最大级别之外的所有合并。Lazy Leveling 改进了最坏情况下更新的成本复杂性，同时在点查找成本、大范围查找成本和空间放大上保持相同的限制，同时在小范围查找成本上提供具有竞争力的限制。我们证明改进的更新开销可以用来降低点查找开销和空间放大。这生成了 图1 中的底部曲线，它提供了更丰富的时空权衡，这是迄今为止最先进的设计无法实现的。</li>
</ul>
</li>
<li><strong>Solution 2: Fluid LSM-Tree for Design Space Fluidity.</strong>
<ul>
<li>我们引入了 Fluid LSM-tree 作为新一代的 LSM Tree 支持在整个 LSM-tree 设计思路中流畅地切换。Fluid LSM-tree 通过分别控制最大级别和所有其他级别合并操作的频率，相对于 Lazy leveling, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。</li>
</ul>
</li>
<li><strong>Solution 3: Dostoevsky to Navigate the Design Space</strong>
<ul>
<li>Dostoevsky: Space-Time Optimized Evolvable Scalable Key-Value Store。Dostoevsky 分析地找到了Fluid LSM-tree 的调优方法，以最大限度地提高特定应用程序工作负载和硬件在空间放大方面的用户约束，通过精简搜索空间来快速找到最佳调优，并在运行时对其进行物理调整。因为 Dostoevsky 跨越了所有现有的设计，并能够针对给定的应用程序 navigate 到最佳的设计，因此它在性能和空间放大方面严格控制了现有的键值存储。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="background">BACKGROUND</h2>
<ul>
<li>如下图所示，为了优化写操作，LSM-Tree 初始时缓冲了所有的更新、插入和删除操作到内存中，当 Buffer 满了之后，LSM Tree 将 Buffer 以 Sorted Run 刷回到了第二层存储，LSM Tree 归并排序 runs 是为了：限制查询操作必须访问的 runs 的数量，以及删除老旧的数据项以回收空间。runs 被组织成 L 个呈指数级增长的层此，Level 0 是主存中的 Buffer，其他层次都位于二级存储。</li>
<li>在合并的 I/O 开销和查询 I/O 开销以及空间放大之前的权衡可以由两个参数来控制，第一个是相邻两个层次之间的比例 T，T 控制了层级的个数因此决定了一个数据能够在层级之间合并多少次。第二个参数是合并策略，决定了数据项在一个 level 内的合并次数。所有的现有设计都使用了 tiering 或 leveling 两种策略。
<ul>
<li>tiering：当一个 level 到达容量时合并该 level 内的 runs</li>
<li>leveling: 当一个新的 run 出现，就会在 level 中执行合并</li>
</ul>
</li>
<li>如下图所示，size ratio 为 4，buffer 大小为一个 entry 的大小。在两种策略中，当 buffer flushing 造成 Level 1 到达容量时触发合并操作。对于 tiering，Level 1 的所有 runs 都被合并成同一个新的 run 放置在 Level 2。而对于 Leveling，合并操作还会包含 Level2 原有的 run。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210114224107.png" alt="20210114224107" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210115172909.png" alt="20210115172909" loading="lazy"></li>
<li><strong>Number of Levels</strong>：Level 0 拥有的数据项个数 <em>B * P</em>。$$L = [log_T(\frac{N}{B<em>P}</em>\frac{T-1}{T})]$$。层级之间的大小比例 T 被限制到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>≤</mo><mi>T</mi><mo>≤</mo><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">2 ≤ T ≤ T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 被定义成 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>N</mi><mrow><mi>B</mi><mo>∗</mo><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{N}{B*P}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，当 size ratio 达到上界时，levels 的数量减少到接近 1，超过上界之后将无结构性的变化。大于下界就表明 第 i 级合并操作的结果运行永远不会大到超过第 i + 1 级。换句话说就是确保了 runs 不会跨多个 levels。</li>
<li><strong>Finding Entries</strong>：因为数据项是异地更新，相同 key 的数据的多个版本可能出现在多个 level 中，甚至对于 tiering 策略可能存在于一个 level 的多个 runs 中，为了确保查询操作总是能够找到最新版本的数据，LSM Tree 采用了如下措施：1. 当数据项被插入到 buffer 中且 buffer 中包含相同 key 对应的数据时，新的数据项将代替老的数据；2. 当两个包含相同 key 的数据项的 runs 被合并的时候，只有最新版本的数据将被保留；3. 为了能够获取到来自不同 runs 的相同 key 的不同数据项的插入顺序，一个单独的 run 只能够和相邻时间的 run 进行合并。从而保证当有两个 runs 包含不同版本的相同 key 对应的数据时，younger run 包含的是最新版本的数据。</li>
<li><strong>Point Lookups</strong>：点查询通过从最小到最大层次进行遍历来查询最新版本的数据，对于 tiering 则是在一个 level 中从最新到最老的 runs 中遍历进行查询。当找到一个匹配当前 key 的数据时则终止。</li>
<li><strong>Range Lookups</strong>：范围查询需要查找指定范围的键对应的所有最新的数据，通过对所有 levels 所有 runs 的相关键范围进行排序合并。当 sort-merging 时，识别出来自不同 runs 具有相同 key 的数据，然后丢弃掉老版本的数据。</li>
<li><strong>Deletes</strong>：通过给每个数据项添加一位 flag 来实现。如果查询操作找到了该数据想的最新版本，且该数据项上有该 flag 那么将不会返回对应的 value 给应用。当一个删除的数据项和 最老的 run 合并的时候，该数据将被删除，因为该数据项已经代替了之前所有插入的具有当前 key 的数据。</li>
<li><strong>Fragmented Merging</strong>：为了换接较大级别上由于长时间合并操作而导致的性能下降，主流设计把 runs 分区成了文件，也叫 Sorted String Tables，然后一次合并一个 SSTable 和下一个 older run 中具有重叠键范围的多个 SSTables，该技术不会影响最坏情况下的合并 I/O 开销，而只会影响这种开销如何调度。在整篇文章中，为了便于阅读，我们将合并操作讨论为具有 runs 的粒度，尽管它们也可以具有 sstables 的粒度。</li>
<li><strong>Space-Amplifcation</strong>：过时条目的存在使存储空间增大的因素称为空间放大。由于磁盘的可承受性，空间放大传统上并不是数据结构设计的主要关注点。然而，SSD 的出现使空间放大成为一个重要的成本问题。我们将空间放大作为成本指标，以提供我们所引入和评估的设计的完整描述。</li>
<li><strong>Fence Pointers</strong>：所有主要的基于 LSM 树的键值存储都在主存中对每次运行的每个块的第一个键建立索引，也就是图 2 所示的 fence pointer，通常这些指针占据内存空间大小为 O(N/B)，但是让查询操作中找到每个 runs 的 key 范围变成了只需要一次 I/O。</li>
<li><strong>Bloom Filters</strong>：为了加速点查找，只需要在主存中为每个 run 维护一个 BloomFilter，点查找在访问存储中相应的 runs 之前首先检查 Bloom flter。如果 filter 返回 true positive，那么查询操作配合 fence pointer 只需要一次 I/O 就能访问对应的 run，从而找到对应的数据项并终止。如果返回 negative，那么将跳过该 run 并节省一次 I/O 操作。但还有 false positive 的情况，浪费一次 I/O 然后再去下一个 run 继续查找该 key。</li>
<li>Bloom flter 有一个有用的特性，如果它被分割成较小的等大小的 Bloom flter，其中的条目也被等分，每一个新的分区布隆滤片的 FPR 渐近与原滤片的 FPR 相同(虽然实际略高)。为了便于讨论，我们将Bloom flters称为非分区的，尽管它们也可以按照工业中的某些设计进行分区（比如每个 run 的每个 block），从而为空间管理提供更大的灵活性。(例如，对于那些不经常被点查询读取的块，可以将其 offload 到存储器中以节省内存)</li>
<li><strong>Applicability Beyond Key-Value Stores</strong>：根据工业上的设计，我们的讨论假设一个键在运行过程中与它的值相邻存储。为了便于阅读，本文中的所有图形都将条目描述为键，但它们表示键-值对。我们的工作也适用于没有 value 的应用程序(例如，LSM-tree 被用来回答关于键的集合成员查询)，其中的值是指向存储在 LSM-tree 之外的数据对象的指针，或者 LSM-tree 被用作解决更复杂算法问题的构建块(例如，图分析)， FTL 设计等)。我们将分析的范围限制在基本操作和 LSM-tree 的大小上，以便它可以很容易地应用于这些其他情况。</li>
</ul>
<h2 id="design-space-and-problem-analysis">DESIGN SPACE AND PROBLEM ANALYSIS</h2>
<ul>
<li>现在，我们分析更新和查找的最差情况下的空间放大和 I/O 成本是如何从不同的级别派生出与合并策略和大小比例相关的。为了分析更新和查找，我们使用磁盘访问模型来计算每个操作的 I/O 数量，其中 I/O 是从二级存储读取或写入一个块。</li>
<li>分析结果如下所示：
<ul>
<li><strong>Updates</strong>：更新成本通常都是由更新条目参与的后续合并操作产生的，分析假设最坏情况的工作负载，其中所有更新的目标条目都在最大级别。这意味着一个过时的条目不会被删除，直到它相应的更新的条目达到最大级别。因此，每个条目都会在所有级别上合并(即，而不是在某个更小的级别上被最近的条目丢弃，从而减少以后合并操作的开销)。
<ul>
<li>tiering：每层合并 O(1) 次，每个合并过程中的 I/O 操作从原始的 run 中拷贝 B 个数据项到新的 run，因此每个数据项平均的更新操作成本开销如图所示。填满 level i，需要 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">B · P · T^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 次更新，导致合并操作拷贝 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">B · P · T^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 个数据项。</li>
<li>leveling：到达 level i 的第 j 个 run 触发了一个合并操作，合并操作包括 level i 现有的 runs，这些 runs 是自上次 level i 为空以来到达的前 T−j 个 runs 的合并操作产生的。因此平均每个数据项在该层数据到达容量之前合并了 T/2 次，可以表示为 O(T)，同样需要除以一个块对应的 B 个数据项。每 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">B · P · T^{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 次更新（每次有一个新的 run come in）之后执行一次合并操作，然后拷贝平均 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{B · P · T^i}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3704599999999998em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0254599999999998em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mpunct mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9020857142857143em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 项数据，通过将复制的条目数除以级别 i 的合并操作的频率，<strong>我们观察到，在长期运行中，每个级别上的合并操作所做的工作量是相同的，直觉是，虽然合并操作在更大的级别上以指数方式完成更多的工作，但它们的频率也以指数方式降低</strong></li>
</ul>
</li>
<li><strong>Analyzing Point Lookups</strong>：为了分析最坏情况下的点查找代价，我们将重点放在 zero-result 点查找（例如查询不存在的 Key）上，因为它们最大化了浪费的 I/O 的平均值。这种分析对于插入前判断是否存在的操作就很有用。开销最大的情况即为所有的 BloomFilter 返回 false positive，此时点查询操作会对每一个 run 发起一次 I/O，对于 leveling 浪费的 I/O 为 O(L)，对于 tiering 浪费的 I/O 为 O(T · L) 。但实际上，Bloom flters 对于不存在的 key 能节省很大一部分 I/O，在工业中，键值存储对每一个Bloom flters使用 10 位，这会导致误报率(FPR)为每个过滤器约为 1%，出于这个原因，我们将重点放在预期的最坏情况点查找成本上，它将点查找发出的 I/O 数量作为关于 Bloom flters FPRs 的长期平均值进行估计。我们估计这个成本为所有Bloom flters的FPRs之和。原因是，查询单个 run 的 I/O 成本是一个独立的随机变量，其期望值等于相应的 Bloom flter 的 FPR，多个独立随机变量的期望值之和等于它们各自的期望值之和。在工业界的键值存储中，所有级别的 BloomFilter 的每个条目的比特数是相同的。因此，最大级别的 Bloom flter(s) 比所有较小级别的 filter 的总和要大，因为它们以指数形式表示更多的条目。根据公式 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mo>(</mo><mi>b</mi><mi>i</mi><mi>t</mi><mi>s</mi><mi mathvariant="normal">/</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>s</mi><mo>)</mo><mo separator="true">⋅</mo><mi>l</mi><mi>n</mi><mo>(</mo><mn>2</mn><msup><mo>)</mo><mn>2</mn></msup></mrow></msup></mrow><annotation encoding="application/x-tex">FPR = e^{−(bits/entries)·ln(2)^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9869199999999998em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869199999999998em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">s</span><span class="mord mtight">/</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">s</span><span class="mclose mtight">)</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">n</span><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，最大层的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><msub><mi>R</mi><mrow><mi>p</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">FPR_{pL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 上界被限制在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，所以对于 leveling，即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span>，对于 tiering，即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo separator="true">⋅</mo><mi>T</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L · T )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span>.
<ul>
<li>关于这个问题的最新论文 Monkey 表明，为所有级别的过滤器设置相同的每个条目的比特数并不能最小化浪费的I/O 的预期数量。相反，Monkey 在最大级别上对 filter 中的每个条目重新分配≈1比特，它使用这些比特来设置较小级别上每个条目的比特数，作为不断增加的等差数列：即 Level i 的每个数据项为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo separator="true">⋅</mo><mo>(</mo><mi>L</mi><mo>−</mo><mi>i</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">a + b · (L - i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span>，a 和 b 都是比较小的常数，这导致 FPR 在最大水平上有一个小的、渐近恒定的增加，在较小的水平上有一个指数下降，因为它们包含较少的条目。由于 FPRs 在较小的级别是指数递减的，所以 FPRs 的总和收敛于一个与级别数无关的乘法常数。Monkey 从点查找的复杂性中去掉了一个 L 的因素，这种复杂性导致了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span> I/O (level)和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span> I/O (tiering)，如图3 (B)所示。对于 zero and non-zero result 的结果点查找以及任何类型的偏差，使用 Monkey 总是有益的。</li>
<li>总的来说，我们观察到使用 Monkey 的<strong>点查找成本主要来自于最大的 level，因为较小的 level 的 FPRs 呈指数级下降，所以访问它们的可能性也呈指数级下降</strong>。</li>
</ul>
</li>
<li><strong>Analyzing Range Lookups</strong>：我们将范围查找的 selectivity 表示为在目标键范围内的所有 run 的唯一条目的数量。范围查找在所有 runs 中扫描和排序合并目标键范围，并从结果集中删除过时的条目。范围查询扫描并排序合并所有 runs 的目标键范围，从结果集中消除老数据。为了分析，如果访问的块数至少是可能的最大级别数的两倍，那么就认为范围查询的范围很大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>s</mi><mi>B</mi></mfrac><mo>&gt;</mo><mn>2</mn><mo separator="true">⋅</mo><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{s}{B} &gt; 2 · L_{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，在均匀随机分布的更新下，这个条件意味着在目标键范围内的大多数条目都有很高的概率处于最大级别。
<ul>
<li>小范围查询对每个 run 发起近一个 I/O，叠加起来就是 leveling o(L)，tiering 就是 O(L·T)。对于长范围查询，在消除过时条目之前的结果集的大小平均是其 selectivity 和空间放大的乘积。我们用这个乘积除以块大小来得到 I/O 成本，tiering 即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mfrac><mrow><mi>T</mi><mo separator="true">⋅</mo><mi>s</mi></mrow><mi>B</mi></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\frac{T·s}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>，leveling 为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mfrac><mi>s</mi><mi>B</mi></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\frac{s}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.095em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></li>
<li><strong>一个关键的区别是，短范围查找从所有级别获得的开销大致相同，而长范围查找的大部分开销来自访问最大级别</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210116202849.png" alt="20210116202849" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><strong>Analyzing Space-Amplifcation</strong>：我们将空间放大定义为条目总数 N 除以唯一条目数 unq，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>=</mo><mfrac><mi>N</mi><mrow><mi>u</mi><mi>n</mi><mi>q</mi></mrow></mfrac><mi mathvariant="normal">−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">amp = \frac{N}{unq} − 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3534389999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">−</span><span class="mord">1</span></span></span></span>。为了分析最坏情况的空间放大，我们观察到 LSM-tree 的 1 到 L−1 级包含其容量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的一部分，而 L 级包含其容量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>T</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{T−1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的剩余部分。使用 leveing，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，从而导致 Level L 的最多有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 是过时的。空间放大因此是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>T</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(1/T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span>。对于 tiering，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，且 Level L 的每个 run 包含相同的数据项集时，Level L 完全由过时的条目组成，所以空间放大是 O(T)，因为 Level L 比所有其他 Level 加起来要大 T−1 倍。<strong>总的来说，在最坏的情况下，带有 leveling 和 tiering 的空间放大主要是由于在最大级别上存在过时的条目</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118112648.png" alt="20210118112648" loading="lazy"></li>
<li><strong>Mapping the Design Space to the Trade-Oﬀ Space</strong>：更新成本与查找和空间放大成本之间存在一种内在的权衡。如下图实线绘制了在y轴上查找和空间放大的不同成本，以及在x轴上更新的成本(当我们改变大小比例时)。当大小比例设置为其限制值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>(意味着存储中只有一个级别)时，tiered 的 LSM-tree 退化为日志，而 leveled 的 LSM-tree 退化为排序的数组。当尺寸比设置为其下限2时，随着 level 和 tiering 的行为趋于一致，性能特征逐渐收敛：级别的数量是相同的，当第二个 run come in 时，每个级别都会触发合并操作。一般来说，随着 leveling/tiering 大小比例的增加，查找成本和空间放大相应 减少/增加，更新成本相应 增加/减少。因此，对权衡空间进行了分区:与分层相比，level 相比于 tiering 具有更好的查找成本和空间放大，更糟糕的更新成本。</li>
<li><strong>The Holy Grail</strong>：图5中的实线反映了Monkey的属性，即当前的最先进的设计。图5 还显示了标记为“难以捉摸的最佳”的虚线。指导我们研究的问题是，其他设计是否可能通过时空权衡更接近甚至达到难以捉摸的最佳设计。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118143538.png" alt="20210118143538" loading="lazy"></li>
<li><strong>The Opportunity: Removing Superﬂuous Merging</strong>：我们已经确定了<strong>不对称性:点查找成本、长范围查找成本和空间放大主要来自最大的级别，而更新成本来自所有级别</strong>。这意味着在更小的级别上合并操作显著地放大了更新成本，同时为空间放大、点查找和远程查找带来的好处相对较小。因此，有一个合并策略的启发，在较小的层次上合并较少次数。</li>
</ul>
<h2 id="lazy-leveling-fluid-lsm-tree-and-dostoevsky">LAZY LEVELING, FLUID LSM-TREE, AND DOSTOEVSKY</h2>
<h3 id="lazy-leveling">Lazy Leveling</h3>
<ul>
<li>Lazy Leveling 一种合并策略，除了LSM-tree的最大级别之外，它完全消除了合并。其动机是，在这些更小的级别上合并会显著增加更新成本，同时对点查找、远程查找和空间放大产生的改进相对较小。相比于 Leveling，Lazy Leveling：
<ul>
<li>improves the cost complexity of updates</li>
<li>maintains the same complexity for point lookups, long range lookups, and space-amplifcation</li>
<li>provides competitive performance for short range lookups.<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118160921.png" alt="20210118160921" loading="lazy"></li>
</ul>
</li>
<li><strong>Basic Structure</strong>：Lazy Leveling 结构如下所示，其核心类似于缓和 tiering 和 leveling 两种结构，它在最大 level 上应用 leveling，在所有其他 level 上应用 tiering。结果，最大 level 的 runs 数量为 1，其他 level 的 runs 数量最多为 T−1 (即，合并操作在第 T 个 run 到达时发生)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118161152.png" alt="20210118161152" loading="lazy"></li>
<li><strong>Bloom Filters Allocation</strong>：如何保持点查找的成本复杂性不变，尽管有更多的 rims 在较小的级别上被检索。我们通过优化不同级别之间的 BloomFilter 内存预算来做到这一点。我们开始建模点查找成本和 filter 的总体内存占用与 FPRs 有关。最坏情况下，每次查找的预期浪费I/O 数由零结果点查询造成，等于每次运行的 Bloom flters 的误阳性率之和。</li>
</ul>
<h3 id="fluid-lsm-tree">Fluid LSM-Tree</h3>
<h2 id="references">References</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/129355502">[1] 知乎 - 叶提：SIGMOD'18|Dostoevsky</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost]]></title>
        <id>https://blog.shunzi.tech/post/CRaft/</id>
        <link href="https://blog.shunzi.tech/post/CRaft/">
        </link>
        <updated>2020-12-16T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>一致性协议主要是在分布式系统中用于保证可靠性和可用性的，现有的一致性协议大多都是要将日志项给备份到所有的服务器中，这种全量的副本的策略在存储和网络上的开销都很大，严重影响性能，所以后来出现了纠删码，即在保证相同的容错能力的条件下减少存储和网络的开销。</li>
<li>RS-Paxos 是第一个支持 EC 数据的一致性协议，但是比起通用的一致性协议，如 Paxos/Raft，可用性都相对更差。我们指出了RSPaxos的活性问题，并试图解决，基于 Raft 提出了 CRaft，既能使用 EC 码像 RS-Paxos 一样降低存储和网络开销，也能保证如 Raft 一样的 liveness。</li>
<li>基于 CRaft 实现了一个 KVs，实验表明相对于 Raft 节省了 66% 的存储空间，写吞吐量提升了 250%，写延迟减少了 60.8%</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>共识算法介绍</strong>：共识协议协议通常保证安全性和活动性，这意味着它们总是返回正确的结果，并且在大多数服务器都没有发生故障的情况下可以完全正常工作。
<ul>
<li>Google’s Chubby 会使用 Paxos 对 metadata 做副本</li>
<li>Gaios(NSDI2011) 表明一致性协议可以被用于所有数据的 replicated</li>
<li>现如今大量应用如 etcd, TinyKV, FSS 等大规模系统都使用了 Raft/Paxos 来 replicated TB 数量级的数据，并提供更好的可用性</li>
</ul>
</li>
<li><strong>多副本介绍</strong>：数据操作通常在分布式系统中被转换为一系列的日志指令，然后使用一致性协议在所有的服务器之间进行备份，所以数据需要经过网络传输到所有的服务器，然后还要刷会到磁盘持久化保存。一致性问题中，容错率如果为 F，那么则至少需要 N = (2F + 1) 的服务器，否则就可能因为分组的原因出现不一致的情况。因此传统的副本策略往往就意味值原始数据量的 N 倍的网络和存储开销，而且随着这些协议在大规模存储系统中得到了越来越多的应用，N 倍的网络和存储开销带来的则是延迟的增加和吞吐量的下降。<strong>所以出现了 Erasure Coding</strong></li>
<li><strong>纠删码介绍</strong>：纠删码相比于全量拷贝的副本策略，极大地减小了存储和网络的开销。通过将数据进行分片，编码分片后的数据并生成一些校验的分片，原始的数据就能从足够数量的分片子集中恢复出来，这时候每个服务器只存储一个分片，而不是数据的全量拷贝，开销极大减小。FSS 中就使用了纠删码来减少存储开销，但是 FSS 在编码之前使用了一个 5 way 流水线 Paxos 来备份完整的用户数据和元数据，因此额外的网络开销还是有 4 倍数据量大小。</li>
<li><strong>RS-Paxos</strong> 是第一个结合了 Paxos 和 EC 的共识协议，虽然减少了存储和网络的开销，但是在可用性上比 Paxos 还是更差，RA-Paxos 牺牲了 liveness 来使用 EC 提升性能，换句话说就是 RS-Paoxs 如果有 N = (2F + 1) 的服务器不再能容忍 F 个错误，即容错率下降了，主要是因为 RS-Paxos 中的提交要求越来越严格。</li>
<li>作者提出了 erasure-coding-supported version of Raft <strong>CRaft</strong> (Coded Raft)。该方案中，一个 leader 有两种方法备份日志项到 followers，如果 leader 能够和足够数量的 followers 通信，那么 leader 将使用分片后的日志项进行备份，即传统纠删码的方式，否则将备份完整的数据以保证可用性。相比于 RS-Paxos，CRaft 最大的不同是拥有和 Paxos/Raft 相同级别的 liveness，而 RS-Paxos 没有，但是两个方案都节省了网络和存储的成本。</li>
</ul>
<h2 id="background">Background</h2>
<h3 id="raft">Raft</h3>
<ul>
<li>https://raft.github.io/</li>
<li>Raft 原始论文：https://raft.github.io/raft.pdf</li>
<li>Raft 中主要有三个角色/三种状态。Candidate 收到了来自大多数 servers 的选票后成为 Leader，一个 Server 只会给 和该 Server 日志同步的 Candidate 投票。每个 Server 每一轮最多投一次，所以 Raft 保证每一轮最多就一个 leader。
<ul>
<li>Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader。</li>
<li>Follower: 类似选民，完全被动</li>
<li>Candidate候选人: 类似Proposer，可以被选为一个新的 Leader</li>
</ul>
</li>
<li>leader 从客户端接收日志条目，并试图将它们复制到其他服务器，迫使其他服务器的日志与自己的日志一致。当 leader 发现这一轮中有日志被被分到了大多数 servers，该日志项和之前的日志将被安全地应用到状态机中。Leader 将提交并应用这些日志项，然后告诉 followers 也 apply 他们。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216220445.png" alt="20201216220445" loading="lazy"></li>
<li>用于实际系统的共识协议通常具有以下特性：
<ul>
<li>Safety：它们不会在所有非拜占庭条件下返回错误的结果</li>
<li>Liveness：只要大多数服务器都处于活动状态，并且能够相互通信和与客户端通信，它们就能完全发挥作用。我们称这组服务器是健康的</li>
</ul>
</li>
<li>Raft 中的 Safety 是由 Leader Completeness Property 来保证的， 如果在给定 term 提交了日志条目，那么该条目将出现在所有编号较高的 term 的 leader 日志中。</li>
<li>Liveness 由 Raft 规则保证，通常使用了一致性协议的系统的服务器的数量常常为奇数，假设 N = 2F + 1，Raft 可以容忍 F 个错误，我们定义一个一致性协议可以容忍的失败数量作为 liveness level，所以此时的 liveness level 为 F，更高的 liveness level 意味着更好的 liveness，没有一个协议的 liveness level 可以达到 F+1，因为如果存在这样的协议，则可能存在两个分裂的 F 个健康服务器组，这两个组可以分别就不同的内容达成一致，这是违反安全特性的。</li>
</ul>
<h3 id="erasure-coding">Erasure Coding</h3>
<ul>
<li>擦除编码是存储系统和网络传输中容忍错误的常用技术。们已经提出了大量的编码，其中最常用的是Reed-Solomon (RS)编码。RS 码中有两个可配置的正整数参数 k 和 m，数据被分成了相同大小的 k 个分片，然后使用这 k 个原始的数据分片计算出 m 个类似的校验分片，也就是编码过程，此时总共将有 k+m 个分片，(k,m)-RS 码就意味着所有分片中的任意 k 个分片就能恢复出原始数据，这就是 RS 码的容错原理。（类似于解方程的过程）</li>
<li>当引入一致性协议，k + m = N，N 为服务器的总数量，存储和网络开销将被见效的全拷贝的 1/k，然而如何保证 safety 和 liveness 不容忽视</li>
</ul>
<h3 id="rs-paxos">RS-Paxos</h3>
<ul>
<li>RS-Paxos 是将纠删编码与 Paxos 相结合的一种 Paxos 的改革版本，可以节省存储和网络成本。在 Paxos 中，命令被完全传输。然而，在 RS-Paxos 中，命令是通过代码片段传输的。根据这一变化，服务器在 RS-Paxos 中只能存储和传输片段，从而降低了存储和网络成本。</li>
<li>为了保证安全性和活动性，Paxos和Raft基于以下包容-排斥原则。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>A</mi><mo>∪</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi>A</mi><mi mathvariant="normal">∣</mi><mo>+</mo><mi mathvariant="normal">∣</mi><mi>B</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">−</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>∩</mo><mi>B</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|A∪B| = |A|+|B| −|A∩B|
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord">−</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span></span></span></span></span></p>
<p>包含排除原则保证在两个不同的服务器组合中至少有一个服务器的数量差距，这样安全性就可以得到保证。</p>
<ul>
<li>RS-Paxos 的想法是增加交集集的大小。具体来说，在选择了一个 (k,m)-RS 代码后，读quorum QR、写 quorum QW 和服务器数量 N 应该符合以下公式。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>R</mi></msub><mo>+</mo><msub><mi>Q</mi><mi>W</mi></msub><mi mathvariant="normal">−</mi><mi>N</mi><mo>≥</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">Q_R +Q_W −N ≥ k
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span></span></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alluxio]]></title>
        <id>https://blog.shunzi.tech/post/Alluxio/</id>
        <link href="https://blog.shunzi.tech/post/Alluxio/">
        </link>
        <updated>2020-12-10T02:21:17.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Alluxio 简单介绍，测试报告，然后会结合一些实际体验。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Alluxio 简单介绍，测试报告，然后会结合一些实际体验。</li>
</ul>
</blockquote>
<!-- more -->
<h1 id="alluxio">Alluxio</h1>
<ul>
<li>Alluxio（之前名为 Tachyon），是一个开源的具有内存级速度的虚拟分布式存储系统， 使得应用程序可以以内存级速度与任何存储系统中的数据进行交互。</li>
<li>源码：https://github.com/Alluxio/alluxio</li>
<li>论文：https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-29.pdf</li>
</ul>
<h2 id="架构">架构</h2>
<ul>
<li>文档：https://docs.alluxio.io/os/user/stable/cn/Overview.html</li>
<li>初衷：建立底层存储和大数据计算框架之间的存储系统，为大数据应用提供一个数量级的加速，同时它还提供了通用的数据访问接口。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201210191906.png" alt="20201210191906" loading="lazy"></li>
<li>主要分为两层：UFS 和 Alluxio
<ul>
<li>UFS：底层文件存储，该存储空间代表不受Alluxio管理的空间。
<ul>
<li>UFS存储可能来自外部文件系统，包括如HDFS或S3。 Alluxio可能连接到一个或多个UFS并在一个命名空间中统一呈现这类底层存储。</li>
<li>通常，UFS存储旨在相当长一段时间持久存储大量数据。</li>
</ul>
</li>
<li>Alluxio 存储：
<ul>
<li>Alluxio 做为一个分布式缓存来管理 Alluxio workers 本地存储，包括内存。这个在用户应用程序与各种底层存储之间的快速数据层带来的是显著提高的I/O性能。</li>
<li>Alluxio存储主要用于存储热的、暂时的数据，而不关注长期的持久性。</li>
<li>要管理的每个Alluxio工作节点的存储数量和类型由用户配置决定。</li>
<li><strong>即使数据当前不在Alluxio存储中，通过Alluxio连接的UFS​​中的文件仍然 对Alluxio客户可见。当客户端尝试读取仅可从UFS获得的文件时数据将被复制到Alluxio存储中。</strong></li>
</ul>
</li>
</ul>
</li>
<li>和其他常见的分布式文件系统对比：<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211095549.png" alt="20201211095549" loading="lazy"></li>
</ul>
<h3 id="角色">角色</h3>
<ul>
<li>Alluxio的设计使用了单Master和多Worker的架构。从高层的概念理解，Alluxio可以被分为三个部分，Master，Worker和Client。
<ul>
<li>Master和Worker一起组成了Alluxio的服务端，它们是系统管理员维护和管理的组件。</li>
<li>Client通常是应用程序，如Spark或MapReduce作业，或者Alluxio的命令行用户。</li>
</ul>
</li>
<li>以前的版本需要借助 ZooKeeper 进行高可用选主，后续的 Alluxio 自己实现了高可用机制。（注：Tachyon 为 Alluxio 旧称）<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211093448.png" alt="20201211093448" loading="lazy"></li>
</ul>
<h4 id="master">Master</h4>
<ul>
<li>
<p>主从模式：主Master主要负责处理全局的系统元数据，从Master不断的读取并处理主Master写的日志。同时从Master会周期性的把所有的状态写入日志。从Master不处理任何请求。</p>
<ul>
<li>主从之间心跳检测</li>
<li>主Master不会主动发起与其他组件的通信，它只是以回复请求的方式与其他组件进行通信。一个Alluxio集群只有一个主Master。</li>
</ul>
</li>
<li>
<p>简单模式：最多只会有一个从Master，而且这个从Master不会被转换为主Maste。</p>
</li>
<li>
<p>高可用模式：可以有零个或者多个从Master。 当主Master异常的时候，系统会选一个从Master担任新的主Master。</p>
</li>
</ul>
<h4 id="worker">Worker</h4>
<ul>
<li>类似于 OSD</li>
<li>Alluxio的Worker负责管理分配给Alluxio的本地资源。这些资源可以是本地内存，SSD 或者硬盘，其可以由用户配置。</li>
<li>Alluxio的Worker以块的形式存储数据，并通过读或创建数据块的方式处理来自Client读写数据的请求。但Worker只负责这些数据块上的数据；文件到块的实际映射只会存储在Master上。</li>
</ul>
<h2 id="features">Features</h2>
<h3 id="全局命名空间">全局命名空间</h3>
<ul>
<li>Alluxio通过使用透明的命名机制和挂载API来实现有效的跨不同底层存储系统的数据管理。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201210220101.png" alt="20201210220101" loading="lazy"></li>
<li>https://www.alluxio.io/resources/whitepapers/unified-namespace-allowing-applications-to-access-data-anywhere/</li>
</ul>
<h3 id="智能多层级缓存">智能多层级缓存</h3>
<ul>
<li>Alluxio支持分层存储，以便管理内存之外的其它存储类型。目前Alluxio支持这些存储类型(存储层)：MEM (内存)，SSD (固态硬盘)，HDD (硬盘驱动器)</li>
<li><strong>单层/多层 区别？</strong></li>
</ul>
<h4 id="单层存储">单层存储</h4>
<ul>
<li>启动时默认分配一个 ramdisk，Alluxio将在每个worker节点上默认发放一个ramdisk并占用一定比例的系统的总内存。 此ramdisk将用作分配给每个Alluxio worker的唯一存储介质。</li>
<li>可以显示地设置每个 Worker 的 ramdisk 大小</li>
</ul>
<pre><code class="language-shell">alluxio.worker.ramdisk.size=16GB
</code></pre>
<ul>
<li>可以指定多个存储介质共同组成一个 level，也可以自定义添加存储介质类型</li>
</ul>
<pre><code class="language-shell">alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd1,/mnt/ssd2
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD,SSD
</code></pre>
<ul>
<li>所提供的路径应该指向安装适当存储介质的本地文件系统中的路径。要启用短路操作，这些路径的权限应该允许客户端用户对该路径进行读、写和执行。例如，启动Alluxio服务的同一用户组中的客户端用户需要770权限。</li>
<li>在更新存储媒体之后，我们需要指出为每个存储目录分配了多少存储空间。例如，如果我们想在ramdisk上使用 16GB，在每个 SSD 上使用 100GB:</li>
</ul>
<pre><code class="language-shell">alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB,100GB
</code></pre>
<h4 id="多层存储">多层存储</h4>
<ul>
<li>通常建议使用具有异构存储介质的单一存储层。在某些环境中，工作负载将受益于基于I/O速度的存储介质显式排序。Alluxio假设层是根据I/O性能从上到下排序的。例如，用户经常指定以下层:
<ul>
<li>MEM</li>
<li>SSD</li>
<li>HDD</li>
</ul>
</li>
<li><strong>写策略</strong>：用户写新的数据块时，默认情况下会将其写入顶层存储。如果顶层没有足够的可用空间， 则会尝试下一层促成。如果在所有层上均未找到存储空间，因Alluxio的设计是易失性存储，Alluxio会释放空间来存储新写入的数据块。会基于  block annotation policies 尝试从 worker 中驱逐数据，如果不能释放出新的空间，那么该写入将会失败。
<ul>
<li>eviction model 是同步的且是代表客户端来执行空间的释放的，主要是为要写入的客户端的数据腾出一块空闲空间，这种同步模式预计不会导致性能下降，因为在 block annotation policies 下有序的一组数据块通常都是可用的。</li>
</ul>
</li>
<li><strong>读策略</strong>：如果数据已经存在于Alluxio中，则客户端将简单地从已存储的数据块读取数据。 如果将Alluxio配置为多层，则不一定是从顶层读取数据块， 因为数据可能已经透明地挪到更低的存储层。有两种数据读取策略：<code>ReadType.CACHE</code> and <code>ReadType.CACHE_PROMOTE</code>。
<ul>
<li>用 <code>ReadType.CACHE_PROMOTE</code> 读取数据将在从worker读取数据前尝试首先将数据块挪到 顶层存储。也可以将其用作为一种数据管理策略 显式地将热数据移动到更高层存储读取。</li>
<li><code>ReadType.CACHE</code> Alluxio将块缓存到有可用空间的最高层。因此，如果该块当前位于磁盘(SSD/HDD)上，您将以磁盘速度读取该缓存块。</li>
</ul>
</li>
</ul>
<pre><code class="language-shell"># configure 2 tiers in Alluxio
alluxio.worker.tieredstore.levels=2
# the first (top) tier to be a memory tier
alluxio.worker.tieredstore.level0.alias=MEM
# defined `/mnt/ramdisk` to be the file path to the first tier
alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk
# defined MEM to be the medium type of the ramdisk directory
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM
# set the quota for the ramdisk to be `100GB`
alluxio.worker.tieredstore.level0.dirs.quota=100GB
# configure the second tier to be a hard disk tier
alluxio.worker.tieredstore.level1.alias=HDD
# configured 3 separate file paths for the second tier
alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3
# defined HDD to be the medium type of the second tier
alluxio.worker.tieredstore.level1.dirs.mediumtype=HDD,HDD,HDD
# define the quota for each of the 3 file paths of the second tier
alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB
</code></pre>
<h5 id="block-allocation-policies">Block Allocation Policies</h5>
<ul>
<li>Alluxio使用块分配策略来定义如何跨多个存储目录(在同一层或不同层中)分配新块。分配策略定义将新块分配到哪个存储目录中。这是通过 worker 属性 <code>alluxio.worker.allocate.class</code> 配置的。
<ul>
<li><code>MaxFreeAllocator</code>：从 0 层开始尝试到最低层，尝试将块分配到当前最具有可用性的存储目录。这是默认行为。</li>
<li><code>RoundRobinAllocator</code>：从 0 层到最低层开始尝试。在每一层上，维护存储目录的循环顺序。尝试按照轮询顺序将新块分配到一个目录中，如果这不起作用，就转到下一层。</li>
<li><code>GreedyAllocator</code>：这是 Allocator 接口的一个示例实现。它从顶层循环到最低层，尝试将新块放入可以包含该块的第一个目录中。</li>
</ul>
</li>
</ul>
<h5 id="experimental-block-allocation-review-policies">[Experimental] Block Allocation Review Policies</h5>
<ul>
<li>这是在Alluxio 2.4.1中增加的一个实验特性。在未来的版本中，接口可能会发生变化。</li>
<li>Alluxio 使用块分配审查策略来补充分配策略。与定义分配应该是什么样子的分配策略相比，分配审查过程验证分配决策，并防止那些不够好的分配决策。评审者与分配器一起工作</li>
<li>这是由worker属性 <code>alluxio.worker.review.class</code> 配置的。
<ul>
<li><code>ProbabilisticBufferReviewer</code>：基于每个存储目录对应的可用的空间，概率性低拒绝把新的数据块写入对应目录的请求。这个概率由 <code>alluxio.worker.reviewer.probabilistic.hardlimit.bytes</code> 和 <code>alluxio.worker.reviewer.probabilistic.softlimit.bytes</code> 来决定。
<ul>
<li>当可用空间低于 hardlimit，默认是 64MB，新的块将被拒绝</li>
<li>当可用空间大于 softlimit，默认 256MB，新的数据块将不会被拒绝</li>
<li>当可用空间介于上下限之间时，接受新的块的写入的概率将会随着可用容量的下降而线性低下降，我们选择在目录被填满之前尽早拒绝新的块，因为当我们读取块中的新数据时，目录中的现有块会扩大大小。在每个目录中留下缓冲区可以减少 eviction 的机会。</li>
</ul>
</li>
<li><code>AcceptingReviewer</code>：此审阅者接受每个块分配。和 v2.4.1 之前的行为完全一样</li>
</ul>
</li>
</ul>
<h5 id="block-annotation-policies">Block Annotation Policies</h5>
<ul>
<li>Alluxio使用块注释策略(从v2.3开始)来保持存储中块的严格顺序。Annotation策略定义了跨层块的顺序，并在以下过程中被参考:
<ul>
<li>Eviction</li>
<li>Dynamic Block Placement.</li>
</ul>
</li>
<li>与写操作一起发生的 Eviction 操作将尝试根据块注释策略执行的顺序删除块。按注释顺序排列的最后一个块是驱逐的第一个候选者，无论它位于哪一层。</li>
<li>可配置对应的 Anotator 类型，<code>alluxio.worker.block.annotator.class</code>。有如下 annotation 实现：
<ul>
<li><code>LRUAnnotator</code>：根据最近最少使用的顺序注释块。这是Alluxio的默认注释器。</li>
<li><code>LRFUAnnotator</code>：使用可配置的权重，根据最近最不常用和最不常用的顺序注释块。
<ul>
<li>如果权重完全偏向最近最少使用的，行为将与LRUAnnotator相同。</li>
<li>使用 <code>alluxio.worker.block.annotator.lrfu.step.factor</code> 和 <code>alluxio.worker.block.annotator.lrfu.attenuation.factor</code> 来配置。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="managing-data-replication-in-alluxio">Managing Data Replication in Alluxio</h4>
<h5 id="passive-replication">Passive Replication</h5>
<ul>
<li>与许多分布式文件系统一样，Alluxio中的每个文件都包含一个或多个分布在集群中存储的存储块。默认情况下，Alluxio可以根据工作负载和存储容量自动调整不同块的复制级别。例如，当更多的客户以类型CACHE或CACHE_PROMOTE请求来读取此块时Alluxio可能会创建此特定块更多副本。当较少使用现有副本时，Alluxio可能会删除一些不常用现有副本 来为经常访问的数据征回空间(块注释策略)。 在同一文件中不同的块可能根据访问频率不同而具有不同数量副本。</li>
<li>默认情况下，此复制或征回决定以及相应的数据传输 对访问存储在Alluxio中数据的用户和应用程序完全透明。</li>
</ul>
<h5 id="active-replication">Active Replication</h5>
<ul>
<li>除了动态复制调整之外，Alluxio还提供API和命令行 界面供用户明确设置文件的复制级别目标范围。 尤其是，用户可以在Alluxio中为文件配置以下两个属性:
<ul>
<li><code>alluxio.user.file.replication.min</code> 是此文件的最小副本数。 默认值为0，即在默认情况下，Alluxio可能会在文件变冷后从Alluxio管理空间完全删除该文件。 通过将此属性设置为正整数，Alluxio 将定期检查此文件中所有块的复制级别。当某些块 的复制数不足时，Alluxio不会删除这些块中的任何一个，而是主动创建更多 副本以恢复其复制级别。</li>
<li><code>alluxio.user.file.replication.max</code> 是最大副本数。一旦文件该属性 设置为正整数，Alluxio将检查复制级别并删除多余的 副本。将此属性设置为-1为不设上限(默认情况)，设置为0以防止 在Alluxio中存储此文件的任何数据。注意，<code>alluxio.user.file.replication.max</code> 的值 必须不少于 <code>alluxio.user.file.replication.min</code>。</li>
</ul>
</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<h3 id="testing-alluxio-for-memory-speed-computation-on-ceph-objects">Testing Alluxio for Memory Speed Computation on Ceph Objects</h3>
<ul>
<li>https://blog.zhaw.ch/icclab/testing-alluxio-for-memory-speed-computation-on-ceph-objects/#more-12747</li>
<li>4th SEPTEMBER 2020</li>
</ul>
<h4 id="环境介绍">环境介绍</h4>
<ul>
<li>底层存储：Ceph mimic
<ul>
<li>6 OpenStack VMs
<ul>
<li>one Ceph monitor</li>
<li>three storage devices running Object Storage Devices (OSDs)</li>
<li>one Ceph RADOS Gateway (RGW) node</li>
<li>one administration node</li>
</ul>
</li>
<li>total storage size of 420GiB was spread over 7 OSD volumes attached to the three OSD nodes</li>
</ul>
</li>
<li>Alluxio 2.3， Java8 (换成 Java11 即升级 Alluxio 后会有后续提升)</li>
<li>Spark 3.0.0</li>
<li>两种模式：
<ul>
<li>单 VM 运行 Alluxio 和 Spark （16vCPU，40GB of memory）</li>
<li>集群模式：two additional Spark and Alluxio worker nodes are configured (with 16vCPUs and 40GB of memory).</li>
</ul>
</li>
<li>对比测试：
<ul>
<li>直接访问 Ceph RGW 和 通过 Alluxio 访问
<ul>
<li>通过 Alluxio 访问时，第一次访问文件的话，Alluxio 会将文件上传到内存中，后续的文件访问将直接命中内存，从而带来显著的性能提升。</li>
</ul>
</li>
<li>不同文件大小： 1GB, 5GB and 10GB，记录第一层和第二次访问文件需要的时间。
<ul>
<li>平均会运行超过 10 次</li>
<li>然后再次启动相同的应用程序，以再次测量相同的文件访问时间。这样做的目的是展示内存中的 Alluxio 缓存如何为以后访问相同数据的应用程序带来好处。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="测试结果">测试结果：</h4>
<ul>
<li>如下为单节点测试测试结果，Ceph 上第二次访问该文件相比于 Alluxio 在 1GB,5GB,10GB 时的执行时间分别为 75x，111x，107x<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211113435.png" alt="20201211113435" loading="lazy"></li>
<li>如下为集群模式下的测试结果，所有情况的整体时间比单机的时候少了很多，Ceph 相比于 Alluxio 的第二次访问时间为 35x, 57x, 65x<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211114027.png" alt="20201211114027" loading="lazy"></li>
</ul>
<h3 id="accelerate-and-scale-big-data-analytics-with-alluxio-and-inteloptanetm-persistent-memory">Accelerate And Scale Big Data AnAlytics with Alluxio And intel®optane™ persistent Memory</h3>
<ul>
<li>https://www.alluxio.io/app/uploads/2020/05/Intel-Alluxio-DCPMM-Whitepaper-200507.pdf</li>
</ul>
<h3 id="reliability-testing">Reliability Testing</h3>
<ul>
<li>TODO</li>
</ul>
<h2 id="install-deploy">Install &amp; Deploy</h2>
<h3 id="single-server">Single Server</h3>
<h4 id="download">Download</h4>
<ul>
<li>Download Binary: https://www.alluxio.io/download/</li>
<li>Choose Version. (eg. Alluxio 2.4.1 Release. 1.4GB)</li>
<li>Tar file: <code>tar -xzf alluxio-2.4.1-bin.tar.gz</code></li>
</ul>
<h4 id="initial-config">Initial Config</h4>
<ul>
<li><code>cd alluxio-2.4.1/conf &amp;&amp; cp alluxio-site.properties.template alluxio-site.properties</code></li>
<li><code>echo &quot;alluxio.master.hostname=localhost&quot; &gt;&gt; conf/alluxio-site.properties</code></li>
<li>[Optional] If use local file system, you can specific configuration in conf files like this: <code>echo &quot;alluxio.master.mount.table.root.ufs=/root/shunzi/Alluxio/tmp&quot; &gt;&gt; conf/alluxio-site.properties</code></li>
<li>Validate env: <code>./bin/alluxio validateEnv local</code></li>
</ul>
<pre><code class="language-cmd">2 Errors:
ValidateRamDiskMountPrivilege
ValidateHdfsVersion
</code></pre>
<h4 id="start-alluxio">Start Alluxio</h4>
<ul>
<li>Format journal and storage directory: <code>./bin/alluxio format</code>
<ul>
<li>It may throw exceptions <code>java.nio.file.NoSuchFileException: /mnt/ramdisk/alluxioworker</code> in <code>log/task.log</code>. So you need to <code>mkdir -p /mnt/ramdisk/alluxioworker</code></li>
</ul>
</li>
<li>Start alluxio (with a master and a worker): <code>./bin/alluxio-start.sh local SudoMount</code></li>
<li>Stop local server: <code>./bin/alluxio-stop.sh local</code>
<ul>
<li><code>./bin/alluxio-stop.sh all</code></li>
</ul>
</li>
</ul>
<h5 id="verify">Verify</h5>
<ul>
<li>Access website <code>http://localhost:19999</code> to check the master server status.</li>
<li>Access website <code>http://localhost:30000</code> to check the worker server status.</li>
<li>For internal network, you can use reverse proxy like this: (And you can access website <strong>master</strong> <code>http://114.116.234.136:19999</code> and <strong>worker</strong> <code>http://114.116.234.136:30000</code>)
<ul>
<li><code>autossh -M 1999 -fNR 19999:localhost:19999 root@114.116.234.136</code></li>
<li><code>autossh -M 3000 -fNR 30000:localhost:30000 root@114.116.234.136</code></li>
</ul>
</li>
</ul>
<h5 id="run-tests">Run tests</h5>
<ul>
<li>Verify run status and run test cases: <code>./bin/alluxio runTests</code></li>
<li>The runTests command runs end-to-end tests on an Alluxio cluster to provide a comprehensive sanity check.</li>
<li>It will generate directory <code>/default_tests_files</code> and use different cache policy to upload files.
<ul>
<li>BASIC_CACHE_ASYNC_THROUGH</li>
<li>BASIC_CACHE_CACHE_THROUGH</li>
<li>BASIC_CACHE_MUST_CACHE</li>
<li>BASIC_CACHE_PROMOTE_ASYNC_THROUGH</li>
<li>BASIC_CACHE_PROMOTE_CACHE_THROUGH</li>
<li>BASIC_CACHE_PROMOTE_MUST_CACHE</li>
<li>BASIC_CACHE_PROMOTE_THROUGH</li>
<li>BASIC_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_ASYNC_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_MUST_CACHE</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_ASYNC_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_MUST_CACHE</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_ASYNC_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_CACHE_THROUGH</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_MUST_CACHE</li>
<li>BASIC_NON_BYTE_BUFFER_NO_CACHE_THROUGH</li>
<li>BASIC_NO_CACHE_ASYNC_THROUGH</li>
<li>BASIC_NO_CACHE_CACHE_THROUGH</li>
<li>BASIC_NO_CACHE_MUST_CACHE</li>
<li>BASIC_NO_CACHE_THROUGH</li>
</ul>
</li>
</ul>
<h4 id="simple-example">Simple Example</h4>
<h5 id="upload-files-from-local-server">Upload files from local server</h5>
<ul>
<li>Show fs command help: <code>./bin/alluxio fs</code></li>
<li>List the files in Alluxio: <code>./bin/alluxio fs ls /</code></li>
<li>Copy files from local server: <code>./bin/alluxio fs copyFromLocal LICENSE /LICENSE</code></li>
<li>List again: <code>./bin/alluxio fs ls /</code></li>
<li>Cat the file: <code>./bin/alluxio fs cat /LICENSE</code></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="https://docs.alluxio.io/os/user/stable/cn/overview/Getting-Started.html">[1] Alluxio 快速上手指南</a></li>
<li><a href="https://blog.csdn.net/baichoufei90/article/details/107322069">[2] CSDN - Alluxio学习</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/127118960">[3] 知乎 - 路云飞：Alluxio 技术分析</a></li>
<li><a href="https://www.zhihu.com/column/alluxio">[4] 知乎 - Alluxio 专栏</a></li>
<li><a href="https://www.jianshu.com/p/481675971727">[5] 简书 - Alluxio：架构及数据流</a></li>
<li><a href="https://www.infoq.cn/article/q58xagobiioimqeem-a9">[6] InfoQ - Alluxio在多级分布式缓存系统中的应用</a></li>
<li><a href="https://pdf.dfcfw.com/pdf/H3_AP201912181371929557_1.pdf?1596649255000.pdf">[7] Alluxio 开源 AI 和大数据存储编排平台-顾荣</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees]]></title>
        <id>https://blog.shunzi.tech/post/osdi-Bourbon/</id>
        <link href="https://blog.shunzi.tech/post/osdi-Bourbon/">
        </link>
        <updated>2020-11-06T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2020 From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2020 From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees</li>
</ul>
</blockquote>
<!--more-->
<blockquote>
<ul>
<li>很久没看 LSM tree 的文章了，恰好这几天 OSDI2020 开了，果不其然还是有 LSM-tree 相关的，而且是和学习索引结合的，之前对  Learned Index 也是一知半解，所以拜读了一下。</li>
<li>这篇文章从写作和行文的角度讲写的特别的清楚，实验做的可能是我见过的最充分的，从一开始发现问题就开始用了大量实验来证明，后续的负载测试也极其丰富，回答了读者可能会有的各种各样的问题。值得一读。</li>
</ul>
</blockquote>
<h2 id="abstract">Abstract</h2>
<ul>
<li>Bourbon，一个利用了机器学习来提供快速查询的 LSM-tree</li>
<li>Bourbon 使用贪婪的分段线性回归来学习 key 分布，以最小的计算代价实现快速查找，并根据成本和收益来决定何时学习是值得的。实验表明，查询性能相比于最先进的 LSMs 提升了 1.23x-1.78x</li>
</ul>
<h2 id="introduction">Introduction</h2>
<h4 id="学习索引是个啥">学习索引是个啥？</h4>
<ul>
<li>作者上来先简要介绍了一波机器学习，当然是为了引出重点 <strong>学习索引</strong>，简单地讲，学习索引就是指当你查询一个 key 的时候，系统使用该索引（或者该函数）预测出你要查询的 key 所对应的位置，相比于传统的数据结构中的查找性能有比较大的提升，某些场景下可能提升可能更为明显，同时一定程度上因为不直接构建具体的数据结构节省了空间开销。基于这项工作，很多人提出了更好的模型、更好的树结构来减少对基于树的索引结构的访问和开销。</li>
</ul>
<h4 id="学习索引和-lsms-能擦出啥火花">学习索引和 LSMs 能擦出啥火花？</h4>
<h5 id="两者理论上的矛盾">两者理论上的矛盾</h5>
<ul>
<li>现有的学习索引大多是基于数据库场景中的 B 树来做的，很少有人提出说将学习索引应用到 LSM-tree 上，所以作者就尝试着把学习索引的想法应用到 LSM-tree 上（LSM-tree 的应用就不再具体介绍）。那么问题来了，为什么其他人没想到说把学习索引用到 LSM-Tree 上呢？<strong>主要是因为学习索引主要针对只读设置而量身定做的，而 LSMs 则主要是针对写进行了优化。</strong></li>
<li>听着很抽象？那先简单解释一下。LSM tree 是对写比较友好的，但是写操作会影响学习索引，因为学习索引通常是基于原有的数据学习出来的，现在数据都变了，那你索引肯定得需要做出相应的改变才能保证你索引的准确性，最直接的办法当然是直接再学习。</li>
<li><strong>然而，作者发现 LSMs 非常适合用于学习索引</strong>，虽然写操作修改了 LSM，但树的大多数部分是不可变的；因此，学习一个预测键/值位置的函数只需要完成一次，并且只要不可变数据存在就可以使用它。然而也有别的问题，可变的键或值大小使学习预测位置的函数变得更加困难，过早地执行模型构建可能导致大量的资源浪费。</li>
</ul>
<h5 id="bourbon-做了啥">Bourbon 做了啥</h5>
<ul>
<li>作者研究了 WiscKey，得出了几条 guidelines
<ul>
<li>虽然学习 LSM 中稳定的低级别是有用的，但是学习更高级别也会带来好处，因为查找必须始终先搜索更高级别</li>
<li>并不是所有的文件都是相同的：一些文件即使在较低的级别也是非常短暂的：系统必须避免学习这些文件，否则会浪费资源</li>
<li>工作负载和数据感知非常重要：根据工作负载和数据加载方式，了解树的某些部分可能比了解其他部分更有益</li>
</ul>
</li>
<li>Bourbon 基于 WiscKey 实现，WiscKey 原本大约 20K 行代码，Bourbon 增加了大约 5K 行，使用分段线性回归，这是一种简单但有效的模型，能够在很小的空间开销下实现快速训练(即学习)和推理(即查找)，使用文件学习:模型建立在文件之上，假设一个LSM文件一旦创建，就不会被修改。实现了一个成本效益分析器，它动态地决定是否学习一个文件，在最大化收益的同时减少了不必要的学习。</li>
</ul>
<h2 id="background">Background</h2>
<h3 id="lsm-leveldb">LSM &amp; LevelDB</h3>
<ul>
<li>如下图所示是 LevelDB 和 WiscKey 的原理示意图。具体的介绍请参考其他资料，此处不再详细展开。本文中提到的 higher level 是指存放了更新的数据的 level，lower level 是指存放了更老的数据的 level。</li>
<li>简要介绍查询步骤，如图所示，便于后文引入学习索引：
<ul>
<li>step1. FindFiles：如果 key 在内存中的 tables 中没有找到，LevelDB 将会查找到一组候选的来自磁盘的可能包含键 k 的 sstables。最坏的情况是 k 可能出现在所有 L0 文件中(因为重叠的范围)，并在每个连续级别的一个文件中</li>
<li>step2. LoadIB+FB：对于每一个候选的 SSTable，其索引块和布隆过滤器块首先被加载</li>
<li>step3. SearchIB：对索引块进行二分查找，从而找到可能包含 k 的数据块</li>
<li>step4. SearchFB：查询过滤器判断 k 是否存在对应的 datablock 中</li>
<li>step5. ReadValue：如果 k 在对应的 datablock 中，相应地读取出对应的 value，然后查询结束。如果上一步的 filter 显示该 key 不存在或者对 datablock 查询时没有找到相应的 key，搜索操作将继续在下一个候选文件中执行</li>
<li>NOTICE：blocks 不一定总是从磁盘中加载出来的，index block 和 filter block，以及经常访问的 data blocks 很有可能就在内存中可以被直接访问（文件系统缓存）。</li>
</ul>
</li>
<li>作者对于索引的步骤和数据访问的步骤做了简单区分。 本文目标即为减少索引过程中的开销。
<ul>
<li>FindFiles, SearchIB, SearchFB, and SearchDB 都是通过文件和 blocks 找到对应的键，也就是所谓的 indexing steps</li>
<li>LoadIB+FB, LoadDB, and ReadValue 从存储中读取数据就是所谓的 data-access steps<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201106165224.png" alt="20201106165224" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="wisckey">WiscKey</h3>
<ul>
<li>WiscKey 是为了解决 LSM-Tree 中比较严峻的写放大问题而提出的，架构如上图所示，主要是讲 Key Value 分离，只是对 Key 使用 LSM 存储，Value 直接使用 Value Log 进行存储，因为数据量小了，写放大也就得到了缓解，同时因为比较小就可以缓存在内存中，因此一个查询操作可能最终只涉及到一次 IO 操作来读取 Value Log 上指定位置的 Value</li>
</ul>
<h3 id="optimizing-lookups-in-lsms">Optimizing Lookups in LSMs</h3>
<ul>
<li>因为 LSM Tree 本身结构的原因，对于 LSM Tree 的查询可能需要对多个 level 进行查询，而且 LSM Tree 本能就是以写性能见长的，在读性能方面表现较差，所以对 LSM Tree 的读操作进行优化就很有必要。</li>
<li>受学习索引的启发，现在有很多工作考虑使用机器学习模型来替代传统的索引结构，核心思想是针对输入训练一个模型（使用如线性回归或者神经网络的方法）从而预测出输入对应的记录子啊排序好了的数据集中的具体地址。模型可能有误差，因此预测有一个相关的误差界。在查找过程中，如果模型预测的键的位置是正确的，则返回记录;如果错误，则在错误范围内执行本地搜索。例如，如果预测的位置是 pos，而最小和最大的误差范围是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>i</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">α_min</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">α_max</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span></span></span></span>，那么根据错误的预测，在 pos - <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>m</mi><mi>i</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">α_mmin</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span></span></span> 和 pos + <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mi>m</mi></msub><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">α_max</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span></span></span></span> 之间进行局部搜索。</li>
<li>学习的索引可以大大加快查找速度。直观地说，一个学习过的索引将 b 树的 O(log-n) 查找变成 O(1) 操作。根据实际的经验表明，学习索引提升 B 树的查询性能约 1.5x-3x</li>
<li>传统的学习索引不支持更新，因为在现有数据上学习的模型会随着修改而改变。但是，LSMs 在写密集型工作负载中的高性能很有吸引力，因为它们只按顺序执行写操作。因此提出了关键问题给：如何实现学习索引同时保证 LSM 对写性能带来的提升？</li>
</ul>
<h2 id="learned-indexes-a-good-match-for-lsms">Learned Indexes: a Good Match for LSMs?</h2>
<h3 id="learned-indexes-beneficial-regimes">Learned Indexes: Beneficial Regimes</h3>
<ul>
<li>LSM-Trees 中的查询操作包含索引和数据访问两个方面的操作，如前面章节所述。优化后的索引如学习索引可以减少索引的一些步骤的开销，但是对于数据访问的开销没什么影响。在 WiscKey 中，学习索引可以减少如 FindFiles, SearchIB, and SearchDB 的开销。因此如果索引在总查找延迟中占相当大的比例，学习索引就可以显著提升查询的性能。</li>
<li>首先，当数据集或它的一部分缓存在内存中时，数据访问成本很低，因此索引成本就变得很重要。下图展示了在 WiscKey 中的延迟分解情况。柱状图的第一条显示了全部缓存在内存中的情况，第二条显示了数据存储在 SATA SSD 上的情况。其实第一条就相当于有缓存的情况，数据访问和索引成本对延迟的贡献几乎是相等的，优化索引部分可以将查找延迟降低约 2 倍，当不缓存数据集时，数据访问成本占主导地位，因此优化索引可能产生较小的好处，大约只有 20%。</li>
<li>然而，学习索引并不局限于数据缓存在内存中的场景。它们为当前流行的快速存储设备提供了优势，并且可以在正在出现的更快的设备上发挥更大的作用，如图所示随着设备的升级，即便延迟大大降低，但是索引结构的操作所占的延迟比重越来越大，如在 Optane SSD 中，索引结构的操作占据了大约 44% 的比例，因此优化索引结构的相关操作可以将性能提升约 1.8x。<strong>随着存储器件的发展，学习索引能够发挥的效果也越来越显著</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201106175207.png" alt="20201106175207" loading="lazy"></li>
</ul>
<h3 id="learned-indexes-with-writes">Learned Indexes with Writes</h3>
<ul>
<li>与传统索引相比，学习索引为只读分析工作负载提供了更高的查找性能。：然而，学习索引的一个主要缺点是它们不支持诸如插入和更新之类的修改，因为修改操作改变了数据的分布，所以模型就必须重新学习，对于写密集型的负载就常常需要重建模型，频繁的重建就会导致比较高的开销。</li>
<li>乍一看，学习索引似乎并不适合那些 LSMs 优化的写密集的负载，然而，我们观察到 LSMs 的设计很适合学习索引。我们的关键认识是，<strong>尽管更新可以改变 LSM 树的一部分，但大部分仍然是不变的</strong>。具体来说，新修改的项缓冲在内存结构中，或者存在于树的较高级别中，而稳定的数据驻留在较低级别。考虑到数据集的很大一部分位于稳定的、较低的级别，对这一部分的查找可以更快，而无需或只需进行少量的重新学习。相比之下，在更高层次的学习可能没有那么有效果，它们变化的速度更快，因此必须经常重新学习。</li>
<li>我们还认识到，<strong>SStable 文件的不可变特性使它们成为理想的学习单元</strong>。一旦学习之后，这些文件就再也不会被更新，因此一个模型可以一直被使用，除非该文件被替换。除此以外，SSTable 内的数据还是有序的，有序的数据就可以采用更简单的模型学习，一个级别是许多不可变文件的集合，也可以使用简单的模型作为一个整体来学习。一个级别中的数据也进行了排序：对各个sstable进行了排序，并且在sstable之间不存在重叠的键范围。</li>
<li>进行了一些实验来证明上述结论，实验的目标是确定一个模型在多长时间内是有用的，以及模型使用的频率。只要SSTable 文件存在，为该文件建立的模型就有用，因此，我们首先测量和分析 SSTable 的寿命。一个模型被使用的额频率将由内部查询的次数决定，因此只需要测试每个文件的内部查询次数即可，因为模型也可以基于整个 level 构建，所以作者也测试了 level 的 lifetimes，实验是基于 WiscKey 做的，但作者认为对应的实验结论也应该适用于大多数 LSM 的实现。</li>
</ul>
<h4 id="sstable-lifetimes">SSTable Lifetimes</h4>
<ul>
<li>下图 a 显示了不同层级的 SSTables 文件的平均寿命（寿命通过使用文件的创建时间和删除时间来衡量）。
<ul>
<li>较低级别的 SSTable 文件的平均寿命大于较高级别的。</li>
<li>在较低的写比例的负载下，即使是较高级别的文件也有相当长的生存期，但较低级别的寿命此时更长</li>
<li>即便随着更高的写比例导致文件的寿命下降，但是对于低级别的文件而言，寿命还是很长</li>
</ul>
</li>
<li>图 b 展示了 5% 的写比例的情况下 L1 和 L4 的寿命的分布情况， 可以发现有的文件寿命非常短，有的文件寿命非常长。如 L1 的大约 50% 的寿命只有 2.5s，如果过了该临界值，寿命就会变得特别长，超过五分钟。而对于 L4，只有很少的文件寿命很短，大约有 2% 的寿命不超过 1s，造成该现象的原因可能是：
<ul>
<li>压缩一个 Li 层的文件会在 Li+1 中创建一个新文件，该文件会被立即选择用于压缩到下一个级别</li>
<li>压缩一个 Li 层的文件会在 Li+1 中创建一个新文件，该文件与从 Li 压缩的下一个文件有重叠的键范围</li>
</ul>
</li>
<li>图 c 展示了不同写请求比例下 L1 和 L4 的寿命分布，规律和 5% 时大体相同。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107111926.png" alt="20201107111926" loading="lazy"></li>
<li>于是乎总结出了两条 guidelines：
<ul>
<li>Favor learning files at lower levels：学习索引最好用于较低层次的文件，因为这些文件的寿命通常比较长</li>
<li>Wait before learning a file：学习索引最好在某个文件存活的时间达到一定阈值之后才开始学习，因为有的文件寿命可能很短，即便是在一些较低层次的文件，因为存活持续了一段时间之后该文件才可能存活的比较长时间。</li>
</ul>
</li>
</ul>
<h4 id="number-of-internal-lookups-per-file">Number of Internal Lookups Per File</h4>
<ul>
<li>为了测试模型的使用频率，就分析了 SSTables 对应的内部查询次数。如下图所示，图 a 显示了数据集以一个随机顺序加载的情况，较高层次对应的总的内部查询次数更多，即便是很大一部分数据驻留在较低的级别上（图 (a)(ii) 则显示了查询不命中的情况），而如图(a)(iii)所示的查询命中的情况时候，低级别的文件的查询次数更多。结果表明更高级别的文件通常服务于一些不命中也就是 negative 的查询操作，虽然采用了 BloomFilter 来尽可能加速 negative lookup 的过程，但是 index block 在查询 filter block 之前还是会被查询。</li>
<li>同时还针对 zipfian 的负载（大多数请求都是针对一小组键的）下进行了同样的测试，结果表明大多数情况下都和随机加载的负载是相似的，除了 positive lookup，如图 (a)(iv)，在 zipfian 负载下，更高层级的文件处理更多的 positive lookups，因为负载经常访问一小组常被更新的键，因此这组键被常常存储在更高的 level 上。</li>
<li>图 b 显示了数据集被顺序加载（keys 按照升序或者降序的方式被插入）的情况，相比于随机加载的情况，就没有了 negative lookups，因为不同 SSTable 的键不会重叠，即便是在不同的 level 上也不会重叠，FindFiles 步骤可以直接找到可能包含该 key 的唯一的文件。因此，较低的级别提供更多的查找，并可以从学习索引中获得更多好处。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107113529.png" alt="20201107113529" loading="lazy"></li>
<li>从上述的实验观察中也得到了两个 guidelines：
<ul>
<li>Do not neglect files at higher levels：即便更底层次的文件寿命更长，处理的查询也更多，但是更高层次的文件在某些负载下也是可能处理很多 negative 甚至 positive 的查询请求的，因此学习索引在高层次的文件中也能让内部查询更快</li>
<li>Be workload- and data-aware：尽管大多数数据位于较低的级别，但如果工作负载不查找这些数据，那么学习这些级别带来的收益就很悠闲；因此，学习索引必须能够感知工作负载的情况。除此以外，数据被加载的顺序性也会影响那些层处理更多的内部查询请求，即会影响请求处理的层级分布情况，因此学习索引还需要感知数据的情况。内部查询请求次数可以同时代表工作负载和数据加载顺序，所以基于请求次数就可以动态地决定是否要学习某个文件。</li>
</ul>
</li>
</ul>
<h4 id="lifetime-of-levels">Lifetime of Levels</h4>
<ul>
<li>前面章节有描述过一整个层级也是可以被学习的，所以作者测试分析了整个层级的寿命。</li>
<li>因为 L0 层无序，L0 层的文件可能有重叠的键范围，所以不能应用层级学习的策略。一旦一个层级被学习了之后，任何对于该层级的更新都可能导致重学习，而层级更新则是指新的 SSTable 文件在该层次被创建，或者一个已经存在的被删除，因此，一个层级的的寿命与单个 SSTable 相同或更短。在层级的粒度上进行学习的好处是不需要在单独的步骤中找到候选 SSTables，而是在查询时候模型直接输出对应的 SSTable 和文件内的偏移。</li>
<li>下图 a 展示了在 5% 的写比例下不同层级的文件变化情况，纵轴上的 0 表示当前时间该层级没有发生变化，此时则可以进行学习。如果大于 0 则表示该层级发生了变化，因此就需要进行重新学习。更高层次对应的文件变化频率更高。随着级别的下降，更改的文件的比例会减少，因为较低的级别在许多文件中包含大量数据</li>
<li>对层级文件的更改通常是突发性的。该突发通常是由压缩引起的该层级中的很多文件同时被修改，因此这些突发的时机在不同层次表现的时间基本相同。这背后的原因是，对于我们使用的数据集，级别 L0 到 L3 已经满了，因此任何在一层上的压缩都会导致<strong>级联压缩</strong>，最终在未满的 L4 级别上存放。在这些突发情况之间层级的文件基本会保持平稳不会发生变化。</li>
<li>但是随着写请求比例的上升，突发间隔会逐渐减小，如图 b 所示，层级的平稳周期将大幅减小。如图所示 5% 时，大约有 5 分钟的周期，在 50% 的时候，就只有大约 25s 了。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107115847.png" alt="20201107115847" loading="lazy"></li>
<li>基于上述实验，又总结了一条 guidelines
<ul>
<li>Do not learn levels for write-heavy workloads：当写请求比例较低的时候，学习一整个层级还算比较合适，但是写密集型的负载，因为层级寿命变得很短就可能导致很频繁的重新学习。</li>
</ul>
</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>通过对 WiscKey 的实验分析，总结了五条 guidelines:
<ul>
<li>Favor learning files at lower levels</li>
<li>Wait before learning a file</li>
<li>Do not neglect files at higher levels</li>
<li>Be workload- and data-aware</li>
<li>Do not learn levels for write-heavy workloads</li>
</ul>
</li>
</ul>
<h2 id="bourbon-design">Bourbon Design</h2>
<h3 id="learning-the-data">Learning the Data</h3>
<ul>
<li>回顾学习索引的目标：预测 key 在一个有序的数据集中的位置。</li>
<li>本文设计了两种学习索引，对应学习的粒度不同。
<ul>
<li>File Learning：预测 key 对应的在文件内的偏移</li>
<li>Level Learning：预测出对应的 SSTable 文件和文件内的偏移</li>
</ul>
</li>
<li>对于学习索引的要求：无论是学习过程还是查询过程，开销都需要很低才能真正优化整个系统。除此以外，因为优化的是磁盘上的数据结构对应的存储系统，空间的开销也需要尽可能的小。作者发现分段线性回归（PLR）能够同时满足上面的要求。PLR 的本质是用一些线段来表示有序的数据集，PLR 构造了一个有误差限制的模型，每个数据点 d 必须在范围 [<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> − δ, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> + δ] 内，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 数据集中的 d 预测的位置，δ 是提前定义好的误差限制。</li>
<li>为了训练 PLR 模型，Bourbon 方案使用了 Greedy-PLR 算法，一次处理一个数据点，如果数据点不能在不超出定义的误差限制的情况下被添加到当前的线段中，那么将创建一个新的线段，并将数据点添加到其中，最终 Greedy-PLR 生成了一组代表数据的线段。Greedy-PLR 的运行时间与数据点的数量呈线性关系。</li>
<li>一旦模型学习完成，推理就会很快。首先，找到包含键的正确线段(使用二分查找)。在该线段内，目标键的位置是通过将键与直线的斜率相乘并加上截距得到的。如果键不在预测的位置，在误差范围内进行局部查询。因此查询操作除了常数时间做局部搜索，只需要花费 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>s</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logs)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">s</span><span class="mclose">)</span></span></span></span> 的时间，其中 s 是线段的数量。PLR 的空间开销很小：每个线段只有几十个字节。</li>
<li>其他模型诸如 RMI，PGMIndex，splines 等可能更适合 LSMs 且提供比 PLR 更好的表现，未来可以采用这些模型来进行实现。</li>
</ul>
<h3 id="supporting-variable-size-values">Supporting Variable-size Values</h3>
<ul>
<li>如果 KV 对大小相同的话，学习索引预测 KV 对的偏移量将会很容易，模型可以将 key 的预测位置乘以 KV 对的大小，从而产生最终的偏移量。但是对于许多系统而言，往往允许任意大小的 KV 对。</li>
<li>Bourbon 要求 Key 是固定大小的，但是 Value 可以是不固定的。作者认为这是一个合理的设计，因为大多数数据集有确定大小的 key，比如 user-id 通常有 16bytes，但是 value 的大小就不固定了。即使 keys 大小不同，可以填充使所有 keys 的大小相同。Bourbon 通过借鉴WiscKey的键值分离思想来支持可变长度的 value。</li>
<li>Key Value 分离的方式，Bourbon 中的 SSTables 就只会包含 key 和对应的指向 value 的指针，value 被单独维护在 value log 中，在这样的模式下，Bourbon 通过从模型中得到预测的位置，然后乘以对应的记录大小（通常是 keySize  + pointerSize），从而获取要访问的 KV 对的偏移量，Value 指针用作 Value Log 的偏移量，最终从该日志读取值。</li>
</ul>
<h3 id="level-vs-file-learning">Level vs. File Learning</h3>
<ul>
<li>前面的分析表明文件的寿命比层级的寿命通常更长，特别是在写密集的负载下，也就意味着以文件的粒度进行学习可能是更好的选择。作者为了在 file 和 level 之间进行权衡测试了不同负载下对应的性能，初始化的时候都加载一个数据集并构建模型，对于只读负载，模型不需要重新学习，在混合负载中，因为数据的改变模型需要重新学习。</li>
<li>如下表所示，混合负载下，Level 明显不如 File，因为有稳定的写入流，系统无法对 Level 进行学习。只有 1.5% 的内部查找采用模型路径；这些查找是在加载数据之后以及初始的 Level 模型可用时执行的。作者观察到所有尝试的 66 次 level 学习都失败了因为在学习完成之前 level 已经发生了改变。由于重新学习的额外成本，level 学习的性能甚至比 50% 写操作的基线还要差.而使用 file model，大比例的查询操作都能从学习索引中获益，因此 file model 相比于基线性能有所提升。</li>
<li>对于读敏感的负载（ 5% 的写），尽管 level model 相比于基线有一定的提升，但还是比 file model 性能表现要差，原因还是因为重新学习的额外成本和仅作用了有限的查询操作，带来的提升有限。</li>
<li>只有在只读负载下，Level 学习才能带来比较大的提升，甚至比起 file learning 都提升了 10%，因此，只有只读工作负载的部署可以从层级学习中获益。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107155836.png" alt="20201107155836" loading="lazy"></li>
<li>鉴于 Bourbon 的目标是在支持写操作的同时提供更快的查找，对于学习粒度来说，level 并不是一个合适的选择，所以 Bourbon 默认使用文件学习，但同时也会支持 level 学习以便适应只读负载。</li>
</ul>
<h3 id="cost-vs-benefit-analyzer">Cost vs. Benefit Analyzer</h3>
<ul>
<li>因为还是有部分文件的寿命较短，对这类文件的学习可能是对资源的浪费，所以需要开销和收益的分析机制来决定是否要对某一个文件进行学习。</li>
</ul>
<h4 id="wait-before-learning">Wait Before Learning</h4>
<ul>
<li>从前文的 Guidelines 中了解到，需要设定一个等待时间的阈值 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，即在学习一个文件之前，需要等待相应的时间，该系数的具体值体现了开销和收益的权衡。值太小，导致一些寿命较短的文件也被学习，引入了较大的开销，值太大导致执行大量的查询的时候，因为模型还未学习构建，导致大量的查询不能通过学习索引来进行优化，导致性能下降。</li>
<li>BOURBON 将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设置为学习一个文件大概所需要的时间。测试发现学习一个文件（最大 4MB）的最长时间大约为 40ms，作者保守地将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设置为 50ms</li>
</ul>
<h4 id="to-learn-a-file-or-not">To Learn a File or Not</h4>
<ul>
<li>虽然现在有了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>w</mi><mi>a</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{wait}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，但是一个文件即便是存活了很长时间但是可能也不是特别有益的。作者实验发现更低级别的文件通常寿命更长，对于有的工作负载和数据集，他们服务的查询操作比更高级别的文件要少得多，更高级别的文件尽管寿命较短，但是在有的场景下服务了大量的 negative lookups。因此除了考虑模型对应的开销以外，还需要考虑模型可能带来的收益。如果一个模型的收益（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）大于构建该模型的开销（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）那么该模型就是有利的。</li>
</ul>
<h5 id="estimating-c_model">Estimating <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></h5>
<ul>
<li>评估开销的一种方式是假设学习过程完全是在后台完成的且不会影响系统其他部分，那么开销就为 0，如果有很多空闲的 core，学习线程可以利用它们，这样就不会干扰前台任务（工作负载的处理或者压缩过程等）。但是 Bourbon 采用了一种比较保守的办法并且假设学习线程会干扰和减慢系统的其他部分，所以，Bourbon 假设开销等于为单个文件构建 PLR 模型的时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>b</mi><mi>u</mi><mi>i</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{build}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，我们发现，这个时间与文件中的数据点数量成线性比例，因此可以通过将训练一个数据点的平均时间和该文件中包含的点的数量相乘从而得到该时间。</li>
</ul>
<h5 id="estimating-b_model">Estimating <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></h5>
<ul>
<li>评估模型带来的收益相对比较复杂，直观地说，模型为内部查找提供的好处由 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mi>b</mi></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">T_b−T_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 给出，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">T_b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">T_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 分别是基线和模型路径中查找的平均时间。如果一个文件在生命周期中服务了 N 个查询请求，那么该模型的净收益即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>T</mi><mi>b</mi></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mi>m</mi></msub><mo>)</mo><mo>∗</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">(T_b−T_m) * N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>。作者将查询操作又划分成了 negative 和 positive，因为大多数 negative 的查询操作在 filter 处就终止了，所以最终的收益模型为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mo>(</mo><mo>(</mo><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub><mo>)</mo><mi mathvariant="normal">∗</mi><msub><mi>N</mi><mi>n</mi></msub><mo>)</mo><mo>+</mo><mo>(</mo><mo>(</mo><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub><mi mathvariant="normal">−</mi><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub><mo>)</mo><mi mathvariant="normal">∗</mi><msub><mi>N</mi><mi>p</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">B_{model} = ((T_{n.b} −T_{n.m}) ∗N_n)+((T_{p.b} −T_{p.m}) ∗N_p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">N_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">N_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 则是 negative lookup 和 positive lookup 的数量，T 为对应分类下的时间。</li>
<li>如果不知道文件将执行的查找次数或查找将花费的时间，则无法计算文件的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。因此分析器为了预估这些指标，维护了这些文件在他们生命周期内的统计信息，为了估计文件 F 的这些指标，分析器使用与 F 处于同一级别的其他文件的统计数据，我们只在同一层次上考虑统计数据，因为这些统计数据在不同层次上差异很大。</li>
<li>Bourbon 在学习一个文件之前的等待时间里，查询操作可能在 baseline 的路劲中被服务处理，Bourbon 将把该过程的处理时间作为基线的查询处理时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{n.b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{p.b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>n</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{n.m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>p</mi><mi mathvariant="normal">.</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{p.m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mtight">.</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 将使用同一层级的其他文件的平均模型查询时间来进行估计。对于 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">N_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">N_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>，分析器首先获取该级别中其他文件的平均 negative 查找和 positive 查找，然后，将其按 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>=</mo><mi>s</mi><mi mathvariant="normal">/</mi><msub><mi>s</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">f = s/s_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的倍数进行缩放，其中 s 是文件的大小，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>s</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">s_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是该层级的文件的平均大小。在估计上述数量的时候，Bourbon 将会过滤掉寿命较短的文件。</li>
<li>当模型开始引导时，分析器可能还没有足够的统计信息，所以，初始化的时候，Bourbon 以 always-learn 的模式来运行，一旦足够的统计信息收集到了之后，分析器就可以开始执行开销和收益的权衡，来判断 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的大小关系来决定是否学习某一个文件。如果同时选中了多个文件进行学习，Bourbon 则把多个文件放在一个最大优先级队列中，按照 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>−</mo><msub><mi>C</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{model} - C_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的顺序进行排序，因此优先级最高的文件对应的收益也就最大。</li>
</ul>
<h4 id="bourbon-putting-it-all-together">Bourbon: Putting it All Together</h4>
<ul>
<li>总体的流程如下图所示，主要分为两条路径：model exist 和 no model 对应的 baseline。baseline 和前文描述的 LevelDB 的检索方式基本相同，只是此处采用了 WiscKey 的方式来布局。</li>
<li>对于学习索引的查询方式，步骤如下：
<ul>
<li>step1. FindFiles：因为使用了文件学习，所以该步骤需要执行，即找到候选的 SSTables 文件</li>
<li>step2. LoadIB+FB：BOURBON 加载了 filter 和 index block，这些块可能已经被缓存在内存中了</li>
<li>step3. Model Lookup：FB： BOURBON 在候选的 SSTables 文件中对要查询的 Key 进行检索，模型相应地输出键 k 对应的文件内偏移 pos 和误差边界 δ。然后 BOURBON 计算包含记录 pos−δ 到 pos+δ 的数据块</li>
<li>step4. SearchFB：首先检查该数据块对应的 filter block 来判断 k 是否存在，如果存在，则 BOURBON 计算要加载的块对应的字节范围（因为 keys 和 pointers 大小固定，计算相对简单）</li>
<li>step5. LoadChunk：加载对应的字节范围</li>
<li>step6. LocateKey：键位于加载的块中，那么该 key 将位于预测的位置上（加载的 chunk 的中点）；如果不在，BOURBON 将会对该 chunk 执行二分查找</li>
<li>step7. ReadValue：使用相应的 Value Pointer 从 ValueLog 中读取对应的 Value<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107165512.png" alt="20201107165512" loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="possible-improvements">Possible improvements</h5>
<ul>
<li>BOURBON 少了一些 features，现有的实现中，不支持字符串 keys 以及 keys 的压缩
<ul>
<li>对于字符串键，我们计划探索的一种方法是将字符串视为base-64整数，并将它们转换为64位整数，然后可以采用本文描述的相同学习方法。虽然这种方法可以很好地用于小 keys，但是大 keys 可能需要更大的整数(大于64位)，因此高效的大整数数学可能是必不可少的。</li>
</ul>
</li>
<li>BOURBON 暂时不支持 level 和 file model 的切换，目前只是一个静态配置。</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>测试之前抛出了几个关键问题：
<ul>
<li>BOURBON 究竟优化了查询的哪些部分？</li>
<li>BOURBON 在有模型但没有写的情况下表现如何？性能会随着数据集、加载顺序以及请求分布的变化发生什么样的变化？</li>
<li>BOURBON 范围查询表现如何？</li>
<li>在有写的情况下，BOURBON 的开销-收益分析器和其他从不重新学习的方法相比表现如何？</li>
<li>BOURBON 在真实负载下的性能也能表现得和预期一致吗？</li>
<li>数据存储在存储设备上时（不再在内存） BOURBON 是否有用？</li>
<li>有限的内存的情况下，BOURBON 是否有用？</li>
<li>BOURBON 在误差范围和空间开销上的权衡是怎么样的？</li>
</ul>
</li>
</ul>
<h3 id="测试环境">测试环境</h3>
<h4 id="硬件环境">硬件环境</h4>
<ul>
<li>20-core Intel Xeon CPU E5-2660</li>
<li>160-GB memory</li>
<li>a 480GB SATA SSD</li>
</ul>
<h4 id="系统参数">系统参数</h4>
<ul>
<li>16B integer keys and 64B values</li>
<li>error bound - 8</li>
<li>Unless specified, our workloads perform 10M operations.</li>
</ul>
<h4 id="负载">负载</h4>
<ul>
<li>构造了四个合成数据集，64M key-value pairs
<ul>
<li>linear, 键都是连续的</li>
<li>segmented-1%, 在连续的100个键之后有一个间隙</li>
<li>segmented-10% , 在10个连续的键之后会有一个间隙</li>
<li>normal，从标准正态分布 N(0,1) 中抽样 64M 个唯一值，并按比例缩放到整数</li>
</ul>
</li>
<li>真实负载：Amazon reviews (AR) &amp; New York OpenStreetMaps (OSM)</li>
</ul>
<h3 id="测试结果">测试结果</h3>
<ul>
<li>
<p><strong>BOURBON 究竟优化了查询的哪些部分？</strong></p>
<ul>
<li>减少了索引花费的时间。图中标记为 Search 的部分对应基线中的 SearchIB 和 SearchDB</li>
<li>还降低了数据访问成本，因为 BOURBON 加载的字节范围比基线加载的整个块要小。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107172404.png" alt="20201107172404" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>BOURBON 在有模型但没有写的情况下表现如何？性能会随着数据集、加载顺序以及请求分布的变化发生什么样的变化？</strong> 无论哪种情况 BOURBON 都可以提供显著的加速</p>
<ul>
<li>对于所有数据集， BOURBON 比 WiscKey 更快；根据数据集的不同，提升也不同(1.23倍到1.78倍)。
<ul>
<li>BOURBON 对线性数据集提升最大，因为它有最小的片段数(每个模型一个)；使用更少的段，找到目标需要检索的段也就更少。</li>
<li>延迟随着段数的增加而增加</li>
</ul>
</li>
<li>level learning 适用于只读负载，BOURBON-level 比基线快1.33 - 1.92倍，比 BOURBON 更好，因为 level 学习查找对应的 SSTables 更快。
<ul>
<li>由于 level 模型只对只读工作负载提供好处，并且与文件模型相比最多提高10%，所以后续测试主要针对 file learning。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107172755.png" alt="20201107172755" loading="lazy"></li>
</ul>
</li>
<li>负载加载顺序的差异，使用顺序加载，sstable 甚至在不同的级别上都不会有重叠的键范围;然而，在随机加载的情况下，某个级别的 sstable 可能会与其他级别的 sstable 重叠。
<ul>
<li>无论负载顺序如何，BOURBON 都比基线有显著的优势（1.47× – 1.61×）。</li>
<li>与顺序加载情况相比，随机加载情况下的平均查找延迟有所增加。这是因为，虽然在顺序情况下没有 negative 的内部查找（10M），但在随机情况下有很多 negative 的查找（23M）</li>
<li>在随机情况下，对基线的加速比顺序情况下要小。虽然 BOURBON 同时优化了 positive 查找和 negative 查找，但 negative 查找的收益较小，因为 negative 的查询路径比较短，在 filter 处可能就终止了，没有加载或搜索数据块，而且 negative 的查询比 positive 的查询多。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107173423.png" alt="20201107173423" loading="lazy"></li>
</ul>
</li>
<li>请求的分布情况，测试了六种请求分布下的延迟，sequential, zipfian, hotspot, exponential, uniform, and latest。
<ul>
<li>BOURBON 使查找速度比基线快1.54倍- 1.76倍。总的来说，无论请求分布如何，BOURBON 都减少了延迟。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107174316.png" alt="20201107174316" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>BOURBON 范围查询表现如何？</strong></p>
<ul>
<li>如图展示了 BOURBON 的吞吐量标准化到了 WiscKey 之后的结果。对于较短的范围，索引开销(即查找范围的第一个键的开销)占主导地位，BOURBON 优化效果比较明显，但随着范围的增大，其效果就不再那么明显，这是因为BOURBON 可以加速索引部分，但它遵循与 WiscKey 类似的路径来扫描后续键。因此，在大范围查询时，索引查询占较少的总性能，性能提升就不明显了。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107174537.png" alt="20201107174537" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>在有写的情况下，BOURBON 的开销-收益分析器和其他从不重新学习的方法相比表现如何？</strong></p>
<ul>
<li>首先了解三种策略：
<ul>
<li>BOURBON-offline 在写发生时不执行学习，模型仅针对最初加载的数据而存在</li>
<li>BOURBON-always 只要写发生了就会重新学习，不考虑成本</li>
<li>BOURBON-cba 会使用开销-收益分析器来决定是否学习</li>
</ul>
</li>
<li>图 a 显示了在前端查询和插入花费的时间，图 b 显示了学习过程花费的时间，图 c 显示了总的时间开销，图 d 显示了采用基线路径的内部查找的比例</li>
<li>结果表明：
<ul>
<li>所有的策略都相比于 WiscKey 减少了前台任务的时间开销，随着写比例的增加，查询比例的减小，优化的效果就减弱。offline 的策略表现最差，即便是在只有 1% 的写的情况下，这时候大多都是通过基线的索引查询方式（如图 d 所示的比例）来进行，所以对于数据改变之后的重新学习十分关键</li>
<li>BOURBON-always 在前台任务的时间开销上表现最好，几乎不会退化成基线的查询方式，但是对应的学习时间就特别长，在 50% 时候就大概花费 134s 进行学习，所以总的时间开销当写请求的比例较大的时候，可能比基线的时间开销还大</li>
<li>写请求比例较低的时候，BOURBON-cba 几乎会学习所有的文件，所以此时和 always 的表现比较相近，当写请求比例增大时，BOURBON-cba 不再学习特别多的文件，学习时间开销只有 13.9s，相比于 always 大大减小，因此这时候的很多查询都是按照基线对应的路径，因为此时数据变化迅速，查找次数较少，学习的收益较小。</li>
</ul>
</li>
<li>总结：积极学习策略提供快速查找，但代价高昂；没有 re-learning 的话几乎不能加快速度。也不理想。相比之下，BOURBON 提供了与积极学习类似的高收益，同时显著降低了总成本<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107183741.png" alt="20201107183741" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>BOURBON 在真实负载下的性能也能表现得和预期一致吗？</strong></p>
<ul>
<li>YCSB：BOURBON 提高了读操作的性能;同时，波旁威士忌并不影响写的表现。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107185902.png" alt="20201107185902" loading="lazy"></li>
<li>SOSD<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107185921.png" alt="20201107185921" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>数据存储在存储设备上时（不再在内存） BOURBON 是否有用？</strong></p>
<ul>
<li>即使数据存在存储设备上，BOURBON 也有一定程度的提升(查询速度比WiscKey快1.25倍到1.28倍)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107191601.png" alt="20201107191601" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107191738.png" alt="20201107191738" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>有限的内存的情况下，BOURBON 是否有用？</strong></p>
<ul>
<li>如下表所示，使用了 SATA SSD 和大小只有数据量的 25% 的内存，BOURBON 速度只有 WiscKey 的 1.04 倍，因为大部分时间都花在了将数据加载到内存上。</li>
<li>相比之下，在 zipfian 工作负载中，索引时间（而不是数据访问时间）占主导地位，因为大量请求访问已经缓存在内存中的一小部分数据，所以能够提供 1.25x 的加速和低得多的延迟。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107191910.png" alt="20201107191910" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>BOURBON 在误差范围和空间开销上的权衡是怎么样的？</strong></p>
<ul>
<li>随着误差范围增大，更少的线段被创建，从而需要检索的线段就更少，延迟也就相应减小，然而当 δ 超过 8，尽管需要检索的分段更少，但是延迟增加了，对于其他数据集也是 δ = 8 是转折点。还显示了空间开销，因为创建的线段更少了，空间开销也就更小了。</li>
<li>对于各种数据集，开销与数据集的总大小相比是很小的(0% - 2%)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201107192311.png" alt="20201107192311" loading="lazy"></li>
</ul>
</li>
</ul>
<h2 id="related-work">Related Work</h2>
<ul>
<li><strong>Learned Index</strong>:
<ul>
<li>XIndex</li>
<li>FITingTree</li>
<li>AIDEL</li>
<li>Alex</li>
<li>SageDB</li>
</ul>
</li>
<li><strong>LSM optimizations</strong>:
<ul>
<li>Monkey</li>
<li>Dostoevsky</li>
<li>HyperLevelDB</li>
<li>bLSM</li>
<li>cLSM</li>
<li>RocksDB</li>
</ul>
</li>
<li><strong>Model choices</strong>:
<ul>
<li>Greedy-PLR</li>
<li>Neural networks</li>
<li>one-pass learning algorithm based on splines</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[intelligent cache research]]></title>
        <id>https://blog.shunzi.tech/post/intelligent-cache-research/</id>
        <link href="https://blog.shunzi.tech/post/intelligent-cache-research/">
        </link>
        <updated>2020-10-25T15:59:49.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>本篇为智能缓存相关的研究调研，可能涉及 AI for System 以及相关缓存策略的设计</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>本篇为智能缓存相关的研究调研，可能涉及 AI for System 以及相关缓存策略的设计</li>
</ul>
</blockquote>
 <!-- more -->
<h2 id="smart-ssd-cache">Smart SSD Cache</h2>
<blockquote>
<ul>
<li>智能缓存顾名思义为不同的负载下提供相应的缓存策略来保证缓存的高效，主要包含多种缓存策略执行和 IO 负载的捕捉两个方面，同时可能结合不同的缓存层级需要进行动态调整。</li>
<li>缓存策略本身很难有普适的，现有做法和方案常常需要根据 IO 负载来进行决策，而对于 IO 负载的建模往往采用统计或者机器学习的方法</li>
</ul>
</blockquote>
<h3 id="oceanstor-v5-系列-v500r007-smartcache">OceanStor V5 系列 V500R007 SmartCache</h3>
<ul>
<li>参考 https://support.huawei.com/enterprise/zh/doc/EDOC1000181455/1064ba78</li>
</ul>
<h4 id="定义">定义</h4>
<ul>
<li>华为技术有限公司开发的 SmartCache 特性又叫智能数据缓存特性。</li>
<li><strong>缓存池化</strong>：利用 SSD 盘对随机小I/O读取速度快的特点，将 SSD 盘组成智能缓存池，将访问频率高的随机小I/O读热点数据从传统的机械硬盘移动到由 SSD 盘组成的高速智能缓存池中。由于 SSD 盘的数据读取速度远远高于机械硬盘，所以 SmartCache 特性可以缩短热点数据的响应时间，从而提升系统的性能。</li>
<li><strong>多粒度</strong>：SmartCache 将智能缓存池划分成多个分区，为业务提供细粒度的SSD缓存资源，不同的业务可以共享同一个分区，也可以分别使用不同的分区，各个分区之间互不影响。从而向关键应用提供更多的缓存资源，保障关键应用的性能。</li>
</ul>
<h4 id="场景">场景</h4>
<ul>
<li>SmartCache特性对LUN（块业务）和文件系统（文件业务）均有效。</li>
<li>SmartCache特性可以提高业务的<strong>读性能</strong>。尤其是存在热点数据，且读操作多于写操作的随机小I/O业务场景。例如：OLTP（Online Transaction Processing ）应用、数据库、Web服务、文件服务应用等。</li>
</ul>
<h4 id="原理">原理</h4>
<ul>
<li>SmartCache特性在对SSD盘资源进行管理上，分为智能缓存池和SmartCache分区两部分。</li>
</ul>
<h5 id="智能缓存池">智能缓存池</h5>
<ul>
<li>智能缓存池管理本控制器的所有SSD盘，用以保证每个智能缓存分区的资源来自不同SSD盘，从而避免不同SSD盘负载不均衡。</li>
<li>存储系统默认在每个控制器上生成一个智能缓存池。</li>
</ul>
<h5 id="读流程">读流程</h5>
<ul>
<li>SSD Cache 缓存命中<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021162117.png" alt="20201021162117" loading="lazy"></li>
<li>SmartCache读未命中
<ul>
<li>当从 HDD 读取到 RAM Cache 后，RAM Cache 将数据返回给应用服务器，同时 RAM Cache 将该数据同步到智能缓存池中。当智能缓存池容量不够时，则智能缓存池根据时间顺序释放旧数据，释放数据内存，完成旧数据的淘汰。</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021162235.png" alt="20201021162235" loading="lazy"></figure>
<h5 id="smartcache分区">SmartCache分区</h5>
<ul>
<li>SmartCache分区负责为业务提供细粒度（4KB、8KB、16KB、32KB、64KB、128KB，与前端I/O自适应，即根据前端下发的I/O大小申请不同粒度的SSD缓存资源）的SSD缓存资源。</li>
<li>每两个控制器创建一个默认的SmartCache分区。除了默认分区外，每两个控制器最多支持创建8个用户自定义分区。</li>
<li>通过SmartCache分区调控，各业务独立使用所分配的SmartCache分区，避免不同类型应用之间的相互影响，保障存储系统整体的服务质量。可以通过设置SmartCache大小，实现不同业务与性能的最佳匹配。通过限制非关键应用的缓存资源，向关键应用提供更多的缓存资源，保障关键应用的性能。</li>
<li>SmartCache分区负责为业务提供细粒度的SSD盘缓存资源，不同的业务可以共享同一个分区，也可以分别使用不同的分区，各个分区之间互不影响。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021162705.png" alt="20201021162705" loading="lazy"></figure>
<h3 id="machine-learning">Machine Learning</h3>
<h4 id="date20-a-machine-learning-based-write-policy-for-ssd-cache-in-cloud-block-storage">DATE20 - A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage</h4>
<ul>
<li><strong>发现</strong>：作者在腾讯云的云存储环境中（主要是块存储系统），测试统计发现大约有 47.9% 的写操作是 write-only 的，即在某一个确定的时间窗口内所写的这些块不会再次被访问。那么这些 write-only 的写操作对应的数据放置在缓存中其实并不会带来性能的提升，反而会占据缓存容量，影响其他真正需要缓存的数据来进行缓存。</li>
<li><strong>问题</strong>：现有的写策略大致分为两种：
<ul>
<li>将所有要写的数据加载到缓存中（write-back 和 write through）
<ul>
<li>write-back：在数据更新时只写入缓存Cache。只在数据被替换出缓存时，被修改的缓存数据才会被写到后端存储。此模式的优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回。</li>
<li>Write-through（直写模式）在数据更新时，同时写入缓存Cache和后端存储。此模式的优点是操作简单；缺点是因为数据修改需要同时写入存储，数据写入速度较慢。</li>
</ul>
</li>
<li>不加载数据到缓存，直接写存储设备 （write-around）</li>
</ul>
</li>
<li><strong>方案</strong>：提出了一种基于机器学习的方法来识别 write-only 数据和 normal 数据，针对不同类型的数据动态应用不同的写策略。最大的挑战是怎么样才能够实时地区分数据类型<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200714201124.png" alt="image" loading="lazy"></li>
<li>使用了五种监督学习方法：Naive Bayes, Logistic Regression, Decision Tree, AdaBoost, and Random Forest。随机森林准确率最高，但是耗时最长，不满足实时性，朴素贝叶斯在准确率、召回率和预测时间上基本做到了 trade-off，所以最终选择了朴素贝叶斯。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master/img/blog/20200714201440.png" alt="image" loading="lazy"></li>
<li><strong>数据指标的选择</strong>：不同于在文件级别或者片上系统的基于机器学习的优化，在块级系统中部署机器学习算法来分类不同的数据面临很大的障碍，因为块级能提供的信息是有限的，常用的信息包括时间特征和空间特征（如最近访问时间和最近访问的地址）。本文中我们扩展了数据指标到 IO 请求，收集了如 average write size, write request ratio 等，这是由于具有类似请求级别的数据往往表现出类似的访问特征
<ul>
<li>Temporal features：这些特性包括数据块的访问近因和时间间隔。
<ul>
<li>Last Access Timestamp 在本文中被定义成了当前时间和最近访问该块的时间间隔。</li>
<li>Average Re-access Time Difference：对一个数据块进行两个相邻访问的时间间隔</li>
</ul>
</li>
<li>Spatial features：这些特性包含地址信息，例如卷 ID、偏移量等
<ul>
<li>因为地址会随着时间不断变化，对算法的效果会有显著影响，在作者实现的算法中其实没有使用空间特征。</li>
</ul>
</li>
<li>Request-level features
<ul>
<li>Average Request Size 平均请求大小，单位 KB，上界为 100KB</li>
<li>Big Request Ratio 大请求比例，请求大小大于 64KB 即为大请求</li>
<li>Small Request Ratio 小请求比例，请求小于 8KB 即为小请求</li>
<li>Write Request Ratio 写请求的比例</li>
</ul>
</li>
</ul>
</li>
<li><strong>统计粒度</strong>： 块设备的最小粒度为一个块，假设一个块为 8KB，如果我们为每一个块都统计对应的数据特征，因为块设备容量很大，那么统计这些特征就会造成巨大的开销，同时太大可能影响算法的准确率，所以实际应用过程中，作者采用了 1MB 为统计粒度（称之为 tablet），1MB 连续的数据块中所包含的最小物理块对应的数据特征相同，从而减少统计的开销。</li>
<li>整个模型的工作流程如下：
<ul>
<li>写请求的所有数据被首先被写入到内存中并顺序地记录在日志文件中，然后当脏数据的比例达到阈值则刷回到后端存储服务器，当在内存写缓冲中要执行对应的刷回操作时，进行下一步</li>
<li>分类器获取最近刷回的数据块的 tablet 特征并预测其写请求类型，如果是 write-only 跳转到下一步，如果不是（即为 normal data）跳转到第四步</li>
<li>如果该数据位于 SSD Cache 中且为脏数据，那么首先将该脏数据刷回到 HDD。对应的数据块将直接在 HDD 上进行写</li>
<li>数据块首先写入到 SSD 缓存然后使用 write-back 的策略异步刷回 HDD</li>
</ul>
</li>
<li>分类器每天训练一次用于第二天的预测，系统运行过程中收集样本数据，当 SSD cache 发生 eviction 的时候将增加一个样本，如果 evicted-data 有一次或者多次读命中，该样本将被设置为 0，也就是 normal data，否则设置为 1，write-only data。在作者的想法中，真实的实现里，write-only 的数据块是指一个在从缓存中 evict 之前不会被读的块</li>
<li><strong>效果</strong>：实验结果表明，与业界广泛部署的回写策略相比，ML-WP 减少了对 SSD 缓存的写流量 41.52%，同时提高了 2.61% 的命中率，降低了 37.52% 的平均读延迟</li>
</ul>
<h5 id="related-work">Related Work</h5>
<ul>
<li>
<p><strong>写策略优化</strong></p>
<ul>
<li>ATC15 - Request-oriented durable write caching for application performance
<ul>
<li>分析IO工作负载（判断是否会发生争用），然后部署最合适的写策略，使用了固定的写策略</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Cache Optimization Based on Machine Learning</strong>:</p>
<ul>
<li>MICRO16 Perceptron learning for reuse prediction
<ul>
<li>感知器学习用于判断 last-level cache 的重用预测 （神经网络做预测）</li>
<li>通过使用多个能够展示程序和存储器行为的特征（七个参数化特征）来从多个视角对LLC中的缓存块的未来重用进行预测。利用预测的结果，设计三种cache的管理策略：block placement，replacement，bypass</li>
<li>PC, address, reference count, etc</li>
</ul>
</li>
<li>ICPP18 Efficient ssd caching by avoiding unnecessary writes using machine learning
<ul>
<li>使用机器学习（决策树）来进行一次访问排除，能够准确地识别和过滤一次访问的照片，并阻止它们写入cache，提高社交网络照片缓存服务的效率。更多的使用照片的相关数据以及用户的登录请求数据来作为数据集</li>
<li>使用 reaccess distance 来定义是否为只访问一次的图像，被定义为从该图像进入cache 开始到下一次被访问之间的总的图像访问量</li>
<li>命中率提高了17%，缓存写操作降低了79%，平均访问延迟降低了 7.5%</li>
</ul>
</li>
<li>NSDI19 Flashield: a hybrid key-value cache that controls flash write amplification
<ul>
<li>由于键值更新会对 SSD 造成严重的有害的小随机I/O写操作，因此使用机器学习（支持向量机 SVM）的方法来选择期望读取频率更高的对象</li>
<li>Flashield这里引入了Flashiness的概念，作为一个对象被Cache价值的一个评价指标，一个高Cache价值的对象要满足两点条件
<ul>
<li>一个对象在访问之后会在不远的将来访问n次(及以上)，这里的n作为参数定义，</li>
<li>在将来的一段时间内不会被修改<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021172230.png" alt="20201021172230" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="new-cache-policy">New Cache Policy</h3>
<h4 id="tpds20-efficient-ssd-cache-for-cloud-block-storage-via-leveraging-block-reuse-distances">TPDS20 - Efficient SSD Cache for Cloud Block Storage via Leveraging Block Reuse Distances</h4>
<ul>
<li>
<p><strong>发现</strong>：我们在一个典型云块存储的服务器端发现了这一点，有很大比例的块具有很大的重用距离，这意味着很多块只有在遥远的将来才会被重新引用。在这样的场景中，如果在每次 miss 时进行简单的替换，新访问的块会污染缓存，而不会给命中率带来任何好处，从而降低缓存效率。现有的缓存算法在存在较大的重用距离时，缓存效率往往不理想</p>
<ul>
<li>如下图所示，A 点意味着 CBS 访问中，有大约 50% 的块的重用距离大于 32GB，而对于内存和主机上的 IO 则要小得多</li>
<li>因此，如果我们使用像LRU这样的缓存算法，当缓存大小等于或小于 32gb 时，50% 的重用块不会被命中<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021174825.png" alt="20201021174825" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>现象分析</strong>：</p>
<ul>
<li>多数具有小重用距离的数据块已经被客户端缓存设备缓存和过滤。</li>
<li>单个云磁盘的容量可以达到几十TB，这比传统磁盘高出几个数量级。因此，数据规模较大的云应用程序可能导致更大的重用距离</li>
<li>在多租户共享云环境中，来自不同租户的访问会混合在一起，导致重用距离增加</li>
</ul>
</li>
<li>
<p><strong>贡献</strong>：我们提出了一种新的缓存算法，专为 CBS 和其他类似场景设计的 LEA，LEA在缓存未命中时默认不进行替换，除非满足某些(惰性)条件。条件包含两个方面：新访问块的频率 和 缓存中的候选逐出块的值（该值是指该块对缓存的重要性或有用性）。这样，块的缓存持续时间可以大大延长。更重要的是，可以大大减少对 SSD 的写操作，延长 SSD 的生命周期</p>
</li>
<li>
<p><strong>Reuse Distance</strong>：数据块的重用距离定义为对同一块的两个连续引用之间的唯一数据量。例如 1-2-4-5-4-3-3-2 的块访问序列中，Block 2 的 reuse distance 就是三个块的大小，在有的研究中，Block 2 的 reuse distance 为 5 个块的大小。本文采用了第二种定义。</p>
</li>
<li>
<p>传统的 LRU 当缓存空间满时，对每次miss进行替换，主要遵循时间局部性原则，这些缓存替换算法的主要目标是保留重用距离较小的块，驱逐重用距离较大的块。当大部分数据重用距离小于缓存大小时，这些算法可以有效地工作。</p>
</li>
</ul>
<h5 id="lea-algorithm">LEA Algorithm</h5>
<ul>
<li>维护两个队列，Lazy Eviction List (LEL) 和 Block Identity List (BIL)。每个队列有两个端点 Insertion Point (IP) 和 Eviction Point (EP)
<ul>
<li>标识只包括块的卷号和地址信息(偏移量)。一个条目包含额外的块信息，如 last_access, reuse_distance, age, flag，等等。条目中的信息用于计算条目块的值。</li>
<li>age 表示块的当前时间和最后一次访问时间(last_access)之间的时间差，如果 age 小于 reuse_distance，它表明该块对缓存具有较高的价值。这里的时间是逻辑时间，当一个新的块请求到来时，逻辑时间增加1。这里，重用距离是最后两次访问之间的时间(而不是访问之间的平均时间)。</li>
<li>flag 也用来判断块的值，当块在 LEL 列表中命中时增加 1，当块被视为块驱逐候选，但由于懒惰替换还没有被驱逐时减少一半，如果 flag 大于 0，它表明该块对于缓存是有价值的。</li>
</ul>
</li>
<li>当发生缓存 miss 时，LEA 不进行替换，只在满足延迟条件时将丢失的块标识插入 BIL 列表，否则，它将执行替换并将丢失的块条目插入到 LEL 列表中。如果在 BIL 列表中命中了一个块，那么它进入LEL列表的概率更高。通过这样做，间接地考虑了访问频率<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021191310.png" alt="20201021191310" loading="lazy"></li>
</ul>
<h4 id="atc20-osca-an-online-model-based-cache-allocation-scheme-in-cloud-block-storage-systems">ATC20 - OSCA: An Online-Model Based Cache Allocation Scheme in Cloud Block Storage Systems</h4>
<ul>
<li>OSCA 可以在非常低的复杂度下找到接近最佳的配置方案，从而提高缓存服务器的总体效率
<ul>
<li>部署了一个新的缓存模型来获得云基础设施块存储系统中每个存储节点的 miss ratio curve (MRC)。使用一种低开销的方法来获得一个时间窗口内从重新访问流量与总流量之比的数据重用距离。然后将得到的重用距离分布转化为 miss ratio curve (MRC)。</li>
<li>通过了解存储节点的缓存需求，将总命中流量指标定义为优化目标</li>
<li>使用动态规划方法搜索接近最优的配置，并基于此解进行缓存重新分配</li>
</ul>
</li>
<li>在实际工作负载下的实验结果表明，模型达到了一个平均值绝对误差(MAE)，可与现有的最先进的技术相媲美，但同时可以做到没有跟踪收集和处理的开销。由于命中率的提高，相对于在相同缓存内存的情况下对所有实例的等分配策略，OSCA 减少了到后端存储服务器的 IO 流量 13.2%<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201021194032.png" alt="20201021194032" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ceph FS 介绍和使用]]></title>
        <id>https://blog.shunzi.tech/post/CephFS/</id>
        <link href="https://blog.shunzi.tech/post/CephFS/">
        </link>
        <updated>2020-10-14T12:32:20.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>一个项目测试使用到了 CephFS，故简要整理 CephFS 资料和相关文档</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>一个项目测试使用到了 CephFS，故简要整理 CephFS 资料和相关文档</li>
</ul>
</blockquote>
<!--more-->
<h2 id="cephfs">CephFS</h2>
<ul>
<li><a href="https://github.com/zjs1224522500/BlogIssue/issues/18">Ceph 常用命令</a></li>
<li><a href="https://chaosd.github.io/2020/08/17/Deploy%20a%20Ceph%20Cluster%20Manually/">Deploy a Ceph Cluster Manually</a></li>
</ul>
<h3 id="overview">Overview</h3>
<ul>
<li>CephFS 应用相比于 RBD/RGW 不够广泛主要是因为文件系统采用树状结构管理数据（文件和目录）、基于查表寻址的设计理念与 Ceph 扁平化的数据管理方式、基于计算进行寻址的设计理念有些违背；其次文件系统的支持常常需要集中的元数据管理服务器来作为树状结构的统一入口，这又与 Ceph 去中心化、追求近乎无限的横向扩展能力的设计思想冲突。</li>
<li>由于分布式文件系统的需求仍旧很大，应用场景尤为广泛，在 Ceph 不断的版本迭代中，CephFS 也取得了越来越好的支持。</li>
</ul>
<h3 id="背景">背景</h3>
<ul>
<li>要想实现分布式文件系统，那么就必须实现分布式文件系统的特点，即具有良好的横向扩展性，性能能够随着存储规模呈线性增长，为了实现这样的目标则需要对文件系统命名空间分而治之，即实现相应的负荷分担和负载均衡，采用相应的数据路由算法。</li>
</ul>
<h4 id="文件系统数据负载均衡分区">文件系统数据负载均衡分区</h4>
<h5 id="静态子树分区">静态子树分区</h5>
<ul>
<li>手工分区，数据直接分配到某个固定的服务节点，负载不均衡时再手动调整。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201013150928.png" alt="20201013150928" loading="lazy"></li>
</ul>
<h5 id="hash-计算分区">HASH 计算分区</h5>
<ul>
<li>HASH 计算数据的存储位置，保证了数据分布的均衡，但如果环境变化（集群规模变化）此时需要固定原有的数据分区而减少数据的迁移，或者根据元数据的访问频率，要想保证 MDS 负载均衡，需要重新决定元数据的分布，此时则不适合使用 HASH</li>
</ul>
<h5 id="动态子树分区">动态子树分区</h5>
<ul>
<li>通过实时监控集群节点的负载，动态调整子树分布于不同的节点。这种方式适合各种异常场景，能根据负载的情况，动态的调整数据分布，不过如果大量数据的迁移肯定会导致业务抖动，影响性能。在元数据存储、流量控制和灵活的资源利用策略方面，动态分区比其他技术有许多优势。</li>
<li>https://ceph.com/wp-content/uploads/2016/08/weil-mds-sc04.pdf<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201013151347.png" alt="20201013151347" loading="lazy"></li>
<li>动态子树分区方法的核心是将文件系统作为层次结构处理。通过将层次结构的子树的权限委托给不同的元数据服务器，对文件系统进行分区。委托可以嵌套:例如，/usr可以分配给一个MDS，而/usr/local可以分配给另一个MDS。但是，在没有显式分配子树的情况下，嵌套在某个点下的整个目录树被假定驻留在同一台服务器上。</li>
<li>这个结构中隐含着层次结构遍历的过程，以便找到并打开嵌套的索引节点，以便随后下降到文件层次结构中。这样的路径遍历对于验证POSIX语义所要求的嵌套项的用户访问权限也是必要的，对于在目录层次结构深处定位一个文件来说，这个过程可能代价很高。</li>
<li>为了允许有效地处理客户机请求(以及正确响应它们所需的路径遍历)，每个MDS都缓存缓存中所有项的前缀索引节点，以便在任何时候缓存的层次结构子集仍然是树结构。也就是说，只有叶子项可以从缓存中过期;在目录中包含的项首先过期之前，不能删除目录。这允许对所有已知项进行权限验证，而不需要任何额外的I/O成本，并保持层次一致性。</li>
<li>为了适应文件系统发展和工作负载变化的要求，MDS集群必须调整目录分区，以保持工作负载的最佳分布。动态分布是必要的，因为层次结构部分的大小和流行度都以一种不均匀和不可预测的方式随时间变化。通过允许MDS节点传输目录层次结构的子树的权限，元数据分区会随着时间的推移进行修改。MDS节点定期交换心跳消息，其中包括对其当前负载级别的描述。此时，忙碌的节点可以识别层次结构中适当流行的部分，并发起一个双重提交事务，将权限传递给非繁忙节点。在此交换过程中，所有活动状态和缓存的元数据都被转移到新的权威节点，这既是为了保持一致性，也是为了避免磁盘I/O，否则，新权威节点将需要磁盘I/O来重新读取它，而磁盘I/O会慢上几个数量级。</li>
</ul>
<h4 id="cephfs-mds-特点">CephFS MDS 特点</h4>
<ul>
<li>采用多实例消除性能瓶颈并提升可靠性</li>
<li>采用大型日志文件和延迟删除日志机制提升元数据读写性能</li>
<li>讲 Inode 内嵌至 Dentry 中来提升文件索引率</li>
<li>采用目录分片重新定义命名空间层次结构，并且目录分片可以在 MDS 实例之间动态迁移，从而实现细粒度的流控和负载均衡机制</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200831094239.png" alt="20200831094239" loading="lazy"></figure>
<h3 id="架构">架构</h3>
<ul>
<li>
<p>虽然 Ceph 文件系统中的 inode 数据存储在 RADOS 中并由客户端直接访问，但是 inode 元数据和目录信息由Ceph metadata server (MDS)管理。MDS 充当所有与元数据相关的活动的中介，将结果信息存储在与文件数据不同的RADOS 池中。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200830135706.png" alt="20200830135706" loading="lazy"></p>
</li>
<li>
<p>CephFS中的所有文件数据都存储为RADOS对象。CephFS客户端可以直接访问RADOS对文件数据进行操作。MDS只处理元数据操作。</p>
</li>
<li>
<p>要读/写CephFS文件，客户端需要有相应inode的“文件读/写”功能。如果客户端没有需要的功能 caps，它发送一个“cap消息”给MDS，告诉MDS它想要什么。MDS将在可能的情况下向客户发布功能 caps。一旦客户端有了“文件读/写”功能，它就可以直接访问 RADOS 来读/写文件数据。文件数据以 <inode number>, <object index> 的形式存储为RADOS对象。如果文件只由一个客户端打开，MDS还会向唯一的客户端提供“文件缓存/缓冲区”功能。“文件缓存”功能意味着客户端缓存可以满足文件读取要求。“文件缓冲区”功能意味着可以在客户端缓存中缓冲文件写。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200831094239.png" alt="20200831094239" loading="lazy"></p>
</li>
</ul>
<h4 id="cepgfs-client-访问示例">CepgFS Client 访问示例</h4>
<ul>
<li>Client 发送 open file 请求给 MDS</li>
<li>MDS 返回 file node, file size, capability 和 stripe 信息</li>
<li>Client 直接 READ/WRITE 数据到 OSDs（如果无 caps 信息需要先向 MDS 请求 caps）</li>
<li>MDS 管理 Client 对该 file 的 capabilities</li>
<li>Client 发送 close file 请求给 MDS，释放 file 的 capabilities，更新 file 的详细信息</li>
</ul>
<h4 id="mds-文件锁">MDS 文件锁</h4>
<ul>
<li>当客户机希望在 inode 上操作时，它将以各种方式查询 MDS，然后授予客户机一组功能。它们授予客户端以各种方式操作 inode 的权限。与其他网络文件系统(例如 NFS 或 SMB)的主要区别之一是，所授予的功能非常细粒度，多个客户机可能在同一个 inode 上拥有不同的功能。</li>
<li>CephFS 客户机可以请求MDS代表它获取或更改 inode 元数据，但是MDS还可以为每个 inode 授予客户机功能 (caps)</li>
</ul>
<pre><code class="language-C">/* generic cap bits */
#define CEPH_CAP_GSHARED     1  /* client can reads (s) */
#define CEPH_CAP_GEXCL       2  /* client can read and update (x) */
#define CEPH_CAP_GCACHE      4  /* (file) client can cache reads (c) */
#define CEPH_CAP_GRD         8  /* (file) client can read (r) */
#define CEPH_CAP_GWR        16  /* (file) client can write (w) */
#define CEPH_CAP_GBUFFER    32  /* (file) client can buffer writes (b) */
#define CEPH_CAP_GWREXTEND  64  /* (file) client can extend EOF (a) */
#define CEPH_CAP_GLAZYIO   128  /* (file) client can perform lazy io (l) */
</code></pre>
<ul>
<li>然后通过特定数量的位进行移位。这些表示 inode 的数据或元数据的一部分，在这些数据或元数据上被授予能力:</li>
</ul>
<pre><code class="language-C">/* per-lock shift */
#define CEPH_CAP_SAUTH      2 /* A */
#define CEPH_CAP_SLINK      4 /* L */
#define CEPH_CAP_SXATTR     6 /* X */
#define CEPH_CAP_SFILE      8 /* F */
</code></pre>
<ul>
<li>一个 Cap 授予客户端 缓存和操作与 inode 关联的部分数据或元数据的能力。当另一个客户机需要访问相同的信息时，MDS 将撤销该 cap，而客户机最终将返回该功能，以及 inode 元数据的更新版本(如果它在保留功能时对其进行了更改)。</li>
<li>客户机可以请求 cap，并且通常会获得这些 cap，但是当 MDS 面临竞争访问或内存压力时，这些 cap 可能会被 revoke。当一个 cap 被 revoke 时，客户端负责尽快返回它。未能及时这样做的客户端可能最终被阻塞并无法与集群通信。</li>
<li>由于缓存是分布式的，所以 MDS 必须非常小心，以确保没有客户机拥有可能与其他客户机的 cap 或它自己执行的操作发生冲突的 cap。这使得 cephfs 客户机比 NFS 这样的文件系统依赖于更大的缓存一致性，在 NFS 中，客户机可以缓存数据和元数据，而这些数据和元数据在服务器上已经更改了。</li>
<li>基于 caps，构建了 ceph 的分布式文件锁，分布式文件锁保证了多个客户端并发且细粒度访问同一文件、目录、文件系统，同时保证一致性、可靠性。ceph 实现的分布式文件系统锁，客户端可见部分是 caps，服务端可见部分包括 caps和各种 lock，每个类型的 lock 又有多种状态，根据客户端的请求、持有、释放情况，lock 转换自身状态，并和客户端同步 caps 信息。最终实现分布式锁的访问。</li>
</ul>
<h3 id="参考链接">参考链接</h3>
<ul>
<li><a href="https://www.jianshu.com/p/e7d79a0d5314">[1] CephFS 介绍及使用经验分享</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[FAST20 Some Interesting Papers Overview]]></title>
        <id>https://blog.shunzi.tech/post/fast20-some/</id>
        <link href="https://blog.shunzi.tech/post/fast20-some/">
        </link>
        <updated>2020-10-12T01:53:00.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>FAST20 Overview</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>FAST20 Overview</li>
</ul>
<!--more-->
<blockquote>
<ul>
<li>最近进入了一个怠惰期，一是苦于没有明确的研究方向，二是沉湎于琐碎小事，所以科研上很多事情都被搁置</li>
<li>遇到了问题就总得寻找解决的办法，所以决定还是好好看几篇论文，看看能不能找到感兴趣的点，再深入挖掘</li>
<li>最近也没太多新的会议，故还是先好好看看 FAST20 上的文章，对一些可能感兴趣的文章做些简单记录</li>
</ul>
</blockquote>
<h1 id="fast20">FAST20</h1>
<ul>
<li>此次会议主要分为了 Cloud Storage、File Systems、HPC Storage、SSD and Reliability、Performance、Key Value Storage、Caching 和 Consistency and Reliability 几个 Topic。本篇文章不会对所有的 Topic 下的所有论文都进行总结记录，譬如 HPC Storage 由于缺乏一定的了解就没有在此处记录，针对每个 Topic 下的文章也只是选择了自己有一定的了解或者感兴趣的几篇来简单地记录。</li>
<li>有的文章会详细展开，有的因为受限于我自己的水平无法深入，故只能泛泛而谈，但会尽力尝试去理解所要解决的问题以及解决问题的方式。</li>
<li>本篇博文会持续记录更新，因为阅读量还挺大的，遇到比较好的感兴趣的论文会单独开一篇进行详细的理解记录。</li>
</ul>
<h2 id="cloud-storage">Cloud Storage</h2>
<ul>
<li>云存储主题下 FAST20 有三篇：
<ul>
<li>MAPX: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems
<ul>
<li>Li Wang, <strong>Didi Chuxing</strong>; Yiming Zhang, NiceX Lab, NUDT; Jiawei Xu and Guangtao Xue, SJTU</li>
</ul>
</li>
<li>Lock-Free Collaboration Support for Cloud Storage Services with Operation Inference and Transformation
<ul>
<li>Jian Chen, Minghao Zhao, and Zhenhua Li, <strong>Tsinghua University</strong>; Ennan Zhai, Alibaba Group Inc.; Feng Qian, University of Minnesota - Twin Cities; Hongyi Chen, Tsinghua University; Yunhao Liu, Michigan State University &amp; Tsinghua University; Tianyin Xu, University of Illinois Urbana-Champaign</li>
</ul>
</li>
<li>POLARDB Meets Computational Storage: Efficiently Support Analytical Workloads in Cloud-Native Relational Database
<ul>
<li>Wei Cao, <strong>Alibaba</strong>; Yang Liu, ScaleFlux; Zhushi Cheng, Alibaba; Ning Zheng, ScaleFlux; Wei Li and Wenjie Wu, Alibaba; Linqiang Ouyang, ScaleFlux; Peng Wang and Yijing Wang, Alibaba; Ray Kuan, ScaleFlux; Zhenjun Liu and Feng Zhu, Alibaba; Tong Zhang, ScaleFlux</li>
</ul>
</li>
</ul>
</li>
<li>云存储主题下的文章被国内的企业和高校包揽，其实不难发现云存储还是更偏向于工业界的实际应用的，而国内的企业在相应的领域都有着各自丰富的积累，特别是在数据库领域更是百花齐放（POLARDB Meets Computational Storage）；也有自己在运维相关存储系统时的经验总结以及对应的优化方案（MAPX）；还有一个比较有意思的点就是近年来比较多的团队协作式的云存储（Lock-Free Collaboration Support for Cloud Storage），又恰逢疫情更是推动了云存储的普及。</li>
</ul>
<h3 id="mapx-controlled-data-migration-in-the-expansion-of-decentralized-object-based-storage-systems">MAPX: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems</h3>
<ul>
<li>这篇因为之前有针对全文的翻译和理解，此处不做过多的介绍，更详细的可以参考
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/111505363">知乎：信息存储论文选读 - MAPX</a></li>
<li><a href="https://blog.shunzi.tech/post/controlled-data-migration-in-the-expansion-of-decentralized-object-based-storage-systems/">shunzi blog - MAPX</a></li>
</ul>
</li>
</ul>
<h4 id="简要介绍">简要介绍</h4>
<ul>
<li><strong>问题</strong>：该文解决的问题其实是 CURSH 算法最大的问题——集群扩容或者添加中间逻辑结构（如PGs）时会造成不受控制的数据迁移，虽然迁移可以在扩展之后立即重新平衡整个系统的负载，但是在扩展规模较大时会导致显著的性能下降。（如以机架的规模进行扩容）</li>
<li><strong>为什么会产生这样的问题？</strong> 设计 CRUSH 算法目的其实主要是为了去中心化，即不需要像 HDFS 等分布式存储系统依赖中心化的目录或元数据管理来寻址对应的数据，而是通过直接计算的方式来实现从而降低对中心化的元数据管理的耦合。但是 CRUSH 相比于中心化管理，中心化目录的存储系统可以保证原有的数据在扩容过程中不受影响，只把新的数据存储在新的存储节点上，CRUSH 由于缺少了关于集群扩容过程中带来的物理存储节点（OSD）的差异信息，在进行计算时都统一处理，由于相应的逻辑节点的权重发生了显著变化，CRUSH 计算结果发生了变化，就会导致大量的数据迁移。</li>
</ul>
<h4 id="解决方案">解决方案</h4>
<ul>
<li>受中心化数据布局策略的启发，致力于实现扩容过程中数据迁移的可控，所以基于 CRUSH 设计实现了 MAPX，核心思想就是<strong>引入时间维度的映射机制来区分 新老 对象/OSD</strong> ，同时保留 CRUSH 算法随机和均匀的优点。</li>
<li>为了尽可能少地修改原有的 CRUSH 算法，在原本的 CRUSH 根节点下插入一个虚拟层，如图所示，每一个虚拟节点代表一次扩容。虚拟层对应的通过使用 MAPX 实现可控数据迁移，通过在执行原本的 CRUSH 算法之前将新的对象映射到新的 layer，因为新的 layer 不会影响原有 layer 的权重，原有对象的放置还是和以前一样，不会发生改变。<br>
<img src="https://blog.shunzi.tech/post-images/1584325944117.png" alt="image" loading="lazy"></li>
<li>MAPX 能保证每一层的负载均衡，主要是因为通用 CRUSH 算法的随机性和均匀性，随着时间的迁移，新的 layer 中的数据增多到和前一个 layer 时则实现了 layers 层面的负载均衡。但是一个 layer 的负载可能会因为 对象的删除、OSD 的宕机发生一些不可预测的负载变化。如图所示，当 layer1 中的负载跟原始集群 layer0 的负载一样高时，则可能会执行一次扩容产生 layer2，假设 layer1 中执行了大量的对象删除操作，则会造成不同 layers 之间的负载不均衡。</li>
<li>为了解决这个问题，MAPX 设计了三种灵活的策略来动态管理 MAPX 中的负载：
<ul>
<li><strong>PG 重映射</strong>：可以控制 PGs 到 Layer 的映射，来保证 layers 负载均衡</li>
<li><strong>集群缩容</strong>：缩容时需要进行 PG 重映射调整负载，但也要保留部分元数据来保证映射关系不变</li>
<li><strong>layers 合并</strong>：使用时间戳来保证物理层的变化在逻辑层 layer 上保持相同以实现负载均衡</li>
</ul>
</li>
</ul>
<h3 id="lock-free-collaboration-support-for-cloud-storage-services-with-operation-inference-and-transformation">Lock-Free Collaboration Support for Cloud Storage Services with Operation Inference and Transformation</h3>
<ul>
<li>这篇文章呢其实主要是对现如今比较时髦的一个场景进行了综合性的分析，也提供了自己的方案。即针对共享文档的编辑时的版本管理和冲突解决的方式进行了深入探讨，结合了市面上现有的多种云存储的团队协作方案，分析之后他们提出了一种无锁的共享文档的编辑的方案，设计了一种比较智能的办法来减少冲突，且开源。</li>
<li>提出了问题同时也利用并设计了算法上的优化，难点主要在于对整个过程进行建模的过程。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201012163936.png" alt="20201012163936" loading="lazy"></li>
<li>这篇文章还有的一个比较有意思的点在于，本文涉及到的别的云存储共享编辑的产品其本身是不开源的，所以在分析这些产品可能的实现方式的时候其实是个很复杂的过程，作者们通过抓包、解密网络流量、分析数据驱动的方法、阅读部分代码和文档的方式才渐渐分析出大致的实现原理</li>
<li>对共享文档的编辑或者云存储协作感兴趣的同学可以深入阅读，由于不涉及到自己更了解的存储系统领域，此处就不班门弄斧了。</li>
</ul>
<h3 id="polardb-meets-computational-storage-efficiently-support-analytical-workloads-in-cloud-native-relational-database">POLARDB Meets Computational Storage: Efficiently Support Analytical Workloads in Cloud-Native Relational Database</h3>
<ul>
<li>开始之前先贴几篇讲解和总结（主要是不想重复造轮子hhhh）
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/111013416">知乎：阿里云数据库技术 - 深度思考 | 读POLARDB论文有感 : 异构计算和数据库软硬一体化设计</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/111615643">知乎：CobbLiu - FAST20论文赏析（一）</a></li>
<li><a href="https://blog.csdn.net/qq_37151108/article/details/105096703">CSDN：黄小米吖 - 论文阅读——POLARDB</a></li>
</ul>
</li>
</ul>
<h4 id="简要介绍-2">简要介绍</h4>
<ul>
<li>PolarDB 整体架构<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201012194710.png" alt="20201012194710" loading="lazy"></li>
<li>存算分离架构的背景下，云原生的关系型数据库需要把数据敏感型的任务（例如 table scan）从前端数据库给下发到后端存储节点，以便充分支持分析工作负载，减小存储节点和计算节点之间的链路传输,，但将该类任务转发到存储节点之后对于存储节点的效率则提出了挑战，新出现的 computational storage drives （也称存内计算或者计算型存储）使得上述想法成为可能。</li>
<li>要想使存储节点有足够的能力支持”计算下推”，通常有两个方案，一是scale-up存储节点的CPU能力；二是在存储节点上配备特殊硬件（比如GPU 或者FPGA）来用这些特殊硬件执行存储节点上的”计算下推“任务。第一个方案会较大增加存储节点的成本；第二个方案会引起存储节点内大量的数据迁移， 同时在拥有多块NVMe SSD的存储节点上很容易使特殊硬件成为热点，制约单个存储节点的对外能力。 论文提出了一个新的思路，直接将”计算下推“的工作offload到物理介质<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201012175245.png" alt="20201012175245" loading="lazy"></li>
<li>然而，异构计算实际可行的实现和实际部署仍然完全缺失，至少在开放文献中是这样。这主要是由于很难解决两个挑战:
<ul>
<li>(1) 如何实际支持跨整个软件层次的表扫描 pushdown
<ul>
<li>由用户空间POLARDB存储引擎发起，该存储引擎通过指定文件中的偏移量来访问数据，而表扫描在物理上由计算存储驱动器提供，它作为原始块设备运行，并使用LBA(逻辑块地址)管理数据。整个存储I/O堆栈位于POLARDB存储引擎和计算存储驱动器之间。因此，我们必须内聚地增强/修改整个软件/驱动程序堆栈，以便创建一个支持表扫描叠加的路径</li>
<li>组成：
<ul>
<li>前端分析处理引擎 PolarDB MPP：该分析处理引擎与 MySQL 协议兼容，可以解析、优化和重写使用 AST(抽象语法树)的 SQL 和许多嵌入式优化规则，它将每个 SQL 查询转换为一个 DAG(有向无环图)执行计划，由操作符和数据流拓扑组成。该引擎原生就支持 pushdown</li>
<li>存储引擎 PolarDB Storage Engine：遵循了 LSM-tree 实现，数据被组织成了多个文件，每个文件包含很多块。
<ul>
<li>原有的实现中，存储引擎可以使用存储节点上的 CPU 来处理 table scan 请求，因此 table scan pushdown 将与底层的 IO 堆栈无关。</li>
<li>为了利用可计算型存储的特点，需要修改该引擎以便将 table scan 请求 pushdown 到 PolarFS 上。存储引擎根据文件中的偏移量访问数据块。每一个 table scan 请求包括：
<ul>
<li>要被扫描的数据的定位信息（文件内的偏移量）；</li>
<li>应用表扫描的表的 schema；</li>
<li>table scan condition。</li>
</ul>
</li>
<li>POLARDB 存储引擎分配一个内存缓冲区来存储从计算存储驱动器返回的数据，每个 table scan 请求都包含这个内存缓冲区的位置</li>
</ul>
</li>
<li>POLARDB部署在分布式文件系统PolarFS上，该文件系统管理跨所有存储节点的数据存储。computational storage drives 只能以 LBA 的形式定位数据，PolarFS 在收到来自 POLARDB存储引擎的每个表扫描请求后需要进行数据转换</li>
<li>计算存储驱动器完全由内核空间中的主机端驱动程序管理，该驱动程序将每个计算存储驱动器公开为块设备。当收到每个表扫描请求时，驱动程序执行以下操作
<ul>
<li>分析 scan conditions，可能对 conditions 重新进行组织以获得更好的性能</li>
<li>驱动程序进一步转换将被扫描数据的位置信息从LBA域到物理块地址(PBA)域，其中每个PBA与NAND闪存中的一个固定位置相关联。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201012195303.png" alt="20201012195303" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>(2) 如何实现低成本的计算存储驱动器具有足够的表扫描处理能力
<ul>
<li>虽然基于FPGA的设计方法可以显著降低开发成本，但FPGA往往比较昂贵。此外,由于<br>
FPGA通常仅工作在200 ~ 300MHz(与之相比，CPU时钟频率为2 ~ 4GHz)，为了实现足够高的性能，我们必须使用大量的电路级实现并行性(因此需要更多的硅资源)。因此，我们必须在我们的实现中开发出能够使用低成本FPGA芯片的解决方案</li>
<li>为了解决计算存储驱动器实现成本的挑战，关键是最大化FPGA硬件资源的利用效率。为了实现这一目标，我们跨软件和硬件层进一步开发了以下技术。
<ul>
<li>Hardware-Friendly Data Block Format<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201012201848.png" alt="20201012201848" loading="lazy"></li>
</ul>
</li>
<li>FPGA 并行化<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201012202048.png" alt="20201012202048" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="个人见解">个人见解</h4>
<ul>
<li>因为自己不是做数据库系统的，所以对于背景可能了解的不够，但这不妨碍对 PolarDB 整体结构的理解，个人感觉 PolarDB 对外提供的虽然是简单的关系型数据库服务，但是内部实现了大量的存储系统的工作。虽然本篇是基于 data-intensive 的负载 offload 到计算型存储的角度来组织的，但还是大体能看出 PolarDB 内部复杂的工作细节，以及各个组件都进行了各种软件栈上的优化。PolarDB 本身是一个很大的分布式存储系统，其中的很多小的创新点也基本都是能够组成一个 Paper Idea 的水准，且有工业界的实际实现，经得住实际负载的考验。</li>
<li>抛开本篇论文的主要内容不谈，PolarDB 本身就是一个十分出众的数据库，此处给一些 PolarDB 的整体介绍链接：
<ul>
<li><a href="https://help.aliyun.com/product/58609.html?spm=5176.155538.1357067.3.28ea1a9cb25UNl">阿里云 - PolarDB 帮助文档</a></li>
<li><a href="https://help.aliyun.com/product/58609.html?spm=5176.155538.1357067.3.28ea1a9cb25UNl">知乎 - 如何评价阿里云新一代关系型数据库 PolarDB？</a></li>
<li><a href="https://developer.aliyun.com/article/721566?utm_content=g_1000082241">阿里云 - 读懂POLARDB不能错过的18篇深度文章！</a></li>
<li><a href="https://github.com/AlibabaCloudDocs/polardb">Github PolarDB 官方文档库</a></li>
</ul>
</li>
</ul>
<h2 id="file-systems">File Systems</h2>
<ul>
<li>Read as Needed: Building WiSER, a Flash-Optimized Search Engine
<ul>
<li>Jun He and Kan Wu, University of Wisconsin—Madison; Sudarsun Kannan, Rutgers University; Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau, University of Wisconsin—Madison</li>
</ul>
</li>
<li>How to Copy Files
<ul>
<li>Yang Zhan, The University of North Carolina at Chapel Hill and Huawei; Alexander Conway, Rutgers University; Yizheng Jiao and Nirjhar Mukherjee, The University of North Carolina at Chapel Hill; Ian Groombridge, Pace University; Michael A. Bender, Stony Brook University; Martin Farach-Colton, Rutgers University; William Jannen, Williams College; Rob Johnson, VMWare Research; Donald E. Porter, The University of North Carolina at Chapel Hill; Jun Yuan, Pace University</li>
</ul>
</li>
</ul>
<h3 id="read-as-needed-building-wiser-a-flash-optimized-search-engine">Read as Needed: Building WiSER, a Flash-Optimized Search Engine</h3>
<ul>
<li>这篇文章是针对现有的搜索引擎进行设计和优化的（以前的研究中很少有针对搜索引擎这类上层应用进行优化的，但随着数据规模的增加，搜索引擎的应用也越来越广泛，尤其是在一些文本检索的领域），搜索引擎本身是一种计算密集型的应用，会有一些读取存储设备的操作，作为一种 &quot;read as needed&quot; 类型的负载，对于存储系统的要求和其他类型的应用负载稍有不同。本文则是实现了一个以相对较少的主存（main memory）提供高吞吐量和低延迟的搜索引擎。</li>
</ul>
<h4 id="简要介绍-3">简要介绍</h4>
<h5 id="背景">背景</h5>
<ul>
<li>
<p>由于本身研究的点不同于以往，所以就按照惯例先介绍了一下这个问题有多关键。此处不表。</p>
</li>
<li>
<p>对于存储系统而言，不管你上层的应用是什么，在存储设备上本质都是数据结构的存取，而搜索引擎和数据库以及图等负载稍有不同，主要使用了倒排索引等结构，又随着 SSD 的普及，所以基于 SSD 的优化方案就比较有搞头。</p>
</li>
<li>
<p>需要注意的是搜索引擎对于存储系统的要求：</p>
<ul>
<li>low data latency：因为搜索引擎的交互性很强，常常需要较低的延迟来保证</li>
<li>high data throughput：因为要处理和检索的数据很多，对于吞吐量的要求也较高</li>
<li>high scalability：因为搜索引擎主要面型文本存储领域，数据规模会随着时间不断地变大，相应地就需要较好的扩展性来提供相应的支持。</li>
</ul>
</li>
<li>
<p>以前的搜索引擎也会面对上述问题，但之前的解决方案都是利用内存作为一种存储介质，由于数据规模较大，为了保证低时延和吞吐量等特性，就常常需要把大量的数据放在内存中（虽然可能会丢，但重建肯定也是一个很大的开销）</p>
</li>
<li>
<p>作者觉得既然现在存储设备已经很快了现在，考虑到对大型数据集使用RAM的成本过高，那么是否可以重新构建一个搜索引擎，以更好地利用 SSD 来实现必要的性能目标，而使用比较少的内存。更明智的做法是重新组织传统的搜索数据结构，以创建和改进读流，从而利用现代 SSD 提供的带宽</p>
</li>
<li>
<p>针对 read as needed 的负载，作者提出了设计理念</p>
<ul>
<li>use small memory</li>
<li>read data from SSDs as needed</li>
<li>do not attempt to cache data in memory</li>
<li>attempt to read data from SSDs efficiently</li>
</ul>
</li>
<li>
<p>分析了现有的搜索引擎，其中 state-of-the-art 的 ElasticSearch。ES 不能实现高性能原因主要是因为读放大，因为 ES 将不同阶段的数据分组到多个位置，并将数据进行排列，使早期的数据项更小。其目的是缓存早期阶段的数据，因为早期阶段的数据要比后期阶段的数据访问得更频繁。然而，按阶段分组数据也可能导致比较大的读放大。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019204848.png" alt="20201019204848" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019205209.png" alt="20201019205209" loading="lazy"></p>
</li>
<li>
<p>因为 SSD 本身的一些限制（有限的带宽、高延迟、大IO友好），以及在 ES 中发现的问题，为了实现上述的搜索引擎对于存储系统的要求，就需要</p>
<ul>
<li>reduce read amplification</li>
<li>hide i/o latency</li>
<li>use large request to improve device efficiency</li>
</ul>
</li>
</ul>
<h5 id="design">Design</h5>
<ul>
<li>设计了四个关键技术
<ul>
<li>Grouping data by term
<ul>
<li>即不再采用分阶段存取在不同的文件中的方式进行存储，而是使用连续的压缩的数据块来进行放置，从而将小的读请求转换成了大的读请求<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019204044.png" alt="20201019204044" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019205307.png" alt="20201019205307" loading="lazy"></li>
</ul>
</li>
<li>Two-way Cost-aware Bloom Filters
<ul>
<li>针对于 phrase query 的场景，作者使用了两路布隆过滤器来帮助快速检索短语。</li>
<li>设置前后两路匹配的原因，是因为 before 和 after 的查询开销可能不一样，会尝试选择开销更小的那一路的布隆过滤器来进行匹配。但可能存在两路开销都很大的情况，这时候则直接使用默认的原有的短语匹配方式。这也就是所谓的 Cost-aware<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019210315.png" alt="20201019210315" loading="lazy"></li>
</ul>
</li>
<li>Adaptive Prefetching
<ul>
<li>为了获得最佳性能，预取应该适应查询和持久数据的结构。在倒排索引的所有数据中，最常被访问的数据包括元数据、跳跃表、文档id和词频，这些数据经常被一起顺序访问;<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019210851.png" alt="20201019210851" loading="lazy"></li>
</ul>
</li>
<li>Trade Disk Space for I/O
<ul>
<li>压缩的粒度和 ES 不同，从而减小读放大，但其实做了 trade-off，压缩后的大小比 ES 压缩后的要大<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201019211333.png" alt="20201019211333" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="个人见解-2">个人见解</h4>
<ul>
<li>这篇其实也不是个人所熟悉的领域，只是因为很早以前自己接触过 ElasticSearch 的应用（ELK 体系那一套），所以对 ES 还是比较感兴趣，这篇文章其实都是在和 ES 对比，可以理解为是对现有的 ES 上的优化，虽然作者指出可以延伸到按需读取场景下的其他负载，但本质还是在搜索引擎的这个领域下的优化。奈何才疏学浅，也就只能总结到这了。</li>
</ul>
<h3 id="how-to-copy-files">How to Copy Files</h3>
<ul>
<li>先放几个链接：
<ul>
<li><a href="https://www.xsky.com/tec/6390/">XSKY解读FAST'20 论文 《关于如何高效率的对文件目录树进行快速克隆操作》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/151392761">知乎 - BetrFS: 一种写优化的文件系统</a></li>
<li><a href="https://github.com/oscarlab/betrfs">Github - BetrFS</a></li>
<li><a href="http://www.betrfs.org/">betrfs</a></li>
</ul>
</li>
<li>文章从标题开始其实就很吸引眼球，毕竟是一个看着很简单的问题，但是作为论文标题就不禁想让我这种门外汉也要尝试着去看看作者在里面写了些啥我不知道的东西哈哈。走马观花看了看发现嗯自己确实不懂。。</li>
<li>本文其实是做的文件系统领域中关于复制或者克隆操作的优化，现有的很多克隆其实是基于 Copy-On-Write 实现的，但原生的 Copy-On-Write 启发算法有一定的问题，所以作者在这上边也下了很多功夫，提出了 copy-on-abundant-write，基于 BetrFS 实现，除此以外还做了一些别的针对性的优化，复制操作性能提升也比较明显。</li>
</ul>
<h4 id="简要介绍-4">简要介绍</h4>
<h5 id="背景-2">背景</h5>
<ul>
<li>当然首先还是按惯例介绍这个问题多重要，所以就先介绍拷贝操作应用广泛，其实主要体现在备份、快照这些机制中，然后云计算领域中的容器、虚拟机等技术使用拷贝比较频繁。但拷贝本身其实可以分为逻辑拷贝和物理拷贝，从字面上也很好理解，物理拷贝肯定就是物理存储空间上进行完整的拷贝了，时间和空间的消耗可能会因为大文件而特别大，所以很多时候都用逻辑拷贝，也是 volume snapshots 中用的比较多的 Copy-On-Write 写时复制。CoW 可以对块设备进行，也可以在文件系统中对文件或者目录进行，取决于具体的产品实现。譬如 Linux 本身支持的 cp -reflink</li>
<li>引出主题 CoW 之后相应地指出现有的 CoW 的问题：标准的 CoW 本身其实是在写放大和局部性之间做了 trade-off。即 CoW 的粒度大小的设定将影响这两方面的表现。
<ul>
<li>如果粒度为文件的大小，那么对文件的小写操作带来的写放大就很严重，相应的写延迟也就增加了，空间也被浪费了（因为共享的数据被写操作给破坏了）</li>
<li>如果粒度很小，更新操作很快，但是容易产生碎片，顺序读副本的开销就变大了，因为局部性被破坏了。</li>
</ul>
</li>
<li>想要实现的效果，也就是文中作者描述为 Nimble clones 的复制操作应该是这样的：（那肯定现有的文件系统里的 CoW 实现就不是 nimble 的，文中有测试）
<ul>
<li>be fast to create</li>
<li>have excellent read locality</li>
<li>have fast writes</li>
<li>conserve space</li>
</ul>
</li>
<li>总结下来问题其实出在对克隆的写操作粒度与共享数据副本的粒度这两点，其实就是对于文件小写，不能直接就将源文件拷贝重写了，要设定一个相对大的 copy size 来保证局部性，即得 buffer 小写。所以就得把对克隆的写操作和副本的相关操作解耦。所以作者选了 BetrFS 来实现自己的方案。毕竟 BetrFS 以写见长，也有一些自己的 buffer 机制，当然主要还是测试出来的，文中的一个关于性能降级的测试。</li>
</ul>
<h5 id="design-2">Design</h5>
<ul>
<li>需要首先理解一下 btrfs 的原理和其中的数据结构 Bε-tree。
<ul>
<li>btrfs 基于 KV 来管理文件系统中的数据，元数据 KV 存储的是 文件全路径 Full Path 到文件系统元数据的映射，数据 KV 存储的是 {fullpath + block number} 到 block的映射。</li>
<li>Bε-tree 可以简单理解为一个带 buffer 缓冲的 B-tree，主要吸收随机小写。</li>
</ul>
</li>
<li>在 Bε-tree 基础上实现逻辑拷贝，即 Bε-DAG。在树中增加新的边 edge，以使得在访问克隆后的文件夹时，能通过某种方法访问克隆前的数据（克隆后的数据在不修改内容前，都是完全共享的），通过使用 DAG （有向无环图）的思想来支持共享访问，对应地需要实现一层如图所示的转换 red-&gt;green<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201029153451.png" alt="20201029153451" loading="lazy"></li>
<li>对于此时的写操作，则需要在 flush 的时候执行 copy-on-write，分为五个步骤执行： Copy-on-Abundant-Write
<ul>
<li>第一步，copy 该节点</li>
<li>第二步，转换文件对应的路径前缀（图中的 green-&gt;red）</li>
<li>第三步，删除无法到达的数据，图示中的蓝色 L，因为拷贝之后没有 blue 上层目录，即在该路径上该文件再也无法访问到，故可以删除</li>
<li>第四步，移动地址转换功能，即把原本的上一层的 red-&gt;green 移动到下一层（因为复制后的节点有一些数据为目录，如图中所示的 A 和 R，故需要指向底层文件，如果 flush 操作针对更底层的文件，那么再相应递归地执行 copy-on-write）</li>
<li>第五步，将数据刷入到复制后的节点中作为新的 buffer（即吸收小写）</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201029155112.png" alt="20201029155112" loading="lazy"></figure>
<ul>
<li>因为 Bε-DAG 中的节点本身比较大，即可以吸收小写，且局部性比较好，所以在读写上表现都很不错，又因为采用了新的上文描述的 Copy-on-Abundant-Write 在空间上的利用率也很高，但还有一个问题没有解决，即拷贝延迟</li>
<li>作者实现了一种称之为 GOTO Message 的机制，用于减小 Copy 操作的延迟。存储的数据大致如 (a,b) - height - dst_node，其中 （a,b）为拷贝操作覆盖的 key 空间/范围，height 表示要拷贝的目标节点的高度，dst_node 表示要拷贝的目标节点。</li>
<li>如果要进行拷贝操作，如图所示，拷贝 /green 到 /violet，那么相应地会先触发 green 的 flush，确保数据一致，然后插入一条 GOTO Message。</li>
<li>查询操作首先查询到了 GOTO Message，那么首先判断 key 是否在相应的区间中，如果在那么就要去 dst_node 处继续查找，相应地会进行前缀转换，直到找到该数据。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201029160655.png" alt="20201029160655" loading="lazy"></li>
<li>相应的 GOTO message 可以和其他数据一样，通过 flush 操作进入下一层，如下图所示，直到 GOTO Message 到达了目标节点高度 + 1 的层级的时候，GOTO Message 就此时可以成为一个真正的指向下一层的目录，也就是文中的 pivot，即 GOTO 将变成紫色的目录节点 /violet</li>
<li>这样子下来拷贝的时间复杂度完全取决于树的高度<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201029161740.png" alt="20201029161740" loading="lazy"></li>
</ul>
<h4 id="个人见解-3">个人见解</h4>
<ul>
<li>这篇呢个人也只是泛泛而谈，主要参考了 PPT 的动画解释，原文其实还涉及到了更为具体的实际的解释，由于重心不在这边所以没有细看，感兴趣的小伙伴可以再深入研究，但是在研究之前最好先大致看一下之前的 btrfs 的论文，至少了解到核心数据结构的设计和读写的流程，才有助于对本篇文章的理解。</li>
<li>想提一下的是，我发现 btrfs 相关的论文已经上了很多次 FAST 以及 TOS 了，而且本身这个文件系统提出来也没几年，FAST15 上提出来的，后续的一些对于该文件系统的迭代都直接被拿来作为了新的 idea，譬如本篇实现的 clone，其实做文件系统同学可以关注一下这个文件系统，我自己简单看了这个文件系统的数据结构之后理解为他其实是在利用 B-Tree 和 LSM-tree 各自的优势，当然主要还是解决 B-Tree 的写的问题，后续如果有相关的工作的话可能会关注一下。</li>
</ul>
<h2 id="ssd-and-reliability">SSD and Reliability</h2>
<ul>
<li>A Study of SSD Reliability in Large Scale Enterprise Storage Deployments
<ul>
<li>Stathis Maneas and Kaveh Mahdaviani, University of Toronto; Tim Emami, NetApp; Bianca Schroeder, University of Toronto</li>
<li><strong>Awarded Best Paper!</strong></li>
</ul>
</li>
<li>Making Disk Failure Predictions SMARTer!
<ul>
<li>Sidi Lu and Bing Luo, Wayne State University; Tirthak Patel, Northeastern University; Yongtao Yao, Wayne State University; Devesh Tiwari, Northeastern University; Weisong Shi, Wayne State University</li>
</ul>
</li>
</ul>
<h3 id="a-study-of-ssd-reliability-in-large-scale-enterprise-storage-deployments">A Study of SSD Reliability in Large Scale Enterprise Storage Deployments</h3>
<ul>
<li>虽然是 SSD 硬件相关的，但毕竟是 BestPaper，而且这篇其实更像是针对一些数据集的分析，不是具体的基于硬件或者协议的优化，想了想决定还是大概看看吧，万一之后有所涉及。</li>
<li>按照惯例放个链接：
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/111808111">CobbLiu - FAST20论文赏析（二）</a></li>
</ul>
</li>
<li>本文首次对企业存储系统中基于 NAND 的 SSD 进行了大规模的现场研究(与分布式数据中心存储系统中的驱动器形成对比)。该研究基于一组非常全面的现场数据，涵盖了一家主要存储供应商(NetApp)的 140 万份 SSD。驱动器包括三个不同的制造商，18种不同的型号，12种不同的容量，和所有主要的闪存技术(SLC, cMLC - consumer-class, eMLC - enterprise-class, 3D-TLC)。这些数据使我们能够研究很多之前没有研究过的因素，包括固件版本的影响，TLC NAND的可靠性，以及RAID系统中驱动器之间的相关性（为这些驱动器收集的数据非常丰富，包括驱动器替换(包括替换的原因)、坏块、使用情况、驱动器年龄、固件版本、驱动器角色(例如，数据、奇偶校验或备用)等信息）。本文介绍了我们的分析，以及由此得出的一些实际影响。</li>
</ul>
<h4 id="简要介绍-5">简要介绍</h4>
<h5 id="背景-3">背景</h5>
<ul>
<li>以前的存储设备的可靠性研究大多基于 HDD，近年来随着 SSD 的普及，对于 SSD 的可靠性研究才不断出现。现有的 SSD 可靠性研究除了在实验室的控制条件下的的相关研究以外，还有来自如 Facebook, Microsoft, Google, and Alibaba 等大型公司基于自己的数据中心中的数据对 SSD 可靠性的分析。但作者发现现有的研究中还是有一些 critical gap，所以本文就来填坑了：
<ul>
<li>没有研究关注企业级存储系统，这些系统中的驱动器、工作负载和可靠性机制可能与云数据中心中的非常不同。比如企业级存储系统中通常使用高端 SSD 并且可靠性通常由 RAID 来保障，而不是分布式存储中的那些副本的一些策略。</li>
<li>现有的研究没有涵盖构建现实的故障模型所需的一些最重要的故障特征，以便计算数据丢失的平均时间等指标，例如，这包括对驱动器替换原因的分析，包括底层问题的范围和相应的修复操作(RAID重建与耗尽驱动器)，以及最重要的是对同一RAID组中驱动器之间的相关性的理解。</li>
</ul>
</li>
</ul>
<h5 id="数据统计分析">数据统计分析</h5>
<ul>
<li>采集的系统和数据对应的相关信息此处不表，直接看相关数据统计。
<ul>
<li>前六列描述了不同厂商对应不同系列的 SSD 的相关参数，包括匿名给出了制造商、容量、接口、闪存颗粒技术、光刻技术、PE 周期（寿命）</li>
<li>后四列描述了不同的 SSD 被用于了何种的环境，包括 OP 比例（用于垃圾回收的保留空间比例）、第一次部署使用该 SSD 的日期、SSD 通电的中位数年数（因为每类 SSD 对应了很多个实际的 SSD）、SSD 的额定使用寿命的平均值和中位数（SSD 所经历的 PE 循环数占其 PE 循环极限的百分比）</li>
<li>最后三列描述了三种不同的 SSD 健康性和可靠性的指标：空闲块的使用比例、坏的扇区的数量、每年的置换率<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201103164701.png" alt="20201103164701" loading="lazy"></li>
</ul>
</li>
<li>从上表中的相关数据统计得出以下结论：
<ul>
<li>平均 ARR 为 0.22%，在 0.07% 到 1.2% 之间波动，比数据中心中的故障率要低得多</li>
<li>即便是具有相同工艺的同厂商的 SSD，相似容量和相似使用期限，年故障率也非常不一样</li>
<li>为坏块保留的备用区域为典型的驱动器提供了大量的资源：即使对于已经在数据中心中存放了好几年的 SSD，使用的备用块的百分比平均也不到 15%。即便是第 99.9 和 第 99.99 使用空闲空间最多的盘都分别为 17% 和 33%</li>
<li>很多 SSD 都没达到 PE 限制，即便是使用了 2-3 年的 SSD，第 99.9 和 第 99.99 寿命消耗的也只消耗了 15% 和 33%，对于绝大多数 SSD 盘来说，因为达到最大可擦写次数而失效的可能性几乎为零。</li>
</ul>
</li>
</ul>
<h5 id="原因分析">原因分析</h5>
<ul>
<li>有不同的原因可以触发更换 SSD，存储层次结构中的不同子系统也可以检测具体触发更换 SSD 的原因。可能由 SSD 本身或者存储层或者文件系统报告相应的问题，如下表所示描述了可能触发更换 SSD 的原因，以及它们的频率，以及系统采取的恢复操作(例如，从要替换的 SSD 复制数据与使用 RAID 重新构建数据)，以及问题的范围(例如，部分数据丢失的风险，完整驱动器丢失的风险，或者没有立即的问题)</li>
<li>原因被大致分为 4 类，分别以 ABCD 表示，严重程度递减。
<ul>
<li>最无关紧要的是 D 类，通常是由 SSD 内部或者更高的存储层在逻辑上触发的，即一些预测 SSD 未来故障的策略，基于之前发生的错误、超时以及磁盘的 SMART 统计信息等，但实际可能没坏。</li>
<li>最严重的是 A 类，通常是因为 SSD 变得完全无法响应，或者 SCSI 层检测到了 SSD 的问题严重到需要立即更换 SSD 并重建构建在当前 SSD 上的 RAID 时。</li>
<li>B 类是指替换发生在当系统怀疑 SSD 丢失了写操作的时候，例如 SSD 根本没有执行写操作，或者将其写到错误的位置，或者以其他方式破坏了写操作，根本原因可能是 SSD 中的固件错误，尽管存储堆栈中的其他层也可能是原因。由于有许多潜在的原因，heuristic（启发式判断） 被用来决定是否触发替换;具体地说，如果一个 SSD 中出现了多个这样的错误，而其他 SSD 中没有错误，那么前一个 SSD 将被替换。</li>
<li>C 类通常是因为命令被丢弃或者执行超时。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201103214752.png" alt="20201103214752" loading="lazy"></li>
</ul>
</li>
<li>抛开所有的分类看具体的错误原因，我们不难发现最常见的错误是 SCSI errors，大约有 1/3 的替换都是因为 SCSI errors 发生，同时也是最严重的错误之一；还有大约 1/3 的替换仅仅是为了预防 SSD 故障才采取的，也就是 D 类错误，预测了磁盘故障可能带来的严重影响，预测性的替换通常是统计了超时次数判断是否达到了阈值</li>
<li>后续测试了不同的因素对 SSD 的 annual replacement rate 的影响，主要对 eMLC 和 3D-TLC SSD 进行了分析，得出了以下发现和结论：
<ul>
<li>盘在刚开始使用的 1 年内的 ARR 是 1 年后 ARR 的 2-3 倍，并且盘片的 ARR 并没有随着使用时间的增加而增加，部分原因可能是因为绝大部分盘片的擦写次数都远没达到总擦写次数。</li>
<li>3D-TLC SSD 比其他类型的有更高的 ARR，但是这个差别很小，SLC、MLC 和 TLC（不同 flash 和 drive） 对 ARR 的影响比使用率对 ARR 的影响更小。</li>
<li>大容量 SSD 不仅整体替换率更高，而且 A 类错误概率更高，可预测的错误概率更低。</li>
<li>更高密度的 SSD 并不总是看到更高的替代率 ARR。事实上，我们观察到，尽管更高密度的 eMLC SSD 有更高的替代率，但这一趋势在 TLC 中是相反的</li>
<li>早期的固件版本可能与较高的替换率相关，这就强调了固件更新的重要性。</li>
<li>具有非空缺陷列表的 SSD 被替换的几率更高，不仅是因为可预测的故障，还因为其他替换原因</li>
<li>更多地使用其 OP 空间的 SSD 很可能在将来被替换</li>
<li>虽然大型 RAID 组有更多的驱动器替换，但我们没有发现每个组的多次故障率(这可能导致数据丢失)与 RAID 组大小相关的证据。原因似乎是在第一次失败后出现后续失败的可能性与 RAID 组大小无关</li>
</ul>
</li>
<li>最终作者总结了一下：
<ul>
<li>早期的固件版本可能与较高的故障率相关，所以得及时更新固件，要保证升级过程无痛且稳定</li>
<li>RAID 组的大小对 SSD 盘的平均 ARR 没有明显的影响</li>
<li>单奇偶校验 RAID 配置(例如，RAID-5)，可能容易发生数据丢失，而实际的数据丢失分析肯定必须考虑相关的故障。</li>
<li>具有非常大容量的驱动器总的故障率更高，出现更严重的故障。较高的故障率可能源于 SSD 上的更多 NAND 和die，这就强调了 SSD 及其系统能够处理部分驱动器故障(如 die 故障)的重要性，NetApp 正朝着这个方向努力，通过从 OP 区域来弥补部分故障的区域带来的容量损失</li>
<li>我们观察到，大容量的 SSD 预测失败率更小，这也提出了一个问题，即大容量 SSD 是否需要不同类型的故障预测器，以及是否需要更多来自 SSD 的内部问题(例如，坏死或 DRAM 问题)作为输入</li>
<li>随着 QLC NAND 的引入，人们重新开始关注 NAND SSD 的可靠性，其 PE 循环限制明显低于当前 TLC NAND。根据我们的数据，我们预测，对于绝大多数企业用户来说，向 QLC 的 PE 周期限制迈进不会带来任何风险，因为 99% 的系统最多使用其驱动器额定寿命的 15%</li>
<li>人们担心 NAND 有限的 PE 周期在 RAID 系统生命周期的后期，由于相关的磨损故障，SSD 可能会对数据的可靠性造成威胁，因为 RAID 组中的驱动器老化速度相同。相反，我们观察到，早期失效导致的相关失败可能是一个更大的威胁。例如，在我们的研究中，3D-TLC 驱动器，早期失效率高峰时的失败率比后期高出 2.5 倍</li>
<li>在选择 SSD 时，比起 flash type（比如 eMLC 还是 3D-TLC），工艺和容量更应该是首要的考量因素。</li>
</ul>
</li>
</ul>
<h4 id="个人见解-4">个人见解</h4>
<ul>
<li>这篇算是分析了 SSD 的一些物理特性和参数和 SSD 的故障之间的关系，以及 SSD 的替换策略的关系，但是文章开始的部分其实限定了场景为企业级存储，我个人对于企业级存储和如阿里等互联网企业的数据中心的存储这两种场景之间的差异其实不太具体理解，而且从 SSD 本身而言，这两种场景下，关于 SSD 的经验仿佛是通用的？这里有点一知半解</li>
<li>确实本篇文章有大量的数据支撑，最重要的其实都是文章总结出的那些发现，有的发现确实也一定程度上颠覆了以前大家对于 SSD 故障原因的一些固有认知，但可能缺少一些更为合理的解释？也可能是自己水平有限，对于相关成因的解释看的一知半解。</li>
</ul>
<h3 id="making-disk-failure-predictions-smarter">Making Disk Failure Predictions SMARTer!</h3>
<ul>
<li>这篇文章针对的领域是磁盘故障预测，之前有博客简要介绍过这篇文章，这篇文章的实验做的非常充分，整个行文也比较行云流水，照例贴一下链接：
<ul>
<li><a href="https://blog.shunzi.tech/post/AI-for-Systems-index/">shunzi - AI For System Papers Index</a></li>
<li><a href="https://nbjl.nankai.edu.cn/2020/0506/c12124a271163/page.htm">NBJL 2020论文导读24：Making Disk Failure Predictions SMARTer!</a></li>
</ul>
</li>
<li>磁盘驱动器是最常被替换的硬件组件之一，它继续对准确的故障预测提出挑战。在这项工作中，我们提出了一个最大的磁盘故障预测研究之一的分析和发现，涵盖了一个大型领先数据中心运营商的64个站点在两个月的时间里总共380,000个硬盘驱动器。我们提出的基于机器学习的模型在10天的预测周期内平均用0.95 F-measure和0.95 Matthews相关系数(MCC)预测磁盘故障</li>
<li>本篇文章就不展开介绍了，解决的其实就还是磁盘故障预测中的准确率低的问题和提前预警的时间较短的问题，而作者在磁盘故障预测的相关机器学习模型中引入了性能和物理位置条件的指标使得磁盘故障的预测模型更为准确，提前预警的时间也大约提升到了 10 天预警。</li>
<li>文章写的很清楚明朗，如果对磁盘故障预测感兴趣的同学，该文值得深入阅读。</li>
</ul>
<h2 id="performance">Performance</h2>
<ul>
<li>An Empirical Guide to the Behavior and Use of Scalable Persistent Memory
<ul>
<li>Jian Yang, Juno Kim, and Morteza Hoseinzadeh, UC San Diego; Joseph Izraelevitz, University of Colorado, Boulder; Steve Swanson, UC San Diego</li>
</ul>
</li>
</ul>
<h3 id="an-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory">An Empirical Guide to the Behavior and Use of Scalable Persistent Memory</h3>
<ul>
<li>关于这篇就不再详细展开了，资料很多，贴几个典型的其他大佬的资料。做 NVM 的肯定是都会看的 8。
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/108671363">zhihu: 暗淡了乌云 - An Empirical Guide For 3D XPoint Persistent Memory</a></li>
<li><a href="https://developer.aliyun.com/article/770338">阿里云开发者社区：Intel PMEM的使用经验和指南</a></li>
</ul>
</li>
<li>整体:<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200828104859.png" alt="20200828104859" loading="lazy"></li>
<li>内存控制器和硬件交互细节： 对NVDIMM的访问首先到达DIMM上的控制器（本文中称为XPController），该控制器协调对Optane介质的访问。与SSD相似，Optane DIMM执行内部地址转换以实现损耗均衡和坏块管理，并为该转换维护AIT (address indirection table)。地址转换后，将实际访问存储介质。由于3D-XPoint物理介质的访问粒度为256B（文中称为XPLine），所以，XPController会将较小的请求转换为较大的256字节的访问以提升性能。然而，因为同样的原因，小数据量的存储会变为RMW（read-modify-write）操作而导致写放大。 XPController有一个小的写合并缓冲区（在本文中称为XPBuffer），用于合并地址相邻的写操作。 由于XPBuffer属于ADR域，因此到达XPBuffer的所有更新都是持久的。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201116111320.png" alt="20201116111320" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20200828113725.png" alt="20200828113725" loading="lazy"></li>
<li>文章通过介绍 Optane PM 以及做了相关的测试得出了几个 Best Practice
<ul>
<li><strong>避免小于 256B 的随机读写</strong>；
<ul>
<li>Optane 的数据更新，在内部介质会进行 read-modify-write 操作。若更新的数据量小于内部操作的数据粒度（256B），会带来写放大，而使得更新效率低。</li>
<li>EWR（Effective Write Ratio，由DIMM的硬件测量）的概念：其为iMC发出的字节数除以实际写入3D-XPoint介质的字节数，即为写放大的倒数。EWR小于1表示，Optane介质写效率低。EWR也可以大于1，此时表示XP-Buffer做了写合并（在内存模式中，因为DRAM的缓存作用，EWR也可以大于1）</li>
<li>展示了Optane DIMM的带宽（三种store命令）与EWR的正相关的关系。一般而言，小数据量的存储使得EWR小于1。 例如，当使用单个线程执行随机的ntstore时，对于64字节的写，EWR为0.25，对于256字节访问，其EWR为0.98。值得注意的是，虽然iMC仅以64B为单位访问DIMM，但是XPBuffer可以将多个64B的写进行缓存，并合并为256B的Optane内部写，所以256字节更新是高效的。由上可知，如果Optane DIMM的访问具有足够好的局部性，同样可以高效地进行小数据量的存储。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201116112306.png" alt="20201116112306" loading="lazy"></li>
<li>为了得到“怎样的局部性才足够”的命题结论，我们设计了一个实验来测量XPBuffer的大小。 首先，我们分配N个XPLine大小（256B）的连续区域。 在实验中，进行循环的写数据。首先，依次更新每个XPLine的前半部分（128 B）， 然后再更新每个XPLine的后半部分。 我们测量每一轮后EWR的值。 图9显示： N 小于64（即16 kB的区域大小）时，EWR接近于1，其表明，后半部分的访问命中了XPBuffer。 N 大于64时，写放大进行了突变，其由XPBuffer miss急剧上升导致。这表明，XPBuffer的大小为16KB。进一步的实验表明，读操作也会占用XPBuffer中的空间从而造成竞争关系。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201116104322.png" alt="20201116104322" loading="lazy"></li>
</ul>
</li>
<li><strong>使用ntstore进行大数据（大于256B）写</strong>
<ul>
<li>一般通过下面操作进行数据写入：store操作后，程序员可以通过clflush/clflushopt操作进行高速缓存evict或通过clwb操作进行写回（write back)，以将数据写入至ADR域并最终写至Optane DIMM；或者，通过ntstore指令绕过高速缓存直接写入Optane DIMM。在进行完上述某种操作后，再进行sfence操作可确保先前的evict，write back和ntstore操作的数据变成持久的。写数据时，采用上述何种操作对性能影响很大。</li>
<li>对于写超过 64B 的数据，每store 64B进行cache的flush操作(相比于不进行flush操作）获得的带宽会更大</li>
<li>对于超过 512B 的访问，ntstore的延迟比store + clwb更低</li>
<li>对于超过 256B 的访问，ntstore操作的带宽也最高</li>
<li>当写入的大小超过 8MB时，写入后再进行刷新操作会导致性能下降，因为其导致了高速缓存容量的失效，从而使得EWR升高</li>
</ul>
</li>
<li><strong>限制访问 Optane DIMM 的并发线程数</strong>
<ul>
<li>XPBuffer的竞争。对XPBuffer中缓存空间的争用将导致逐出次数增加，触发写3D-XPoint介质，这将使EWR降低。</li>
<li>iMC中的竞争。每个线程随机访问N个DIMM（线程间分布均匀）。随着N的增加，针对每个DIMM的写入次数会增加，但是每个DIMM的带宽会下降。</li>
<li>当对交错的Optane DIMM进行随机 4KB 访问时，Optane带宽急剧下降。当写入的数据大小为 24kB 和 48kB，出现了性能的小峰值，其访问在6个DIMM上完美分布。</li>
</ul>
</li>
<li>避免 NUMA 访问（尤其对于是 read-modify-write 操作序列）。
<ul>
<li>Optane的NUMA效应远大于DRAM，因此应更加努力地避免跨插槽的存储器通信。对于读写混合且包含多线程访问的情况，其成本特别高。</li>
<li>对于本地和远程访问，单线程带宽差距不大。而对于多线程访问，随着访问压力的提高，远程访问性能会更快下降，从而导致相对于本地访问而言性能较低。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="key-value-storage">Key Value Storage</h2>
<ul>
<li>Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook
<ul>
<li>Zhichao Cao, University of Minnesota, Twin Cities, and Facebook; Siying Dong and Sagar Vemuri, Facebook; David H.C. Du, University of Minnesota, Twin Cities</li>
</ul>
</li>
<li>FPGA-Accelerated Compactions for LSM-based Key-Value Store
<ul>
<li>Teng Zhang, Alibaba Group, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang University; Jianying Wang, Xuntao Cheng, and Hao Xu, Alibaba Group; Nanlong Yu, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang University; Gui Huang, Tieying Zhang, Dengcheng He, Feifei Li, and Wei Cao, Alibaba Group; Zhongdong Huang and Jianling Sun, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang University</li>
</ul>
</li>
<li>HotRing: A Hotspot-Aware In-Memory Key-Value Store
<ul>
<li>Jiqiang Chen, Liang Chen, Sheng Wang, Guoyun Zhu, Yuanyuan Sun, Huan Liu, and Feifei Li, Alibaba Group</li>
</ul>
</li>
</ul>
<h3 id="characterizing-modeling-and-benchmarking-rocksdb-key-value-workloads-at-facebook">Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook</h3>
<ul>
<li>这篇文章可能和其他文章都有所不同，主要做 Benchmarking 方面的工作，和 Facebook 合作完成，对 Facebook 中现有的 RocksDB 典型的应用场景进行了表征，同时提出了一种新的更接近实际生产环境负载的模型考虑来取代 YCSB。照例先贴几个链接：
<ul>
<li><a href="https://jiangyuhang17.github.io/2020/07/16/FAST20-RocksDBBenchmark/">jiangyuhang17. - Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook 论文笔记</a></li>
<li><a href="https://www.pianshen.com/article/61721875389/">程序员大本营 - 【论文阅读】Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook</a></li>
</ul>
</li>
<li>持久化的键值存储是现代 IT 基础设施的基石，但是现有的表征 KV 存储的真是工作负载的研究有一定的局限，主要因为缺乏跟踪/分析工具以及在操作环境中收集跟踪的困难。本文对Facebook上三个典型的RocksDB生产用例的工作负载进行了详细表征:
<ul>
<li>UDB：用于MySQL中用来存储社交图数据，使用RocksDB作为底层存储；</li>
<li>ZippyDB：用于存储分布式对象存储的元数据的分布式KV；</li>
<li>UP2X：用于存储AI/ML数据的分布式KV。</li>
</ul>
</li>
<li>通过分析以上三种应用，有以下发现：
<ul>
<li>键和值大小的分布与用例/应用程序高度相关</li>
<li>KV 对的访问具有良好的局部性，并遵循一定的特殊模式</li>
<li>收集的性能指标在 UDB 中显示强烈的昼夜变化模式，而其他两个没有</li>
</ul>
</li>
<li>除此以外，作者表明现如今被广泛应用的 KV 负载 YCSB 尽管提供了各种工作负载的配置和 KV 对访问分布模型，但是忽视了键的空间局部性，和实际生产环境中的工作负载还是有一定的差距，于是本文提出了一种基于键范围的工作负载，并开发了一个可以更好地模拟真实键值存储的工作负载的基准。</li>
</ul>
<h4 id="简要介绍-6">简要介绍</h4>
<h5 id="introduction-background">Introduction &amp; Background</h5>
<ul>
<li>首先描述工作的意义，主要是解决现如今提升 KV 存储的性能比较困难，原因主要表现在：
<ul>
<li>对KVstores的真实工作负载表征和分析的研究非常有限，KV-stores的性能与应用程序生成的工作负载高度相关</li>
<li>描述KV-store工作负载的分析方法与现有的块存储或文件系统的工作负载特性研究不同（语义不同）</li>
<li>在评估KV-store的底层存储系统时，我们不知道KV-store基准生成的工作负载是否能代表真实的KV-store工作负载</li>
</ul>
</li>
<li>为了解决上面的问题，本文主要就做了三方面的事情：对 RocksDB 的 workload 进行 characterize, model, and benchmark
<ul>
<li>引入了一系列工具，可以在生产环境中应用，主要是收集 KV 层的查询 traces，replay traces 和分析 traces。且已开源：https://github.com/facebook/rocksdb/wiki/RocksDB-Trace%2C-Replay%2C-Analyzer%2C-and-Workload-Generation</li>
<li>为了更好地了解KV工作负载及其与应用程序之间的关系，分析了 UDB， ZippyDB，UP2X，有以下发现：
<ul>
<li>UDB 和 ZippyDB 中的查询主要是 read，而 UP2X 中的主要查询类型是 read-modify-write (Merge)</li>
<li>由于上层应用程序的键组合设计，键大小通常较小且分布狭窄，大的值大小只在某些特殊情况下出现</li>
<li>大多数 KV 对是冷的(访问较少)，只有一小部分 KV 对经常被访问</li>
<li>Get, Put, and Iterator 都有很强的针对基于键的空间局部性（比如经常访问的 KV 对通常在空间的分布上也相对较近），与上层应用程序的请求局部性密切相关的一些键范围非常热（经常被访问）</li>
<li>UDB 中的访问显式地表现为昼夜模式</li>
</ul>
</li>
<li>发现尽管 YCSB 可以生成与 ZippyDB 工作负载类似的键值(KV)查询统计数据，但 RocksDB 存储 I/Os 可能会有很大的不同，这个问题主要是由 YCSB 生成的工作负载忽略键的空间局部性这一事实引起的。YCSB 中热的键值对可以在整个键空间中随机分配，也可以聚集在一起，这将导致存储中访问的数据块与与 KV 查询相关的数据块之间的 I/O 不匹配。在不考虑键空间局部性的情况下，基准测试生成的工作负载将导致 RocksDB 的读放大和写放大比实际工作负载大。于是作者提出了一种基于键范围的热度的工作负载建模方法。整个键空间被划分为较小的键范围，并且我们对这些较小的键范围的热度进行建模。在新的基准测试中，将根据键范围热度的分布将查询分配给键范围，并且在每个键范围中热键将被紧密地分配。</li>
</ul>
</li>
<li>RockDB 介绍：相比于普通的 LSM Tree 或 LevelDB，从架构上来看主要是多了一个 Column Family 的概念<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201116170504.png" alt="20201116170504" loading="lazy"></li>
<li>三种应用场景介绍：
<ul>
<li>UDB：Facebook 的社交图数据长期存储在UDB中，这是一个分片 MySQL 数据库层。UDB 依赖 MySQL 实例来处理所有的查询，查询会被 MyRocks 转换成对于 RocksDB 的查询。图数据主要被维护成点和边，相应地在 RocksDB 中使用了不同的列族来存储对应的数据。收集了 14 天的 traces，也单独分析了最后一天 24 小时的负载
<ul>
<li>Object，Assoc，Assoc_count，Object_2ry，Assoc_2ry和Non_SG</li>
</ul>
</li>
<li>ZippyDB：基于 RocksDB 的分布式 KV 存储，使用 Paxos 来保证数据一致性和可靠性。KV 对被划分为切片，每个切片由一个 RocksDB 实例支持。选择一个副本作为主切片，其他副本作为次要切片。主切片处理对某个切片的所有写操作。如果读取需要强一致性，则读取请求(如 Get 和 Scan )仅由主切片处理。一个 ZippyDB 查询被转换为一组RocksDB 查询(一个或多个)。</li>
<li>UP2X：Facebook 使用各种 AI/ML 服务支持社交网络，并使用大量动态变化的数据集(如用户活动统计计数器)进行AI/ML预测和推断。UP2X 是分布式的专门开发的 KV-store，用于将这种类型的数据存储为 KV 对。当用户使用 Facebook 服务时，UP2X 中的 KV 对会经常更新，例如计数器增加时。如果 UP2X 在每个 Put 之前调用 Get 来实现读-修改-写操作，由于随机 Get 的速度相对较慢，它将产生很高的开销。UP2X 利用 RocksDB Merge 接口避免在更新过程中获取 Gets。</li>
</ul>
</li>
</ul>
<h5 id="methodology-and-tool-set">Methodology and Tool Set</h5>
<ul>
<li>开源的 RocksDB 负载分析和表征工具 https://github.com/facebook/rocksdb/wiki/RocksDB-Trace%2C-Replay%2C-Analyzer%2C-and-Workload-Generation</li>
<li><strong>Tracing</strong>：工具收集 RocksDB 对外暴露的开放接口对应的查询信息，并将信息记录在 trace files 中。主要包括 query type、CF ID、key、query specific data、timestamp。对于 Put 和 Merge，将值信息存储在特定于查询的数据中，对于 Seek 和 SeekForPrev 之类的迭代器查询，扫描长度(在 Seek 或 SeekForPrev 之后调用 Next 或 Prev 的次数)存储在特定于查询的数据中。为了在跟踪文件中记录每个查询的跟踪记录，需要使用锁来序列化所有查询，这可能会带来一些性能开销，但是，根据常规生产工作负载下的生产中的性能监视统计数据，我们没有观察到跟踪工具导致的吞吐量下降或延迟增加。</li>
<li><strong>Trace Replaying</strong>：回放工具根据跟踪记录信息向 RocksDB 发出查询，查询之间的时间间隔遵循跟踪中的时间戳，通过设置不同的快进和多线程参数，RocksDB 可以对不同强度的工作负载进行基准测试。但是，多线程不能保证查询顺序。Replayer 生成的工作负载可以看作是真实世界的工作负载。</li>
<li><strong>Trace Analyzing</strong>：由于工作负载跟踪的潜在性能开销，很难跟踪大规模和长时间的工作负载，此外，跟踪文件的内容对其用户/所有者来说是敏感和机密的，因此，RocksDB 用户很难与其他 RocksDB 开发人员或第三方公司的开发人员共享跟踪信息。为了解决这些限制，我们提出了一种分析 RocksDB 工作负载的方法，该方法根据跟踪中的信息来分析工作负载。
<ul>
<li>针对每一个 CF 中的 KV 对、query numbers、query types 的详细统计摘要</li>
<li>key value 大小统计</li>
<li>kv 对的流行度（热度）</li>
<li>键的空间局部性，它将访问的键与数据库中所有现有的键按排序顺序组合在一起</li>
<li>查询次数/秒 统计</li>
</ul>
</li>
<li><strong>Modeling and Benchmarking</strong>：首先选定两个变量计算对应的相关系数，来确定变量相关性较低。通过这种方式，每个变量都可以单独建模，然后我们将收集到的工作负载匹配到不同的统计模型中，以找出哪一个具有最低的拟合误差，哪一个比总是将不同的工作负载匹配到相同的模型(如Zipfian)更精确。然后，该基准可以基于这些概率模型生成KV查询。</li>
</ul>
<h5 id="general-statistics-of-workloads">General Statistics of Workloads</h5>
<ul>
<li>我们将介绍每个用例的一般工作负载统计，包括每个 CF 中的查询组合，KV-pair 热度分布和每秒查询数。</li>
<li><strong>Query Composition</strong>：Get 是 UDB 和 ZippyDB 中最常用的查询类型，而 Merge 在 UP2X 查询中占主导地位。在不同的 CF 中查询组合可能都会有很大不同，
<ul>
<li>如下图所示 UDB 的统计<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201117092355.png" alt="20201117092355" loading="lazy"></li>
<li>ZippyDB 只有一个 CF，Get : Put : Delete : Iterator = 78 : 13 : 6 : 3</li>
<li>UP2X Merge : Get : Put = 92.53 : 7.46 : 0.01</li>
</ul>
</li>
<li><strong>KV-Pair Hotness Distribution</strong>：UDB 和 ZippyDB 中大部分 KV 数据是冷数据
<ul>
<li>UDB 24 小时内访问的 key 最高不超过 3%，而 14 天内访问的 key 最高不超过 15%。如下为 UDB 的 Get 与 Put 操作的 KV 数据访问的 CDF 图。对于 Get，除了 Assoc 之外，其他 CF 的数据 60% 或者以上的数据都只被访问了一次。对于 Put，超过 75% 的数据都只被访问一次，Put 次数超过 10 的数据仅占不到2%，所以UDB中的KV数据大部分很少被Update。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201117093340.png" alt="20201117093340" loading="lazy"></li>
<li>对于ZippyDB，大约80%的key只被访问一次，1%的key被访问超过100次，因此表现出较好的局部性。约73%的数据只被Put一次，访问次数超过10次的数据仅有0.001%，因此Put的局部性较差。</li>
<li>UP2X：对于UP2X，Get与Merge的访问次数分布较广，并且访问次数高的数据占比比较大。</li>
</ul>
</li>
<li><strong>QPS (Queries Per Second)</strong>：UDB 的部分 CF 表现出较强的昼夜模式，这跟社交网络用户习惯相关（白天上 Facebook，晚上在睡觉），ZippyDB 和 UP2X 没有表现出这样的特征。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201117094915.png" alt="20201117094915" loading="lazy"></li>
<li><strong>Key and Value Sizes</strong>：key size通常比较小，value size的大小与具体数据类型有关，key size的标准差较小但是value size较大， UDB平均的value size比其他两个例子要大<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201117095426.png" alt="20201117095426" loading="lazy"></li>
<li><strong>Key-Space and Temporal Patterns</strong>：统计方法：对key按递增顺序编号，然后统计每个key的访问次数绘制heat-map，统计key的访问时间绘制time-series。结论：heat-map可以看到三种DB的访问具有较强的key space locality，也就是热数据往往聚集分布在某些key space内。UDB的Delete/Single Delete以及UP2X的Merge的time-series表明其访问具有时间局部性
<ul>
<li>UDB：KV数据访问并不会随机分布在整个key space，而是根据key-space进行区分，部分key-space访问热度较高，这部分数据占比较小，而部分基本没有访问。属于同一个MySQL table的数据物理上也相邻存储，部分SST和block具有较高的热度，可以考虑基于这个优化compaction以及cache。</li>
<li>ZippyDB：ZippyDB的访问具有较为明显的key-space locality</li>
<li>UP2X：UP2X的访问也表现出较强的key-space locality，只有后半段key被访问，而前半段key基本没有访问。merge的time-series表现出来merge每一段时间内会集中访问一个range内的key。</li>
</ul>
</li>
</ul>
<h5 id="modeling-and-benchmarking">Modeling and Benchmarking</h5>
<ul>
<li>一些研究使用YCSB/db_bench + LevelDB/RocksDB来基准测试KV存储的存储性能。研究人员通常认为YCSB产生的工作量接近于实际工作量。对于实际的工作负载，YCSB可以针对给定的查询类型比率，KV对热度分布和值大小分布生成具有相似统计信息的查询。但是，尚不清楚它们在实际工作负载中生成的工作负载是否与基础存储系统的I/O相匹配。</li>
<li>为了对此进行调查，我们集中于存储I/O统计信息，例如由RocksDB中的perf_stat和io_stat收集的块读取，块缓存命中，读取字节和写入字节。为了排除可能影响存储I/O的其他因素，我们重放跟踪并在干净的服务器中收集统计信息。基准测试也在同一服务器中进行评估，以确保设置相同。为确保重放期间生成的RocksDB存储I/O与生产环境中的I/O相同，我们在收集跟踪的同一RocksDB的快照中重放跟踪。快照是在我们开始跟踪时创建的。YCSB是NoSQL应用程序的基准测试，而ZippyDB是典型的分布式KV存储。因此，预期YCSB生成的工作量接近ZippyDB的工作量，我们以ZippyDB为例进行调查。由于特殊的插件要求以及UDB和UP2X的工作量复杂性，我们没有分析这两个用例的存储统计信息。</li>
<li>总的来说YCSB测workload相比于真实的trace会造成更大的读放大以及更小的写放大，同时cache命中率也更低。其中的主要原因在于忽略了真实workload的key space locality，YCSB中的热点数据随机分布在整个key范围内，访问这些key会造成大量的block读并且被缓存，而这些block可能仅包含较少的热数据，cache大小有限所以降低了cache命中率。对于put，随机的热点分布使得数据在前几层就被compaction掉，所以造成更小的写放大，update的数据具有key space locality，那么新数据会不断写入，就数据一直往下compact直到新数据遇到旧数据才会被处理掉。</li>
<li><strong>Key-Range Based Modeling</strong>：整个键空间被划分成几个较小的键范围。我们不再仅仅基于整个键空间统计数据对KV对访问进行建模，而是关注这些键范围的热度。实验发现 <strong>当键范围大小接近SST文件中KV对的平均数目时，它可以保留数据块级别和SST级别的局部性。因此，我们使用每个SST文件的平均KV对数作为键范围大小。</strong> 然后将key size，value size以及QPS套到模型里面去，再处理kv的访问次数，访问顺序以及每个range的平均访问次数，然后和原有的负载进行对比测试。
<ul>
<li>Prefix_dist：基于建模构建的workload</li>
<li>Prefix_random：随机将冷热数据分布到各个key-range</li>
<li>All_random：热数据随机分布到整个key space</li>
<li>All_dist：热数据集中放置<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201117100641.png" alt="20201117100641" loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="个人见解-5">个人见解</h4>
<ul>
<li>做 Benchmarking 的工作今年来比较少，一般也只有相应的企业才能统计出相应的数据特征。Benchmarking 常常意味着一系列工具都需要去实现，如本文中的负载的 Trace 收集分析统计，意义很大，对于后续的 KV 存储研究提供了新的基准测试，也配套了相应的工具可以自己去收集相应的负载，且已开源。总之对于 KV 存储的研究，未来可能会成为新的测试基准。</li>
</ul>
<h3 id="fpga-accelerated-compactions-for-lsm-based-key-value-store">FPGA-Accelerated Compactions for LSM-based Key-Value Store</h3>
<ul>
<li>本文主要还是解决 LSM Tree 中的压缩慢的问题，压缩慢还会因为资源的争用影响整个存储系统处理的性能，然后呢也是得出相同的结论，存储系统的瓶颈在往 CPU 转移，这个观点在之前看的 KVell （SOSP19）的那篇文章就已经充分说明。采用的解决办法呢其实就是替 CPU 减压，引入一个新的处理单元进来，FPGA，因为 compaction 操作其实本质就是归并排序，所以 FPGA 完全可以胜任，这个思路其实也不是很新奇，ATC20 的 Best Paper 的 PinK 其实也采用了这种方法。</li>
<li>其实问题和思路都不算是特别地别具一格（这里没有去追究前面文章里的具体的时间先后顺序），但是毕竟是经历了工业界的验证的，阿里巴巴自研的 X-Engine 的存储引擎就使用了本文描述的技术，所以也不妨深入读一读，看看有什么有意思的点。</li>
<li>照例贴链接：
<ul>
<li><a href="https://developer.aliyun.com/article/748726">阿里云-开发者社区：X-Engine 研究综述</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/100139565">知乎：匠心之作 | 厉害了！阿里云自研存储引擎X-Engine又发顶会啦</a></li>
<li><a href="https://www.jianshu.com/p/2e2b37fd3ad6">简书 - Glitter试做一号机：FPGA-Accelerated Compactions for LSM-based Key-Value Store</a></li>
</ul>
</li>
<li>本文主要提出的方法就是将压缩给下沉到 FPGA 来执行，从而加速压缩，减小 CPU 瓶颈。测试表明该方法加速压缩 2-5 倍，系统吞吐量提升了 23%，能源效率（每瓦特处理的事务数量）提升了 31.7%。</li>
<li>BTW，X-Engine 的论文发表在 SIGMOD19</li>
</ul>
<h4 id="简要介绍-7">简要介绍</h4>
<h5 id="introduction-background-2">Introduction &amp; Background</h5>
<ul>
<li>
<p>首先大致介绍了基于 LSM Tree 的 KV 的应用场景，以新零售为例做了简单介绍。除此以外，KV 主要还会和其他数据库一起作为缓存或者索引来提供服务，这也是 KV 存储的关键的一些应用。然后简单介绍 LSM Tree，LSM 这种数据组织形式通常迫使查询遍历多个级别，以合并分散的记录以获得完整的答案或查找记录，甚至使用索引。这样的操作会带来跳过标记为删除的无效记录的额外开销。为了控制这些 drawbacks，后台压缩操作被引入，在相邻的层之间合并键范围重叠的数据块，并删除已经标记为删除的记录，目的是保持 LSM 树在一个适当的分层形状。</p>
</li>
<li>
<p>引入了一个新的概念，<strong>WPI（write and point read-intensive）负载</strong>，在长时间的 WPI 负载下，因为 LSM Tree 本身数据结构维护的不好（比如出现过大的 levels），性能就会表现得越来越差。</p>
</li>
<li>
<p>作者总结发现在 LSM Tree 中有一个很困难的 trade-off，即分配给关键路径上查询操作和事务处理的资源 和 分配给后台压缩线程的资源（后台压缩操作需要消耗大量的计算资源和磁盘 I/O 资源，特别是对于既包含读又包含写的 WPI 负载），如果给 compactions 分配更多的软件线程，就会在降低 CPU 实际处理查询和事务的风险下，加大了对存储的后台维护。</p>
</li>
<li>
<p>如图 1 所示，描绘了在 WPI （75% 点查询、25% 写操作）的负载下吞吐量随着用于 Compaction 线程增加的变化情况。线程数小于 32 之前都是随着线程数单调递增，从而带来显著的性能优势。然而，随着更多的线程用于 Compactions，CPU 逐渐饱和，然后会产生 CPU 争用，因此系统吞吐量在 32 个线程之后下降。后文将有更详细的研究表明 32 线程压缩的时候仍然不够快，无法解决上述问题。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201207203824.png" alt="20201207203824" loading="lazy"></p>
</li>
<li>
<p>现有的优化 Compaction 的研究主要采用了两种方法：</p>
<ul>
<li>一种是通过利用数据分布的特点（几乎是有序的，没有重叠的范围）来减小来减少每次压缩的负载，从而避免不必要的合并。如 X-engine（SIGMOD19），VT-Tree（FAST13）
<ul>
<li>或者通过将数据分割到多个分区，并在需要时分别为每个分区调度 Compaction 操作。The partitioned exponential file for database storage management(VLDBJ07)</li>
</ul>
</li>
<li>另外一种方法是优化压缩的时机和选择哪些数据来压缩(when and where)。bLSM (SIGMOD12)。理想情况下，为了提高性能，压缩应该在最需要的时候完成，并且它的执行与系统中其他操作的资源竞争最小
<ul>
<li>但是上述的关于 when 和 where 的条件在 WPI 负载下经常都是会有冲突的，因为对compaction 的需求和对高吞吐量的需求经常同时达到峰值。因此，对于CPU和I/O，存储系统中的资源竞争仍然是一个挑战，通过增加 CPU 线程限制了压缩速度的可伸缩性，并留下了性能问题。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>本文提出讲 Compaction 的压力从 CPU 向 FPGA 转移从而加速压缩的执行。理论角度上分析，</p>
<ul>
<li>offload compaction 一定程度上将 CPU 从 I/O 密集型应用中解放了出来，从而让存储系统使用更少的 CPU 或者在使用相等数量的 CPU 的情况下增加了吞吐量，在公有云的场景下都能降低成本开销。</li>
<li>相比于更喜欢进行 SIMD（单指令多数据流） 类型的计算的 GPU，FPGA 更符合加速压缩操作的需求，压缩任务本质是计算任务的 pipeline。我们还发现对于比较小的 KV 对的压缩合并往往会被计算资源给限制住，可能是因为现在磁盘的 I/O 带宽越来越高。</li>
<li>与其他方案相比，FPGA 的高能源效率在降低总拥有成本(TCO)方面具有竞争优势。KV 存储的用户对于成本其实很敏感，因为存储往往都意味着需要永久支出这笔成本。所以在云场景下，使用 FPGA 来 offload compaction 将能显著提升 LSM Tree 的经济效益。</li>
</ul>
</li>
<li>
<p>在 FPGA 上，我们设计并实现了压缩操作，一个包含了三个阶段的流水线操作：</p>
<ul>
<li>decoding/encoding inputs/outputs</li>
<li>merging data</li>
<li>managing intermediate data in buffers</li>
</ul>
</li>
<li>
<p>我们还实现了一个 FPGA 驱动程序和异步压缩任务调度器，以方便 offload 和提高效率，该方案被集成在 X-Engine 中，使用了不同的 WPI 负载来进行测试。效果显著。</p>
</li>
<li>
<p>以前的 LSM Tree 结构如图 a 所示，C0 满了之后合并到 C1，合并的开销会随着 C1 的大小的增加而增加，为了限制这样的开销，更好的选择是是把一个磁盘组件分成不同层级的多个组件，每个组件然后比前一层的组件大。但是会有写放大，因为一个 KV 对可能不得不合并很多次，同时也有读放大，因为查询不得不访问多个具有重叠键范围的组件。</p>
</li>
<li>
<p>为了限制读写放大，许多研究提出了如图 b 所示的分层存储结构。该结构具有优化的内存数据结构、多层磁盘组件，每一层由多个文件或细粒度的数据块组成。数据首先被插入到内存的 memtables 中（常用跳表来实现），一旦 memtables 满了之后转变成 immutable memtables，然后刷入到磁盘上的 L0 层，这里的 Lk 和以前的 LSM 树中的 Ck 是类似的，最大的区别是 Lk 是被分区成很多个文件（也就是 RocksDB 中的 SSTables）或者数据块（X-Engine 中的 extents），对应的合并策略有两种类型：</p>
<ul>
<li>将其与目标级别中的现有数据合并，即所谓的 level 策略，这种方法以压缩速度为代价，将数据按良好的排序顺序保存在某个级别中</li>
<li>另外一种只需将数据追加到下一层，而不进行合并，这称为 tier 策略，压缩本身速度很快，但会牺牲某个级别内的排序顺序<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201208111918.png" alt="20201208111918" loading="lazy"></li>
</ul>
</li>
<li>
<p>尽管上面介绍了最先进的优化，但我们发现，由于以下原因，压缩速度慢仍然会导致运行WPI 工作负载的 LSM-tree KV 存储的性能疲劳问题：</p>
<ul>
<li>Shattered L0：L0 层的数据块通常具有重叠的键范围，因为它们直接从主存中刷新而没有合并。除非压缩及时合并它们，否则点查找可能不得不检查多个块，以找到单个键，甚至索引也是如此。在这种压缩速度慢的情况下，随着时间的推移，L0 中存储的数据块会不断增加查找开销。这种破碎的 L0 对性能有重大影响，因为由于数据局部性，刷新到L0的记录仍然非常热(即很可能被访问)</li>
<li>Shifting Bottlenecks：压缩操作自然由多个阶段组成:解码、合并和编码，因为KV记录通常是前缀编码的。为了确定这些阶段中的瓶颈，我们对 SSD 上由单个 CPU 线程执行的单个压缩任务进行概要分析。如图所示执行时间分解，随着 value 大小的增加，计算时间(解码、合并、编码)的百分比减少。而对于小 KV，计算占据整个压缩过程的 60% 的开销。当 value 大小达到 128 字节时，I/O 操作占用了大部分 CPU 时间。这个分解表明随着 KV 大小的增加，瓶颈从 CPU 转移到 I/O。这表明在合并小 KVs 时压缩以计算为限，在其他情况下压缩以 I/O 为限。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201208202644.png" alt="20201208202644" loading="lazy"></li>
</ul>
</li>
<li>
<p>本文就是将上述的三个阶段给映射成 pipeline，并 offload 到专门的加速器。因为有了更快的压缩操作，L0 的数据块合并的更加频繁，因为 offloading，CPUs 从 heavy 的压缩操作中释放出来，从而腾出更多的资源用于事务和查询操作的处理。加速器选了 FPGA，因为 FPGA 它适合加速压缩等计算任务的 pipeline，且具有低 TCO 的低能耗，以及作为可插拔的 PCIe-attached 加速器的灵活性</p>
</li>
<li>
<p>原文还有一些关于 FPGA 本身的介绍，以及 FPGA 和 KV Store 结合的场景介绍。此处简单提及和 KVs 的结合问题，现有的研究大致有两种结合方式：</p>
<ul>
<li>bump-in-the-wire：将 FPGA 放在 CPU 和 disk 之间，该方法中，FPGA 类似于一个 data filter，在 FPGA 的片上 RAM 较小，只能临时保存数据流的一个 slice</li>
<li>随着片上 RAM 大小增加，FPGA 现在能像一个 co-processor 一样工作，所以可以把一些大数据量的处理任务给 offload 到 FPGA 上执行。这种方法适用于异步任务，在此期间，CPU 不会因为 offloaded 任务而 stall，本文就采用了这种方式，CPU 只用于任务的生成。Samsung 的 SmartSSD 也采用了这种方案实现计算型存储。</li>
</ul>
</li>
</ul>
<h5 id="design-and-implementation">Design and Implementation</h5>
<ul>
<li>
<p><strong>Overview</strong>：利用 Memtables 缓冲新插入的 KV 记录，利用缓存来缓冲热 KV 对和数据块，磁盘上的每一层包含多个 extents，每个 extent 依次用关联的索引和过滤器存储 KV 记录。X-Engine 在内存中还维护了一个全局索引用于加速查询。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201208211801.png" alt="20201208211801" loading="lazy"></p>
</li>
<li>
<p>为了 offload compactions，首先设计了一个 Task Queue 来缓冲新触发的压缩任务，一个 Result Queue 缓冲在内存中的压缩过的 KV 记录。软件层面，引入了一个 Driver 来 offload compactions，包括管理从 host 到 FPGA 的数据传输过程，在 FPGA 上，设计并应用了多个 Compaction Units (CUs) 来负责合并 KV 对。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201208212133.png" alt="20201208212133" loading="lazy"></p>
</li>
<li>
<p><strong>Managing Compaction Tasks</strong>：设计了三种线程，builder threads, dispatcher threads and driver threads 分别用于构建压缩任务、分发压缩任务到 FPGA 的 CU，install 压缩后的数据块到存储中。</p>
<ul>
<li><strong>Builder thread</strong>：对于每个触发的压缩，Builder thread 将 extents 分区合并到多个大小相近的组中，每个组然后形成一个压缩任务，且将数据加载在内存中。FPGA 压缩任务。将构建一个 FPGA 压缩任务，其中包含所需的元数据，包括指向任务队列的指针、输入数据、结果和一个回调函数（将压缩后的数据块从FPGA传输到主存）、返回代码（指示任务是否成功完成）和压缩后任务的其他元数据。压缩任务会被推送到 task queue，等待被分发到 FPGA 的 CU 上，Builder Thread 也会检查 result queue 并把压缩后的数据块 install 回存储设备中（如果任务被成功处理的话），如果 FPGA 处理压缩任务失败了（比如 KV 的大小达到 FPGA 的容量），将重新启动一个 CPU 压缩线程来重新处理该任务。实践表明，offload 到 FPGA 上平均只有 0.03% 的任务处理失败。</li>
<li><strong>Dispatcher thread</strong>：dispatcher 消费任务队列，然后以 Round-Robin 的策略把任务分发到 FPGA 上所有的 CU。由于压缩任务大小相似，这种循环调度实现了 FPGA 上多个 CUs 之间的工作负载均衡分配。dispatcher 还会通知驱动线程将数据从设备内存传输到 FPGA。</li>
<li><strong>Driver thread</strong>：Driver thread 将输入数据和一个压缩任务一起传输到 FPGA 上的 device memory，然后通知对应的 CU 开始工作，当压缩任务完成之后，Driver thread 被中断以执行回调函数，回调函数将压缩后的数据块传输到宿主机 Memory，然后把完成后的任务推送到 result queue。</li>
<li>在这样的实现中，需要调整压缩任务的大小以为 compaction unit 提供充分的数据，在 CUs 之间保证负载均衡，同时也可以限制重试在 FPGA 上执行任务的开销。我们还会对 builder/dispatcher/installer 线程数单独进行调整，以实现比较稳定的吞吐量表现。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209092309.png" alt="20201209092309" loading="lazy"></li>
</ul>
</li>
</ul>
<p><strong>Instruction and Data Paths</strong></p>
<ul>
<li>为了驱动 FPGA 用于压缩，我们需要通过 PCIe 总线传输指令和数据，为了最大化传输效率，设计了 Instruction and Data Paths，还设计了 Interruption Mechanism 来通知 Installer Thread，Memory Management Unit (MMU) 来管理 FPGA 上的设备内存。
<ul>
<li>Instruction Path：指令路径是为像 CU 可用性检查这样的小而频繁的数据传输而设计的</li>
<li>Compaction Data Path：数据路径使用 DMA。要压缩的数据通过此路径传输。这样，CPU 就不参与数据传输过程了。</li>
<li>Interruption Mechanism：当压缩任务完成时，将通过PCIe发送一个中断，然后在中断向量的帮助下将结果写回主机</li>
<li>Memory Management Unit (MMU)：MMU 在设备存储器上分配内存来存储从主机复制的输入数据<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209100819.png" alt="20201209100819" loading="lazy"></li>
</ul>
</li>
</ul>
<p><strong>Compaction Unit</strong></p>
<ul>
<li>压缩单元(CU)是在FPGA上实现压缩操作的逻辑实现。很多个 CUs 被部署到同一个 FPGA 上，具体的数量取决于时机可用的资源。如下图所示了 CU 的设计，如下设计中，压缩任务由几个阶段共同组成：Decoder, Merger, KV Transfer, and Encoder。我们引入缓冲区<br>
(即KV 环缓冲器、键缓冲器)在后续模块和控制器之间协调各模块的执行。
<ul>
<li><strong>Decoder</strong>：KV 存储的 key 通常都是前缀编码的，为了节省空间，所以 Decoder 需要解码输入的 KV 记录。如果采用了 tiering compaction 策略，可能存在多种输入数据块的合并方式。而对于 levelling policy，只会有最多两个输入数据的方式，来自两个相邻的层。同时我们实际发现 tiering 最多也只有两个或四个输入方式，如果在一个 CU 里我们放置两个 decoders，每一个 decode 一种输入，那么我们需要构建三个 两路压缩 来实现一个 四路压缩，如果我们使用四个 decoder 会多消耗 40% 的硬件资源，而无需在 2-4 路压缩方式下执行额外的任务。因此我们在每个 CU 中放置了四个 decoders，decoder 将 解码后的 KV 对输送到了 KV Ring Buffers 中。</li>
<li><strong>KV Ring Buffer</strong>：KV Ring Buffer 缓存解码后的 KV 记录，我们在实践中观察到 KV 的大小很少超过 6KB，因此我们给每一个 KV Ring Buffer 配备了 32*8KB 的 slots，额外的 2KB 可以用于存储如 KV 长度这样的元数据信息。我们还设计了三种状态信号， FLAG_EMPTY, FLAG_HALF_FULL and FLAG_FULL 来表示 buffer 的空间情况。从一个空缓冲区开始，解码器持续解码并填充这个缓冲区，当处于 FLAG_HALF_FULL 时，下游的 Merger 将被允许独去缓冲区中填充的数据然后开始合并，在 Merger 工作的同时，decoder 持续地填充数据直到填满，我们匹配 merger 和解码器的速度，这样，通过填充一半的环形缓冲区，merger 将花费相同的时间来完成它的工作。通过这种方式可以有效地将 decoder 和 merger 流水线化。如果两个模块速度不匹配，Controller 将停止 decoder。我们为 merger 维护了一个读指针来标记从哪里读取的 KV 记录，并为解码器类似地维护了写指针。</li>
<li><strong>KV Transfer, Key Buffer and Merger</strong>：只有 keys 被传输到了 Key Buffer，然后 Merger 进行比较。如果一个 key 被限定为一个输出，KV Transfer 模块将相应的 KV 记录从上游的 KV Ring Buffer 传输到 KV Output Buffer，数据结构和 Ring Buffer 完全相同。Controller 收到比较结果的通知后移动 ring buffer 中相应的 read pointer。如图 9 所示，如果 way 2 上的 KV 是最小的 KV，Controller 将会通知 KV Transfer 将该条记录传输，使用对应的读指针进行寻址，然后再移动到下一个位置来拉取用于下一轮比较的 entry。</li>
<li><strong>Encoder</strong>： 该模块编码合并后的 KV 记录，并放置到设备内存，因为这只有一种合并后的数据，所以每个 CU 只需要一个 encoder。</li>
<li><strong>Controller</strong>：Controller 在 CU 中更多地是作为一个协调者，管理 KV ring buffer 的读写指针（分别为 merger 和 decoder 管理），也需要给每一个信号传递开始或者停止的信号。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209105209.png" alt="20201209105209" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209111854.png" alt="20201209111854" loading="lazy"></li>
</ul>
</li>
</ul>
<p><strong>Analytical Model for CU</strong>：</p>
<ul>
<li>由于采用流水线设计，因此匹配不同模块的吞吐量非常重要，以避免硬件资源的过度供应和浪费，但是，很难通过如此多的调优选项来确定每个组件的资源数量。因此提出了一种分析模型来指导资源分配。参数如下：<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209113239.png" alt="20201209113239" loading="lazy"></li>
<li>CU 的吞吐量由最慢阶段的吞吐量来决定，只有一个例外 KV Transfer，当 KV Transfer 工作时，合并 KVs 的转移与其前一阶段的 Merger 串行执行，这种情况下，他们的开销应该合并起来计算。每一阶段的成本由其计算和内存开销组成，除了在启动此阶段时消耗的恒定数量的基本周期之外。</li>
<li>等式 2 3 4 5 都建模了每个 KV 对在对应组件内的开销，等式 6 建模了在 host 和 device 之间数据传输的吞吐量，主要受到 PCIe 带宽的限制。通过检查硬件实现和性能分析，我们使用表1中列出的数据初始化这些模型。例如一个 KV 对，8B key，32B value，每个 KV 在每个阶段消耗的周期为 16， 55， 23， 52（Decoder, Merger, KV ransfer, and Encoder stages）。整体的吞吐量为 3.6M records/s<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209113313.png" alt="20201209113313" loading="lazy"></li>
</ul>
<h5 id="evaluation">Evaluation</h5>
<ul>
<li>
<p>Hardware：</p>
<ul>
<li>two Intel Xeon Platinum 8163 2.5 GHz 24-core CPUs with two-way hyperthreading</li>
<li>a 768 GB Samsung DDR4-2666 main memory</li>
<li>a RAID 0 consisting of 10 Samsung SSDs</li>
<li>a Xilinx Virtex UltraScale+ VU9P FPGA board (running at 200MHz) with a 16 GB device memory to this server through a x16 PCIe Gen 3 interface</li>
</ul>
</li>
<li>
<p>Software: Linux 4.9.79, X-Engine</p>
</li>
<li>
<p>KV 对越小，更多的开销都是在 offload 过程，随着 KV 的增大，逐渐被稀释这部分开销，所以 KV 对越大，性能提升越明显<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209114414.png" alt="20201209114414" loading="lazy"></p>
</li>
<li>
<p>总吞吐量总是限制在合并和 KV 传输阶段的组合，既不不是对设备存储器的访问，也不是通过 PCIe 的数据传输<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209114638.png" alt="20201209114638" loading="lazy"></p>
</li>
<li>
<p>24 线程以前，压缩比 KV 存储的总写吞吐量要慢，因此它不能摄取lsm树中足够的新写数据。因此，使用更多线程加速压缩是有回报的。从24个线程到32个线程，吞吐量几乎没有变化。大于 32 个线程之后，随着线程的增加吞吐量下降。原因主要在于更多的线程也就引入了更多的资源争用，如前面图 1 所示整体的 CPU 利用率接近 100%，此时瓶颈在于 CPU。第二个原因是前面增加的线程已经让磁盘 I/O 饱和，持续增加线程只会造成更多的查询/事务处理和 compaction 之间的 I/O 争用。</p>
</li>
<li>
<p>FPGA-offloading 和 CPU-based 两种对比发现，FPGA-offloading 比最好的 CPU-based 的吞吐量都要高出 23%，因为不仅加速了压缩，还减少了 CPU 的争用。又因为 FPGA 上的压缩任务比较好的调度和分配，没有观察到对于内存的消耗上有太显著的区别，CPU-Only 的负载相对消耗更多的内存带宽。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209115040.png" alt="20201209115040" loading="lazy"></p>
</li>
<li>
<p>吞吐量都有提升，延迟有所下降，尾延迟也有所下降，操作的响应时间也有降低，能源消耗也有所降低，整体的能源效率有所提高。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209151041.png" alt="20201209151041" loading="lazy"></p>
</li>
<li>
<p>当读比例在 WPI 负载中下降到了 75% 时，也就是此时成为一个写密集的 WPI 负载，我们提出的卸载方法的性能优于基准，在所有情况下都减少了约 10% 的 CPU 消耗，并且由于其吞吐量高于 CPU，因此消耗的 I/Os 稍微多一些。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209151956.png" alt="20201209151956" loading="lazy"></p>
</li>
<li>
<p>在 DBBench 和 YCSB 基准测试中，与读密集型的操作相比，当工作负载中有大量写操作时，提出的 FPGA-offload 压缩对 KV 存储的贡献更大，因为压缩在工作负载中 lsm 树只在写操作时触发<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209152230.png" alt="20201209152230" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209152257.png" alt="20201209152257" loading="lazy"></p>
</li>
</ul>
<h5 id="related-work">Related Work</h5>
<p><strong>Software Optimizations of Compactions</strong></p>
<ul>
<li>FAST13 的 VT-tree 使用拼接技术来避免排序和不重叠的键范围的不必要的磁盘 I/Os，但是这种方法受数据分布的影响，可能导致碎片化，使范围扫描和压缩的性能变差。</li>
<li>SIGMOD12 的 bLSM 和 VLDBJ07 的 PE 考虑了数据分布，都是将键范围分渠道多个键的子范围并限制比较热的键范围的数据压缩。</li>
<li>IPDPS14 的 PCP 观察到压缩可以流水线化，使用多个 CPUs 和存储设备来充分利用 CPU 和 I/O 资源从而加速压缩过程。</li>
<li>SIGMOD18 的 Better spacetime trade-offs for lsm-tree based key-value stores via adaptive removal of superfluous merging 通过合并尽可能少的内容来实现查找成本和空间的给定边界，从而提供了更丰富的时空权衡，并提出了一种混合压缩策略(即，对最大级别使用 levelling，对其余级别使用 tiering) 来减少写入放大</li>
<li>SIGMOD19 的 X-engine 提出将 LSM-tree 中的数据分割成小数据块，并在压缩过程中广泛重用键范围不重叠的数据块，以减少写放大。</li>
<li>所有这些软件优化和我们提出的基于 FPGA 的优化是正交的，即可以一起作用来显著提升系统性能并降低能耗。</li>
</ul>
<p><strong>Hardware Accelerations in Databases</strong></p>
<ul>
<li>数据库领域早就在尝试使用各种其他类型的硬件来加速数据的读写。</li>
</ul>
<h4 id="个人见解-6">个人见解</h4>
<ul>
<li>本文还是问题找的比较关键准确，所以后续的提升方案才能带来如此大的提升，而且这个问题也基本是学术界很多学者都已经发现了的问题，只是各自的解决方式不同，但大体思路其实是相通的，即借助其他处理器资源来减少 CPU 的争用问题。不同的是不同层级的优化，如很多做 KVSSD 的研究人员更多是在 SSD 那一层去做这样的优化，本文则更多地是从系统层面来做优化，一个好的点是可能很少人关注到引入新硬件后引入的成本和能耗问题，这个确实云厂商比较关注这方面的问题，所以在评价的角度上能够多出一环。</li>
<li>看完这篇文章的具体感受就是，处理器现在也是百家争鸣，具有各自鲜明特点的硬件单元堆积在板子上可能将会成为一个趋势，即把更专门的运算交给更专业的人去做，而不是像以前 CPU 一锅端。</li>
</ul>
<h3 id="hotring-a-hotspot-aware-in-memory-key-value-store">HotRing: A Hotspot-Aware In-Memory Key-Value Store</h3>
<ul>
<li>本文主要针对的内存 KVs，最主要的应用还是作为 Cache，在大量的互联网场景中都有应用，作为 Cache 需要面对和解决的问题其实就是热点数据的访问。内存中的 KVs 最常见的数据结构就是 HASH，诸如像 Redis 这样的成熟的的内存 KVs，大多都是使用 HASH 来实现。而本文立足的场景也是阿里云淘宝这种并发高的电商场景，需要强劲的缓存支撑，淘宝的 Tair 也算是业界比较知名的数据库团队。</li>
<li>本文主要还是针对热点数据的优化，更多地是从数据结构本身出发，因为相关参考资料也已经讲的很多了，此处主要介绍问题和大致的方案。</li>
<li>贴几个链接：
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/109774040">知乎 - 暗淡了乌云：看了几篇FAST 2020</a></li>
<li><a href="https://www.cnblogs.com/helloworldcode/p/13022166.html">CNBlogs - 晓乎：HotRing: A Hotspot-Aware In-Memory Key-Value Store</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/112928251">知乎 - 阿里技术：性能提升2.58倍！阿里最快KV存储引擎揭秘</a></li>
</ul>
</li>
<li>本文主要探索针对内存索引结构的热点感知的相关设计，作者首先分析了理想的热点感知的索引的潜在收益，并讨论了有效利用热点感知可能面临的挑战（热点变化和并发访问问题）， 基于此分析提出了可以热点感知的 KVs 设计 HotRing，针对一小部分 items 进行大规模并发访问而优化的。HotRing 基于有序的一个有序的环状 HASH 索引结构，通过移动 head 指针来快速访问热点数据。也会应用一个比较轻量的策略来检测运行时的热点变化。HotRing 在设计中采用了无锁的结构，所以并发操作能够更好地利用多核架构的性能。实验表明我们的方法相比于其他内存 KVs 在高度倾斜的负载下实现了 2.58x 的提升。</li>
</ul>
<h4 id="简要介绍-8">简要介绍</h4>
<h5 id="introduction-background-3">Introduction &amp; Background</h5>
<ul>
<li>
<p>常见的内存 KVs，如 Memcached、Redis、SIGMOD18 的 FASTER(HASH)、NSDI13 的 MemC3(HASH)、NSDI14 的 MICA(HASH) 等。热点问题是一个广泛的问题，有一些集群层面上的热点问题的解决方案，如一致性 HASH、数据迁移、前端数据缓存等，同样单节点上的热点问题也需要被解决。比如计算机体系结构利用垂直的存储结构来缓存最常访问的数据在更低延迟的存储介质上。但是对于内存 KVs 内部的热点数据问题往往忽略了。作者从阿里巴巴的生产环境中收集了内存 KVs 的访问情况如图所示，可以发现 <strong>日常分布中的百分之五十和极端分布中的百分之九十的访问都只会访问到总的数据的 1%</strong>，这说明网络时代的热点问题空前严重。（具体的原因主要是在线应用的活跃用户数量持续增长，一些热门事件都会导致在短期内对于少部分数据的大量访问，所以对这些热点数据的快速访问是比较关键的，除此以外这些应用程序之下的基础设施变得复杂，一个小错误，例如由于软件错误或配置错误，可能导致(不可预测的)重复访问一个项，例如无休止地读取和返回一个错误消息。理想的效果是这些不可预测的热点不会导致整个系统崩溃或阻塞）<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201209172247.png" alt="20201209172247" loading="lazy"></p>
</li>
<li>
<p>HASH 索引是最流行的内存 KV 数据结构，特别是针对于一些不需要范围查询的场景。下图展示了 HASH 索引的一种通用结构，bucket + list。HASH 值可以分为两部分，一部分用作 HASH 表的索引，一部分用作 list 种的比对，从而减少比较过程中的需要比较的对象数据的长度。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211203318.png" alt="20201211203318" loading="lazy"></p>
</li>
<li>
<p>现有的内存 HASH 索引无法感知热点数据，即未对热点数据进行区分（如上图中的 热点数据 item3 位于冲突链的末尾，就会比之前多更多的内存访问次数，在一些倾斜负载的情况下热项访问成本的轻微增加可能导致整体性能的严重下降）</p>
</li>
<li>
<p>采用的相同的策略管理所有数据，但是理论分析表明查询热点数据的开销比理想的策略大得多，虽然存在一些减少内存访问的机制，但它们只能提供有限的效率。</p>
<ul>
<li>比如 CPU 利用自己的缓存来加速吗，但是只有 32M 大小，完全不足以缓存热点数据，即便是有一些缓存友好的数据结构</li>
<li>可以扩大哈希表(即通过重新哈希)，以减少冲突链的长度，从而查找一个热项所需的内存访问更少，Rehash 可以帮助减少冲突链的长度，但会显著增加内存占用。特别是当 HASH 表已经很大的时候，就不再推荐重 HASH。例如，对于两个连续的重哈希操作，第二个需要两倍的内存空间，但只带来一半的效率(就减少链长度而言)。</li>
</ul>
</li>
<li>
<p>在本文中，我们提出了 HotRing，这是一种支持热点的内存 KVS，它利用 HASH 索引来优化对一小部分数据项(即热点)的大规模并发访问。最初的想法是让查找一个项所需的内存访问与它的热度负相关，即越热的项读取速度越快。但是需要解决两个问题：</p>
<ul>
<li>hotspot shif：热点项集不断变化，我们需要及时发现并适应这种变化
<ul>
<li><strong>使用 ordered-ring 来解决，当热点变化时，bucket headers 可以直接重新指向热的数据项，而不会影响正确性</strong></li>
<li>同时设计了一个轻量的机制来检测运行时的热点变化</li>
</ul>
</li>
<li>concurrent access：热点本质上是被大量并发请求访问的，我们需要为它们维持高并发性
<ul>
<li><strong>基于现有的无锁数据结构（无锁链表）采用了一种无锁的设计，并扩展该数据结构以支持 HotRing 所需要的其他操作，包括热点变化检测、头指针移动以及有序环 rehash</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="design-3">Design</h5>
<ul>
<li>
<p>HotRing 的 HASH 索引结构如下所示，将以往的传统的 HASH 冲突链表转变为冲突环，原本的链表最后的项将和链表的首项连接起来，当环中只有一项数据时，所有指针都指向自身。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201211211441.png" alt="20201211211441" loading="lazy"></p>
</li>
<li>
<p><strong>Ordered-Ring Hash Index</strong>：环状结构的问题就是没有遍历的终止条件（当对应 bucket 内无对应数据时），就需要一个机制来安全地终止查询过程。很直观的想法就是将 HEAD 指向的第一项标记为停止遍历的标识，但是对于并发请求时会有问题，比如删除了标记的项。所以本文提出了有序环结构来决定查询的过程，使用 Key 和 Tag 进行排序。（先按照 tag 排序，然后按照 key 排序），相比于以往的链式结构，平均查询次数减少到了 (n/2) + 1 次<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201214104313.png" alt="20201214104313" loading="lazy"></p>
</li>
<li>
<p><strong>Hotspot Shift Identification</strong>：此处讨论的都是 bucket 内的热点数据的变化，通常冲突链上有 5-10 个数据项，然后根据热点数据的比例 0.1 到 0.2 推断大约每个 Bucket 内有一个热点数据，可以直接把 HEAD 指针指向唯一的热点数据，从而避免数据的重新组织并减少内存开销。为了获取较好的性能，需要考虑两个因素：</p>
<ul>
<li>热点识别的准确率：热点识别的准确性是通过识别热点所占的比例来衡量的</li>
<li>响应延迟：响应延迟是一个新的热点出现和我们成功检测到它之间的时间跨度。</li>
</ul>
</li>
<li>
<p>基于上述两个因素提出了两种策略： random movement 和 statistical sampling strategy</p>
<ul>
<li><strong>Random Movement</strong>：响应时间短，但是准确率低，其基本思想是，头指针从即时决策周期性地移动到潜在热点，而不记录任何历史元数据。每个线程维护一个 ThreadLocal 变量来记录该线程执行的请求数量，每 R 个请求后，线程决定是否执行头指针的移动操作。如果第 R 次访问是对热数据的访问，那么指针不移动，如果是对冷数据的访问，那么 HEAD 指针指向该冷数据，因为可能即将成为热数据。参数 R 将影响响应延迟和识别的准确率，如果 R 太小，为了实现稳定的性能的响应延迟会很低，但是将导致频繁的且低效的头指针移动。我们的场景中，数据负载是严重倾斜的，因此头指针的移动往往不频繁，根据经验，参数R默认设置为5，该参数可以提供较低的反应延迟和可忽略的性能影响。
<ul>
<li>如果负载的倾斜状况不是很明显，该策略将变得特别低效，最重要的是该策略无法处理一个冲突环中多个热点数据的问题，多个热点数据的时候，头指针频繁地移动，并不会加速对热点数据的访问反而会对正常操作产生不利影响。</li>
</ul>
</li>
<li><strong>Statistical Sampling Strategy</strong>：为了实现更高的性能，我们设计了一种统计采样策略，旨在提供更准确的热点识别与稍高的反应延迟。
<ul>
<li>首先考虑数据格式：Head Pointer 和 Item。将同时记录 Ring 和 Item 级别的统计数据。
<ul>
<li>Head Pointer 由三部分组成：
<ul>
<li>Active bit: 用于控制该策略的 flag</li>
<li>Total Counter: 对该冲突环的访问总次数</li>
<li>Address: 为对应项的物理地址</li>
</ul>
</li>
<li>Item 由以下几部分组成：
<ul>
<li>Rehash：用于控制 rehash 过程的 flag</li>
<li>Occupied：用于保证并发时的准确性</li>
<li>Counter：用于记录该项的访问次数<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201214202717.png" alt="20201214202717" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>Statistical Sampling：如何在这么大的 HASH 表中以一个低开销的方式动态识别出热点数据是一个极具挑战性的问题，关键是尽量减少开销，同时保持精度，这是通过周期采样在HotRing实现的。每个线程维护一个 ThreadLocal 变量用于计数已经处理了的请求数，每 R 个请求之后我们决定是否要开启新一轮的采样，通过改变 Active flag 的值来控制。如果第 R 个访问时热点数据，意味着当前的热点识别仍然是准确的，采样无需触发，如果是冷数据，意味着热点发生了变化，然后我们开始采样。R 默认还是设置为 5，当 Active 位被置为 1，后续对环的访问将被记录在 Total Counter 和对应的 Item 的 Counter 中。采样的策略要求额外的 CAS 操作，可能会导致临时的性能降级。为了减少这个时间，我们将样本的数量与每个环中的项目数量设置相等，我们认为这已经提供了足够的信息来派生新的热点。</li>
<li>Hotspot Adjustment：采样完成后，最后一个访问线程负责频率计算和热点调整。该线程首先使用 CAS 原语 RESET Active BIt，这确保了只有一个线程将执行后续任务。然后，该线程计算环中每个项的访问频率（<strong>用于后续计算 income</strong>），对应项的 Counter 除以该环的 Total Counter，然后计算每个项被头指针指向的 income，即某个项被选中被头指针指向对应的平均内存访问次数。然后选择出 income 最小的项作为热数据从而确保热点最快被访问，头指针的移动也是 CAS 操作。对于多个热点数据的情况，可以计算出相对较优的位置从而避免热点数据之前的频繁移动。调整了热点数据之后，负责的线程重置所有的计数器便于下次采样。</li>
<li>Write-Intensive Hotspot with RCU：对于更新操作，如果 value 小于 8 byte，那么可以就地更新。这个例子中，reading 和 updating 一个项被视作相同热度的操作，但是对于 value 比较大的项的更新操作就完全不同了。RCU（Read-Copy-Write）协议的情况下，前项的指针需要被修改因为在更新过程中指向了新的 item，如果HEAD 指针指向的写密集型热点被修改，整个冲突链将被遍历才能到达前项，也就是说一个写密集型的热数据也会造成他的前项数据变热，基于此我们稍微修改了统计采样策略。对于 RCU 更新，改为只对它的前一项的计数器加一。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201214211131.png" alt="20201214211131" loading="lazy"></li>
<li>Hotspot Inheritance：当对head项执行RCU更新或删除时，我们需要将head指针移动到另一个项。但是，如果头指针随机移动，它可能指向一个冷的数据，这将导致热点识别策略被频繁触发。此外，识别策略的频繁触发会严重影响系统的性能。如果环中就一个数据项，CAS 直接修改头指针来完成更新和删除操作，如果这有很多项，HotRing 使用现有的热点信息（头指针指向的位置）来继承对应的热度。针对RCU的更新和删除操作，我们设计了不同的头指针移动策略，以保证热点调整的有效性：
<ul>
<li>对于head项的RCU更新，由于访问的时间局部性，最近更新的项有很高的被立即访问的概率，因此头指针被移动到 head 的新版本</li>
<li>对于头项的删除，头指针只是移动到下一个项，这是一种简单而有效的解决方案</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Concurrent Operations</strong>：HEAD 可移动导致并发控制更为复杂，但其实主要控制几个关键数据的原子操作即可。如 Next Item Address 后项指针需要使用 CAS 操作来保证原子性。</p>
<ul>
<li>Insert 操作如图 A 所示，需要保证只有一个修改后向指针的操作成功。</li>
<li>Update 小于 8 byte 时无需额外的保证，对于 RCU 操作才需要进行控制。还是图 A 的例子，一个插入 C 的线程需要修改 B 的后向指针，一个线程需要更新 B 的数据为 B‘，这两个操作因为更新的是不同的数据指针其实都会成功，但是因为 B 对于环而言不可见了，即便 C 插入已经成功，所以后续对 B 的操作就会出错（C 的插入更新 B 的后向指针的操作），在图 b 中也有相同的问题，所以使用 Occupied 位来确保正确性，所以使用两个步骤来完成操作。对于 Update&amp;Insert 操作，需要更新后向指针的 B 先把 Occupied 位置原子性第置为 1 ，一旦 Occupied 被置为 1，插入 C 的完整操作将会失败并需要重试。然后更新 A 的后项指针指向 B’，然后将 B‘ 的 Occupied 位重置。</li>
<li>Delete：删除是通过将原本指向了已删除项的指针修改为下一个项来实现的。因此需要保证要删除的项的后向指针在操作过程中不被改变，还是借助 Occupied 位来实现。如图 C 所示</li>
<li>Head Pointer Movement：
<ul>
<li>如何处理正常操作的和热点识别策略引起的头指针移动 形成的并发的情况？ Occupied</li>
<li>如何处理头部指针移动，造成更新或删除头部项目? HotRing不仅需要 occupy 准备删除的项，还需要 occupy 下一个项。因为如果在删除操作期间下一项未被占用，则下一个节点可能已被更改，这使得头指针指向一个无效的项。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201214220309.png" alt="20201214220309" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Lock-free Rehash</strong>：我们在HotRing中提出了一种无锁的重新哈希策略，允许在数据量增加时灵活地重新哈希。传统的重哈希策略是由哈希表的装载因子(即链的平均长度)触发的。但是，这没有考虑热点的影响，因此不适合HotRing。为了使索引适应热点项的增长，HotRing使用访问开销(即检索一个项的平均内存访问次数)来触发重新哈希。</p>
<ul>
<li>Initialization：创建一个后台 HASH 线程，线程通过共享 tag 的最高位来初始化新哈希表，新哈希表的大小是旧哈希表的两倍。如图 a 所示。同时，重哈希线程创建一个由两个子重哈希项组成的重哈希节点，这两个子重哈希项分别对应两个新的头指针。每个重哈希项的格式与数据项相同，只是没有存储有效的KV对。HotRing 通过每个项中的Rehash 位来标识重新散列项。在初始化阶段，两个子重散列项的标记设置不同。如图 b 所示，对应的重散列项分别将标签设置为0和T/2。</li>
<li>Split: 重哈希线程通过向环中插入两个重哈希项来分割环。如图 C 所示，将重新哈希项分别插入到项B和项E的前面，作为标签范围的边界来划分环。当两个插入操作完成时，新建表被激活。之后，依次访问新表需要通过比较 tag 选择相应的头指针，而以前访问(从旧的表）通过 tag 重新散列节点继续。可以在不影响并发读和写的情况下正确地访问所有数据。到目前为止，对项的访问在逻辑上被划分为两条路径。</li>
<li>Deletion: 如图 d 所示，在此之前，重散列线程必须维护一个过渡期，以确保从旧表发起的所有访问都已经完成，比如read-copy-update同步原语的过渡期。当所有访问结束时，重新哈希线程可以安全地删除旧表，然后重新哈希节点。注意，过渡期只阻塞重散列线程，而不阻塞访问线程。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201214222311.png" alt="20201214222311" loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="个人见解-7">个人见解</h4>
<ul>
<li>本文还是问题找的比较关键准确，当然主要是因为淘宝有对应的真实场景，冷热数据的访问影响极大，但相较于以往针对缓存算法中冷热数据的识别，本文将问题更多地聚焦在 HASH 冲突链内，所以方向性也就比较明确，个人觉得可以在缩小问题范围之前加一个测试来说明 HASH 冲突链内的热点问题的严重性，来进一步证明把这个大问题缩小范围的有效性和可行性。</li>
<li>本文设计的数据结构还是挺巧妙的，乍一看链变环仿佛没啥新东西，但其中蕴藏了很多有意思的问题，尤其是并发场景下的一些数据访问的问题，你使用了有序环的结构减少内存访问次数，但也就需要见招拆招地解决环结构本身的问题，对于并发访问的场景考虑的也很周全。</li>
</ul>
<h2 id="caching">Caching</h2>
<ul>
<li>BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server
<ul>
<li>Shucheng Wang, Ziyi Lu, and Qiang Cao, Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System; Hong Jiang, Department of Computer Science and Engineering, University of Texas at Arlington; Jie Yao, School of Computer Science and Technology, Huazhong University of Science and Technology; Yuanyuan Dong and Puyuan Yang, Alibaba Group</li>
</ul>
</li>
</ul>
<h3 id="bcw-buffer-controlled-writes-to-hdds-for-ssd-hdd-hybrid-storage-server">BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</h3>
<ul>
<li>
<p>本文是隔壁实验室大佬发的，和 Alibaba 也有一些合作（盘古），有一些生产环境中的实际问题和数据可以借鉴。发现的问题也算是比较关键。</p>
</li>
<li>
<p>贴几个链接：</p>
<ul>
<li><a href="https://developer.aliyun.com/article/758338">FAST20 论文学习：Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</a></li>
<li><a href="https://nbjl.nankai.edu.cn/2020/0308/c12124a266896/page.htm">NBJL 2020论文导读10：BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Servere</a></li>
</ul>
</li>
<li>
<p><strong>场景</strong>：本文主要针对混合存储（Hybrid Storage）场景，SSD 作为缓存为 HDD 加速。HDD 因为容量大、成本低在数据中心还是有着很多的应用，特别是对一些冷数据的存储，但对于当下对于存储设备的性能需求，HDD 又无法提供用户期待的性能，所以大多数公司会使用性能更好的 SSD 来为 HDD 加速，在保证大容量低成本的同时提供一定的性能保证。</p>
</li>
<li>
<p><strong>问题</strong>：在阿里的生产环境中发现（Pangu storage nodes A (Cloud Computing), B (Cloud Storage), C and D (Structured Storage)），这种混合存储的场景下有一个很严重的问题，就是对于 HDD 和 SSD 的使用严重不均衡，即对 SSD 过度使用，而 HDD 的利用率较低，如果负载是一些写密集型的负载，那么 SSD 磨损的情况更严重，而且因为 SSD 本身的机制的原因，写密集将导致频繁的 GC 从而增大 SSD 响应的尾延迟。</p>
<ul>
<li>写入的数据量极大，每个 SSD 每天写入的数据有接近 3TB，接近于 DWPD (Drive Writes Per Day)，严重影响 SSD 的可靠性（相当于每天满负荷运行） https://www.kingston.com/cn/ssd/dwpd</li>
<li>有很多突发 IO，SSD 需要处理这种突发的写密集型负载</li>
<li>写入到 SSD 和 HDD 的数据量差别较大，利用率有 10 个百分点的差距，而且 HDD 大多是转储 SSD 的数据，基本不直接服务用户请求</li>
<li>存在长尾延迟，因为队列阻塞。主要原因有两类：单个 IO 操作较大 1MB；频繁的垃圾回收</li>
<li>小 IO 占据了所有 IO 的很大一部分<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201215104026.png" alt="20201215104026" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>以往的方案</strong>：以往解决上述问题的方案主要有两类，一类则简单粗暴直接增加 SSD，但成本开销巨大；另一类则是利用相对空闲的 HDD 来吸收部分写，也就是所谓的 SSD 写重定向 SSD-Write-Redirect (SWR)  SoCC 2019，该方案可以一定程度上解决 SSD 队列的阻塞问题，HDD 上的延迟比 SSD 高 3-12 倍，对于大多数需要微妙级延迟的小型写操作来说，这即使不是不可接受的，也显然是不可取的。所以方案目标还是想降低 HDD 的延迟到 SSD 级别。</p>
</li>
<li>
<p><strong>发现</strong>：除了上述问题以外，作者还有对于 HDD 新的发现，一系列连续的、顺序的对 HDD 的写操作呈现出周期性的、阶梯状的写延迟模式（低、中、高），原因主要是因为 HDD 控制器将写操作给缓冲了。这个发现也就意味着 HDD 本身是可以提供微妙级的 IO 写延迟的（如果进行了适当的写操作调度），这时候和 SSD 的写性能相近。这些观察结果促使作者有效地利用 HDD 的这种性能潜力来吸收尽可能多的写操作，从而避免 SSD 的过度使用而不会降低性能。</p>
<ul>
<li>如图所示，无论是多大的 IO 操作，HDD 都表现出了低中高三种延迟模式，且具有很强的周期性，前两种可以视作微秒级的响应，因为写操作一旦被写到内置的控制器 buffer 中就已经认为该写操作完成了。</li>
<li>但是当写缓冲满了之后，host 写操作不得不被阻塞直到缓冲的数据被 flush 到磁盘，造成较慢的写操作，这一发现启发我们充分利用 HDD 的缓冲写提供的性能潜力，在 SSD 上提高性能的同时减少写损失。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201215101124.png" alt="20201215101124" loading="lazy"></li>
<li>测试了不同品牌不同容量的 HDD，发现都有相同的规律，至于为什么有这样的规律，主要是因为 HDD 中有内建的 DRAM，但是 DRAM 中只有很少的一部分容量会被用于缓冲写 IO，剩下的大部分容量主要用于 read-ahead cache 预读缓存、ECC buffer、sector remapping buffer、prefetching buffer，然而，在 HDD 中使用此内置 DRAM 的具体策略(随HDD模型的不同而不同)通常仅为 HDD 制造商专有。幸运的是，写缓冲区的实际大小可以通过分析从外部测量。</li>
<li>成功缓冲写操作后，HDD会立即通知主机请求完成。当缓冲的数据到达阈值之后，HDD 将强制执行刷回，这期间将阻塞后续的写入直到 buffer 被释放。值得注意的是，在空闲一段时间后，由于将数据刷新到磁盘，HDD 缓冲区可能隐式变为空。但是，要显式清空缓冲区，我们可以主动调用 <code>sync()</code> 来强制刷新。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201215115010.png" alt="20201215115010" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>贡献</strong>：本文基于上述问题和发现，对 HDD 的性能表现进行了建模，提出了一个预测模型来准确地决定下一个写延迟状态（因为 HDD 内置的控制器 Buffer 对于 HOST 而言是完全不可见的，主机只能根据当前的延迟状况判断状态，利用缓冲写周期和当前写状态信息，可以实现对下一次写状态的预测）。基于这个模型，提出了一种写方法， Buffer-Controlled Write approach, BCW 来主动和有效地控制缓冲写，所以在 HDD 的低延迟、中延迟阶段调度数据的写操作，而在高延迟阶段使用填充数据。基于 BCW，设计了一个混合的 IO 调度器（MIOS）来根据写模式、运行时队列长度和磁盘状态自适应地将传入数据引导到 SSD 和 HDD。在真实生产环境下的负载和 Benchmarks 都表明 MIOS 移除了写到 SSD 数据的 93%，减少了 65% 平均延迟和 85% 尾延迟。</p>
</li>
</ul>
<h4 id="design-4">Design</h4>
<h5 id="the-hdd-buffered-write-model">The HDD Buffered-Write Model</h5>
<ul>
<li>建模如下，每个时间序列都以 Sync 操作开始，F 表示可以完全缓冲写入到 HDD Buffer 的阶段，M 则是该 Buffer 越来越接近满的阶段，S 则是 Buffer 已满后续的写入都会被阻塞的阶段。
<ul>
<li>Fast stage lasts for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">W_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> data written</li>
<li>Mid stage	lasts	for	<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">W_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> data	written<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216105852.png" alt="20201216105852" loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="write-state-predictor">Write-state Predictor</h5>
<ul>
<li>F/A : The current write state is F and the buffer is available. Next write state is most likely to be F.</li>
<li>F/U : Although the current write state is F, the buffer is unavailable. Next write state is likely to change to S.</li>
<li>M/A : The current write state is M and the buffer is available. Next write state is most likely to remain M.</li>
<li>M/U : Although the current write state is M, the buffer is unavailable. Next write state should be S.</li>
<li>S : The current write state is S. Next write state will be M with a high probability.</li>
<li>The Sync operation will force the next write state and buffer state back to be F/A in all cases<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216111112.png" alt="20201216111112" loading="lazy"></li>
<li>通过监视 IO 请求大小和延迟，并计算写缓冲区中的空闲空间确定了当前的写状态，F，M 或 S，也就是说，记录当前写状态(F 或 M)中的 ADW，并与 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">W_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">W_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 进行比较，以预测下一个写状态。</li>
<li>关于预测准确度的问题，实验表明对于 S 状态的预测相对较低，S 状态的低预测精度是由于当实际的 S 状态被错误地预测为另一种状态时，预测策略倾向于 S 状态以减少性能下降。</li>
</ul>
<h5 id="buffer-controlled-writes">Buffer-Controlled Writes</h5>
<ul>
<li>缓冲区控制写(BCW)是一种 HDD 写方法，它确保用户使用 F 或 M 写状态进行写操作，并避免分配慢写操作。BCW 的核心思想是使缓冲写可控。</li>
<li>激活 BCW 后，将调用 sync() 操作来强制同步以主动清空缓冲区，如果预测处于 F 或 M 状态，BCW 将向 HDD 发送连续的用户写操作，否则，将非用户数据填充到 HDD，直到达到缓冲写的最大设置循环(或无限)序列。如果队列中有用户请求，BCW 按顺序写入它们。写完成后，BCW将它的写大小添加到 ADW，并相应地更新写状态。</li>
<li>在请求稀少的轻量级或空闲工作负载期间，HDD 请求队列将时不时为空，使写流不连续。为了确保按顺序和连续模式缓冲写操作的稳定性和确定性，BCW 将主动地将非用户数据填充到磁盘上写入。填充数据有两种类型：（即使对于每个填充写，BCW 仍然执行写状态预测算法）
<ul>
<li>PF：用于用 4KB 的非用户数据填充 F 和 M 状态，小的 PF 可以尽量减少用户请求的等待时间</li>
<li>PS：用更大的块大小填充 S 状态。例如 64KB 的非用户数据，大的 PS 帮助更快地触发慢写操作</li>
</ul>
</li>
<li>BCW 持续计算当前状态( F 或 M )的 ADW。当 ADW 接近 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">W_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">W_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 时，意味着硬盘驱动器缓冲写处于快速或中期阶段的末尾。S 写状态可能在多次写之后发生。此时，BCW 会通知调度程序，并使用 PS 主动触发慢写。为了避免用户写操作的长延迟，在这个阶段，所有传入的用户请求都必须被引导到其他存储设备，比如 SSD。S 状态的写完成之后，下一个写操作将为 M，然后 BCW 重新设置 ADW 并再次接受用户请求。</li>
<li>作者发现在 F 状态下 ADW 达到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">W_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 之前没有必要主动地填充写操作。因为这时候物理磁盘操作尚未触发，缓冲区可以在此期间吸收用户请求，短时间之后达到该阈值，这意味着缓冲区将开始将数据刷新到磁盘，下一个写状态将更改为 S。另一方面，当 ADW 长时间小于 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">W_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 时，磁盘可以自动刷新缓冲过的数据，以便下一个写状态可能是 F。但是，它不会影响性能。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216155537.png" alt="20201216155537" loading="lazy"></li>
</ul>
<h5 id="mixed-io-scheduler">Mixed IO scheduler</h5>
<ul>
<li>调度器根据写状态预测器的结果和当前队列状态决定是否将用户写操作引导到 HDD 请求队列。</li>
<li>架构如下所示，MIOS 在运行时监控 SSD 和 HDD 的所有请求队列，明智地触发 BCW 进程，并确定是否应该将用户写操作定向到所选的 HDD 或 SSD。MIOS 在配置过程中在每个 HDD 中创建一个设备文件。设备文件以仅追加的方式存储BCW 写入。在 MIOS 调度之前，执行概要分析来确定写状态预测器的关键参数(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">W_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">W_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>等)<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216160553.png" alt="20201216160553" loading="lazy"></li>
<li><strong>调度策略</strong>：SSD 在 t 时刻的请求队列长度 l(t) 是关键参数，当大于预定义的阈值 L，调度器通过预测用户的写状态是 F 或 M 来引导用户写到 HDD。阈值 L 是根据 SSD 上的实际性能测量值预先确定的。设定的标准是，我们测量不同 SSD 队列长度下的写延迟，如果队列长度为 l 的请求延迟大于在 M 状态下，我们只需设置阈值 L 为最小的 l 即可。其基本原理是，当 SSD 队列长度大于 L 时，SSD 写操作的延迟将与 BCW 的 F 或 M 写状态下 HDD 上的延迟相同。L 可以在运行时根据工作负载行为和存储设备配置实验性地确定和调整。此策略虽然不能避免，但可以缓解工作负载激增或繁重时的长尾延迟以及 SSD 上的垃圾回收，在这些情况下，SSD 请求队列长度可能是其平均长度的 8-10 倍。因此，重定向的 HDD 写操作不仅减轻了突发请求和重 gc 带来的 SSD 压力，抑制了长尾延迟，而且还降低了平均延迟。</li>
<li>另外，当SSD的队列长度小于 L 时，可选触发 BCW。在这种情况下，启用或禁用 BCW 分别表示为 MIOS_E 或MIOS_D。换句话说，当 SSD 的队列长度小于 L 时，MIOS_E 策略允许使用 BCW 重定向。MIOS_D 策略相反地可以在 SSD 队列长度小于 L 时关闭重定向。注意，在 M 写状态下的 HDD 的写延迟仍然比 SSD 的要高得多。重定向后的请求延迟可能会增加。因此，当 l(t) 小于 l 时，我们只重定向用户请求以利用 MIOS_E 中 HDD 的 F 写状态。</li>
<li>通常，一个典型的混合存储节点包含多个 SSD 和 HDD。我们把所有的磁盘分成独立的 SSD/HDD 对，每个对包含一个SSD 和一个或多个 HDD。每个 SSD/HDD 对都由一个独立的 MIOS 调度程序实例管理。</li>
<li>最后，MIOS 需要对 HDD 的完全控制。这意味着 BCW 中的 HDD 不会受到其他 IO 操作的干扰。当一个 HDD 正在执行 BCW 并且一个读请求到达时，MIOS 立即挂起 BCW 并提供这个读。此时，它将尝试将所有写操作重定向到其他空闲磁盘。对于以读为主的工作负载，可以禁用 BCW 以避免干扰读操作。</li>
</ul>
<h4 id="个人见解-8">个人见解</h4>
<ul>
<li>SSD/HDD 混合存储的场景其实在工业界的应用极为广泛，但仿佛很少有人关注这种场景下存在的问题，即便是考虑到对于 SSD 的磨损也大多是通过对单一的 SSD 层面上进行优化来减少磨损，而从整个混合存储系统的角度出发，屏蔽器件内部的技术细节，还是在 HDD/SSD 的写调度上发力来减少对 SSD 的消耗，当然关键主要还是对于 HDD 的测试发现了 HDD 本身的性能变化的规律，还是比较巧妙的。</li>
<li>本文的设计与实现其实在对于 HDD 的测试之后就已经呼之欲出了，但作者还是设计了很多细节上的东西来提供了更多实现方面的细节，包括一些具体调度策略的伪代码，使得该方案在工业界落地也成为可能，</li>
</ul>
<h2 id="consistency-and-reliability">Consistency and Reliability</h2>
<ul>
<li>
<p>CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost</p>
<ul>
<li>Zizhong Wang, Tongliang Li, Haixia Wang, Airan Shao, Yunren Bai, Shangming Cai, Zihan Xu, and Dongsheng Wang, Tsinghua University</li>
</ul>
</li>
<li>
<p>Hybrid Data Reliability for Emerging Key-Value Storage Devices</p>
<ul>
<li>Rekha Pitchumani and Yang-suk Kee, Memory Solutions Lab, Samsung Semiconductor Inc.</li>
</ul>
</li>
<li>
<p>Strong and Efficient Consistency with Consistency-Aware Durability</p>
<ul>
<li>Aishwarya Ganesan, Ramnatthan Alagappan, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau, University of Wisconsin–Madison</li>
<li><strong>Awarded Best Paper!</strong></li>
</ul>
</li>
</ul>
<h3 id="craft-an-erasure-coding-supported-version-of-raft-for-reducing-storage-cost-and-network-cost">CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost</h3>
<ul>
<li>因为很久没单开分布式的文章的坑了，所以干脆拿这篇作为一个 Motivation，然后把 Raft、Paxos 也总结一下。虽然这篇文章跟 EC 有很大关系，但是更多的还是解决一致性的一些问题吧。</li>
<li>完成之后此处贴链接。</li>
</ul>
]]></content>
    </entry>
</feed>