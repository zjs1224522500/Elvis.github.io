<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.shunzi.tech</id>
    <title>Elvis Zhang</title>
    <updated>2021-04-06T07:52:10.265Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.shunzi.tech"/>
    <link rel="self" href="https://blog.shunzi.tech/atom.xml"/>
    <subtitle>The easy way or the right way.</subtitle>
    <logo>https://blog.shunzi.tech/images/avatar.png</logo>
    <icon>https://blog.shunzi.tech/favicon.ico</icon>
    <rights>All rights reserved 2021, Elvis Zhang</rights>
    <entry>
        <title type="html"><![CDATA[Series Three of Basic of Concurrency - Condition Variables]]></title>
        <id>https://blog.shunzi.tech/post/basic-of-concurrency-one/</id>
        <link href="https://blog.shunzi.tech/post/basic-of-concurrency-one/">
        </link>
        <updated>2021-04-05T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第三篇（Condition Variables），条件变量。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第三篇（Condition Variables），条件变量。</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="chapter-index">Chapter Index</h2>
<ul>
<li><a href="">Series One of Basic of Concurrency - Concurrency and Threads</a></li>
<li><a href="../lock/">Series Two of Basic of Concurrency - Lock</a></li>
<li><a href="">Series Three of Basic of Concurrency - Condition Variables</a></li>
<li><a href="">Series Four of Basic of Concurrency - Semaphores</a></li>
<li><a href="">Series Five of Basic of Concurrency - Bugs and Event-Based Concurrency</a></li>
</ul>
<h2 id="condition-variables">Condition Variables</h2>
<ul>
<li>到目前为止，我们已经形成了锁的概念，看到了如何通过硬件和操作系统支持的正确组合来实现锁。然而，锁并不是并发程序设计所需的唯一原语。</li>
<li>具体来说，在很多情况下，线程需要检查某一条件（condition）满足之后，才会继续运行。例如，父线程需要检查子线程是否执行完毕 [这常被称为 join()]。这种等待如何实现呢？我们来看如下所示的代码。</li>
</ul>
<pre><code class="language-C">1 void *child(void *arg) {
2   printf(&quot;child\n&quot;);
3   // XXX how to indicate we are done?
4   return NULL;
5 }
6
7 int main(int argc, char *argv[]) {
8   printf(&quot;parent: begin\n&quot;);
9   pthread_t c;
10  Pthread_create(&amp;c, NULL, child, NULL); // create child
11  // XXX how to wait for child?
12  printf(&quot;parent: end\n&quot;);
13  return 0;
14 }
</code></pre>
<ul>
<li>我们期望能看到这样的输出：</li>
</ul>
<pre><code>parent: begin
child
parent: end 
</code></pre>
<ul>
<li>我们可以尝试用一个共享变量，如下所示。这种解决方案一般能工作，但是效率低下，因为主线程会自旋检查，浪费 CPU 时间。我们希望有某种方式让父线程休眠，直到等待的条件满足（即子线程完成执行）。</li>
</ul>
<pre><code class="language-C">1 volatile int done = 0;
2
3 void *child(void *arg) {
4   printf(&quot;child\n&quot;);
5   done = 1;
6   return NULL;
7 }
8
9 int main(int argc, char *argv[]) {
10  printf(&quot;parent: begin\n&quot;);
11  pthread_t c;
12  Pthread_create(&amp;c, NULL, child, NULL); // create child
13  while (done == 0)
14      ; // spin
15  printf(&quot;parent: end\n&quot;);
16  return 0;
17 }
</code></pre>
<ul>
<li><strong>CRUX: 多线程程序中，一个线程等待某些条件是很常见的。简单的方案是自旋直到条件满足，这是极其低效的，某些情况下甚至是错误的。那么，线程应该如何等待一个条件？</strong></li>
</ul>
<h3 id="definition-and-routines">Definition and Routines</h3>
<ul>
<li>线程可以使用条件变量（condition variable），来等待一个条件变成真。条件变量是一个显式队列，当某些执行状态（即条件，condition）不满足时，线程可以把自己加入队列，等待（waiting）该条件。另外某个线程，当它改变了上述状态时，就可以唤醒一个或者多个等待线程（通过在该条件上发信号），让它们继续执行。Dijkstra 最早在“私有信号量”中提出这种思想。Hoare 后来在关于观察者的工作中，将类似的思想称为条件变量。</li>
<li>要声明这样的条件变量，只要像这样写：pthread_cond_t c;，这里声明 c 是一个条件变量（注意：还需要适当的初始化）。条件变量有两种相关操作：wait() 和 signal()。线程要睡眠的时候，调用 wait()。当线程想唤醒等待在某个条件变量上的睡眠线程时，调用 signal()。具体来说，POSIX 调用如下所示。</li>
</ul>
<pre><code class="language-C">pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);
pthread_cond_signal(pthread_cond_t *c);
</code></pre>
<ul>
<li>我们常简称为 wait()和 signal()。你可能注意到一点，wait()调用有一个参数，它是互斥量。它假定在 wait()调用时，这个互斥量是已上锁状态。wait()的职责是释放锁，并让调用线程休眠（原子地）。当线程被唤醒时（在另外某个线程发信号给它后），它必须重新获取锁，再返回调用者。这样复杂的步骤也是为了避免在线程陷入休眠时，产生一些竞态条件。我们观察一下如下所示代码中 join 问题的解决方法，以加深理解。</li>
</ul>
<pre><code class="language-C">1 int done = 0;
2 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
3 pthread_cond_t c = PTHREAD_COND_INITIALIZER;
4
5 void thr_exit() {
6   Pthread_mutex_lock(&amp;m);
7   done = 1;
8   Pthread_cond_signal(&amp;c);
9   Pthread_mutex_unlock(&amp;m);
10 }
11
12 void *child(void *arg) {
13  printf(&quot;child\n&quot;);
14  thr_exit();
15  return NULL;
16 }
17
18 void thr_join() {
19  Pthread_mutex_lock(&amp;m);
20  while (done == 0)
21      Pthread_cond_wait(&amp;c, &amp;m);
22  Pthread_mutex_unlock(&amp;m);
23 }
24
25 int main(int argc, char *argv[]) {
26  printf(&quot;parent: begin\n&quot;);
27  pthread_t p;
28  Pthread_create(&amp;p, NULL, child, NULL);
29  thr_join();
30  printf(&quot;parent: end\n&quot;);
31  return 0;
32 }
</code></pre>
<ul>
<li>有两种情况需要考虑。
<ul>
<li>第一种情况是父线程创建出子线程，但自己继续运行（假设只有一个处理器），然后马上调用 thr_join()等待子线程。在这种情况下，它会先获取锁，检查子进程是否完成（还没有完成），然后调用 wait()，让自己休眠。子线程最终得以运行，打印出“child”，并调用 thr_exit()函数唤醒父进程，这段代码会在获得锁后设置状态变量 done，然后向父线程发信号唤醒它。最后，父线程会运行（从 wait()调用返回并持有锁），释放锁，打印出“parent:end”。</li>
<li>第二种情况是，子线程在创建后，立刻运行，设置变量 done 为 1，调用 signal 函数唤醒其他线程（这里没有其他线程），然后结束。父线程运行后，调用 thr_join()时，发现 done 已经是 1 了，就直接返回。</li>
</ul>
</li>
<li>最后一点说明：你可能看到父线程使用了一个 while 循环，而不是 if 语句来判断是否需要等待。虽然从逻辑上来说没有必要使用循环语句，但这样做总是好的（后面我们会加以说明）。</li>
<li>为了确保理解 thr_exit()和 thr_join()中每个部分的重要性，我们来看一些其他的实现。首先，你可能会怀疑状态变量 done 是否需要。代码像下面这样如何？正确吗？</li>
</ul>
<pre><code class="language-C">1 void thr_exit() {
2   Pthread_mutex_lock(&amp;m);
3   Pthread_cond_signal(&amp;c);
4   Pthread_mutex_unlock(&amp;m);
5 }
6
7 void thr_join() {
8   Pthread_mutex_lock(&amp;m);
9   Pthread_cond_wait(&amp;c, &amp;m);
10  Pthread_mutex_unlock(&amp;m);
11 }

</code></pre>
<ul>
<li>这段代码是有问题的。假设子线程立刻运行，并且调用 thr_exit()。在这种情况下，子线程发送信号，但此时却没有在条件变量上睡眠等待的线程。父线程运行时，就会调用 wait 并卡在那里，没有其他线程会唤醒它。通过这个例子，你应该认识到变量 done 的重要性，它记录了线程有兴趣知道的值。睡眠、唤醒和锁都离不开它。</li>
<li>下面是另一个糟糕的实现。在这个例子中，我们假设线程在发信号和等待时都不加锁。会发生什么问题？想想看！</li>
</ul>
<pre><code class="language-C">1 void thr_exit() {
2   done = 1;
3   Pthread_cond_signal(&amp;c);
4 }
5
6 void thr_join() {
7   if (done == 0)
8       Pthread_cond_wait(&amp;c);
9 }

</code></pre>
<ul>
<li>这里的问题是一个微妙的竞态条件。具体来说，如果父进程调用 thr_join()，然后检查完 done 的值为 0，然后试图睡眠。但在调用 wait 进入睡眠之前，父进程被中断。子线程修改变量 done 为 1，发出信号，同样没有等待线程。父线程再次运行时，就会长眠不醒，这就惨了。</li>
</ul>
<blockquote>
<ul>
<li><strong>提示：发信号时总是持有锁</strong></li>
<li>尽管并不是所有情况下都严格需要，但有效且简单的做法，还是在使用条件变量发送信号时持有锁。虽然上面的例子是必须加锁的情况，但也有一些情况可以不加锁，而这可能是你应该避免的。因此，为了简单，请在调用 signal 时持有锁（hold the lock when calling signal）。</li>
<li>这个提示的反面，即调用 wait 时持有锁，不只是建议，而是 wait 的语义强制要求的。因为 wait 调用总是假设你调用它时已经持有锁、调用者睡眠之前会释放锁以及返回前重新持有锁。因此，这个提示的一般化形式是正确的：调用 signal 和 wait 时要持有锁（hold the lock when calling signal or wait），你会保持身心健康的。</li>
</ul>
</blockquote>
<ul>
<li>希望通过这个简单的 join 示例，你可以看到使用条件变量的一些基本要求。为了确保你能理解，我们现在来看一个更复杂的例子：生产者/消费者（producer/consumer）或有界缓冲区（bounded-buffer）问题。</li>
</ul>
<h3 id="the-producerconsumer-bounded-buffer-problem">The Producer/Consumer (Bounded Buffer) Problem</h3>
<ul>
<li>本章要面对的下一个问题，是生产者/消费者（producer/consumer）问题，也叫作有界缓冲区（bounded buffer）问题。这一问题最早由 Dijkstra 提出。实际上也正是通过研究这一问题，Dijkstra 和他的同事发明了通用的信号量（它可用作锁或条件变量）。</li>
<li>假设有一个或多个生产者线程和一个或多个消费者线程。生产者把生成的数据项放入缓冲区；消费者从缓冲区取走数据项，以某种方式消费。很多实际的系统中都会有这种场景。例如，在多线程的网络服务器中，一个生产者将 HTTP 请求放入工作队列（即有界缓冲区），消费线程从队列中取走请求并处理。</li>
<li>我们在使用管道连接不同程序的输出和输入时，也会使用有界缓冲区，例如 grep foo file.txt | wc -l。这个例子并发执行了两个进程，grep 进程从 file.txt 中查找包括“foo”的行，写到标准输出；UNIX shell 把输出重定向到管道（通过 pipe 系统调用创建）。管道的另一端是 wc 进程的标准输入，wc 统计完行数后打印出结果。因此，grep 进程是生产者，wc 是进程是消费者，它们之间是内核中的有界缓冲区，而你在这个例子里只是一个开心的用户。</li>
<li>因为有界缓冲区是共享资源，所以我们必须通过同步机制来访问它，以免产生竞态条件。为了更好地理解这个问题，我们来看一些实际的代码。- 首先需要一个共享缓冲区，让生产者放入数据，消费者取出数据。简单起见，我们就拿一个整数来做缓冲区（你当然可以想到用一个指向数据结构的指针来代替），两个内部函数将值放入缓冲区，从缓冲区取值。</li>
</ul>
<pre><code class="language-C">1 int buffer;
2 int count = 0; // initially, empty
3
4 void put(int value) {
5   assert(count == 0);
6   count = 1;
7   buffer = value;
8 }
9
10 int get() {
11  assert(count == 1);
12  count = 0;
13  return buffer;
14 }

</code></pre>
<ul>
<li>很简单，不是吗？put()函数会假设缓冲区是空的，把一个值存在缓冲区，然后把 count 设置为 1 表示缓冲区满了。get()函数刚好相反，把缓冲区清空后（即将 count 设置为 0），并返回该值。不用担心这个共享缓冲区只能存储一条数据，稍后我们会一般化，用队列保存更多数据项，这会比听起来更有趣。</li>
<li>现在我们需要编写一些函数，知道何时可以访问缓冲区，以便将数据放入缓冲区或从缓冲区取出数据。条件是显而易见的：仅在 count 为 0 时（即缓冲器为空时），才将数据放入缓冲器中。仅在计数为 1 时（即缓冲器已满时），才从缓冲器获得数据。如果我们编写同步代码，让生产者将数据放入已满的缓冲区，或消费者从空的数据获取数据，就做错了（在这段代码中，断言将触发）。</li>
<li>这项工作将由两种类型的线程完成，其中一类我们称之为生产者（producer）线程，另<br>
一类我们称之为消费者（consumer）线程。下面展示了一个生产者的代码，它将一个整<br>
数放入共享缓冲区 loops 次，以及一个消费者，它从该共享缓冲区中获取数据（永远不停），每次打印出从共享缓冲区中提取的数据项。</li>
</ul>
<pre><code class="language-C">1 void *producer(void *arg) {
2   int i;
3   int loops = (int) arg;
4   for (i = 0; i &lt; loops; i++) {
5       put(i);
6   }
7 }
8
9 void *consumer(void *arg) {
10  while (1) {
11      int tmp = get();
12      printf(&quot;%d\n&quot;, tmp);
13  }
14 }
</code></pre>
<h4 id="a-broken-solution">A Broken Solution</h4>
<ul>
<li>假设只有一个生产者和一个消费者。显然，put()和 get()函数之中会有临界区，因为 put() 更新缓冲区，get()读取缓冲区。但是，给代码加锁没有用，我们还需别的东西。不奇怪，别的东西就是某些条件变量。在这个（有问题的）首次尝试中，我们用了条件变量 cond 和相关的锁 mutex。</li>
</ul>
<pre><code class="language-C">1 int loops; // must initialize somewhere...
2 cond_t cond;
3 mutex_t mutex;
4
5 void *producer(void *arg) {
6   int i;
7   for (i = 0; i &lt; loops; i++) {
8       Pthread_mutex_lock(&amp;mutex); // p1
9       if (count == 1) // p2
10          Pthread_cond_wait(&amp;cond, &amp;mutex); // p3
11      put(i); // p4
12      Pthread_cond_signal(&amp;cond); // p5
13      Pthread_mutex_unlock(&amp;mutex); // p6
14  }
15 }
16
17 void *consumer(void *arg) {
18  int i;
19  for (i = 0; i &lt; loops; i++) {
20      Pthread_mutex_lock(&amp;mutex); // c1
21      if (count == 0) // c2
22          Pthread_cond_wait(&amp;cond, &amp;mutex); // c3
23      int tmp = get(); // c4
24      Pthread_cond_signal(&amp;cond); // c5
25      Pthread_mutex_unlock(&amp;mutex); // c6
26      printf(&quot;%d\n&quot;, tmp);
27  }
28 }

</code></pre>
<ul>
<li>来看看生产者和消费者之间的信号逻辑。当生产者想要填充缓冲区时，它等待缓冲区变空（p1～p3）。消费者具有完全相同的逻辑，但等待不同的条件——变满（c1～c3）。当只有一个生产者和一个消费者时，上图中的代码能够正常运行。但如果有超过一个线程（例如两个消费者），这个方案会有两个严重的问题。哪两个问题？</li>
<li>……（暂停思考一下）……</li>
<li>我们来理解第一个问题，它与等待之前的 if 语句有关。假设有两个消费者（Tc1 和 Tc2），一个生产者（Tp）。首先，一个消费者（Tc1）先开始执行，它获得锁（c1），检查缓冲区是否可以消费（c2），然后等待（c3）（这会释放锁）。</li>
<li>接着生产者（Tp）运行。它获取锁（p1），检查缓冲区是否满（p2），发现没满就给缓冲区加入一个数字（p4）。然后生产者发出信号，说缓冲区已满（p5）。关键的是，这让第一个消费者（Tc1）不再睡在条件变量上，进入就绪队列。Tc1 现在可以运行（但还未运行）。生产者继续执行，直到发现缓冲区满后睡眠（p6,p1-p3）。</li>
<li>这时问题发生了：另一个消费者（Tc2）抢先执行，消费了缓冲区中的值（c1,c2,c4,c5,c6，跳过了 c3 的等待，因为缓冲区是满的）。现在假设 Tc1 运行，在从 wait 返回之前，它获取了锁，然后返回。然后它调用了 get() (p4)，但缓冲区已无法消费！断言触发，代码不能像预期那样工作。显然，我们应该设法阻止 Tc1 去消费，因为 Tc2 插进来，消费了缓冲区中之前生产的一个值。下表展示了每个线程的动作，以及它的调度程序状态（就绪、运行、睡眠）随时间的变化。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405165844.png" alt="20210405165844" loading="lazy"></li>
<li>问题产生的原因很简单：在 Tc1 被生产者唤醒后，但在它运行之前，缓冲区的状态改变了（由于 Tc2）。发信号给线程只是唤醒它们，暗示状态发生了变化（在这个例子中，就是值已被放入缓冲区），但并不会保证在它运行之前状态一直是期望的情况。信号的这种释义常称为Mesa 语义（Mesa semantic），为了纪念以这种方式建立条件变量的首次研究。另一种释义是 Hoare 语义（Hoare semantic），虽然实现难度大，但是会保证被唤醒线程立刻执行。实际上，几乎所有系统都采用了 Mesa 语义。</li>
</ul>
<h4 id="better-but-still-broken-while-not-if">Better, But Still Broken: While, Not If</h4>
<ul>
<li>幸运的是，修复这个问题很简单：把 if 语句改为 while。当消费者 Tc1 被唤醒后，立刻再次检查共享变量（c2）。如果缓冲区此时为空，消费者就会回去继续睡眠（c3）。生产者中相应的 if 也改为 while（p2）。</li>
</ul>
<pre><code class="language-C">1 int loops; // must initialize somewhere...
2 cond_t cond;
3 mutex_t mutex;
4
5 void *producer(void *arg) {
6   int i;
7   for (i = 0; i &lt; loops; i++) {
8       Pthread_mutex_lock(&amp;mutex); // p1
9       while (count == 1) // p2
10          Pthread_cond_wait(&amp;cond, &amp;mutex); // p3
11      put(i); // p4
12      Pthread_cond_signal(&amp;cond); // p5
13      Pthread_mutex_unlock(&amp;mutex); // p6
14  }
15 }
16
17 void *consumer(void *arg) {
18  int i;
19  for (i = 0; i &lt; loops; i++) {
20      Pthread_mutex_lock(&amp;mutex); // c1
21      while (count == 0) // c2
22          Pthread_cond_wait(&amp;cond, &amp;mutex); // c3
23      int tmp = get(); // c4
24      Pthread_cond_signal(&amp;cond); // c5
25      Pthread_mutex_unlock(&amp;mutex); // c6
26      printf(&quot;%d\n&quot;, tmp);
27  }
28 }

</code></pre>
<ul>
<li>由于 Mesa 语义，我们要记住一条关于条件变量的简单规则：总是使用 while 循环（always use while loop）。虽然有时候不需要重新检查条件，但这样做总是安全的，做了就开心了。</li>
<li>但是，这段代码仍然有一个问题，也是上文提到的两个问题之一。你能想到吗？它和我们只用了一个条件变量有关。尝试弄清楚这个问题是什么，再继续阅读。想一下！</li>
<li>……（暂停想一想，或者闭一下眼）……</li>
<li>我们来确认一下你想得对不对。假设两个消费者（Tc1 和 Tc2）先运行，都睡眠了（c3）。生产者开始运行，在缓冲区放入一个值，唤醒了一个消费者（假定是 Tc1），并开始睡眠。现在是一个消费者马上要运行（Tc1），两个线程（Tc2 和 Tp）都等待在同一个条件变量上。问题马上就要出现了</li>
<li>消费者 Tc1 醒过来并从 wait()调用返回（c3），重新检查条件（c2），发现缓冲区是满的，消费了这个值（c4）。这个消费者然后在该条件上发信号（c5），唤醒一个在睡眠的线程。但是，应该唤醒哪个线程呢？</li>
<li>因为消费者已经清空了缓冲区，很显然，应该唤醒生产者。但是，如果它唤醒了 Tc2（这绝对是可能的，取决于等待队列是如何管理的），问题就出现了。具体来说，消费者 Tc2 会醒过来，发现队列为空（c2），又继续回去睡眠（c3）。生产者 Tp 刚才在缓冲区中放了一个值，现在在睡眠。另一个消费者线程 Tc1 也回去睡眠了。3 个线程都在睡眠，显然是一个缺陷。由表可以看到这个可怕灾难的步骤。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405170637.png" alt="20210405170637" loading="lazy"></li>
<li>信号显然需要，但必须更有指向性。消费者不应该唤醒消费者，而应该只唤醒生产者，反之亦然。</li>
</ul>
<h4 id="the-single-buffer-producerconsumer-solution">The Single Buffer Producer/Consumer Solution</h4>
<ul>
<li>解决方案也很简单：使用两个条件变量，而不是一个，以便正确地发出信号，在系统状态改变时，哪类线程应该唤醒。</li>
</ul>
<pre><code class="language-C">1 cond_t empty, fill;
2 mutex_t mutex;
3
4 void *producer(void *arg) {
5 int i;
6 for (i = 0; i &lt; loops; i++) {
7   Pthread_mutex_lock(&amp;mutex);
8   while (count == 1)
9       Pthread_cond_wait(&amp;empty, &amp;mutex);
10  put(i);
11  Pthread_cond_signal(&amp;fill);
12  Pthread_mutex_unlock(&amp;mutex);
13  }
14 }
15
16 void *consumer(void *arg) {
17  int i;
18  for (i = 0; i &lt; loops; i++) {
19      Pthread_mutex_lock(&amp;mutex);
20      while (count == 0)
21          Pthread_cond_wait(&amp;fill, &amp;mutex);
22      int tmp = get();
23      Pthread_cond_signal(&amp;empty);
24      Pthread_mutex_unlock(&amp;mutex);
25      printf(&quot;%d\n&quot;, tmp);
26  }
27 }
</code></pre>
<ul>
<li>在上述代码中，生产者线程等待条件变量 empty，发信号给变量 fill。相应地，消费者线程等待 fill，发信号给 empty。这样做，从设计上避免了上述第二个问题：消费者再也不会唤醒消费者，生产者也不会唤醒生产者。</li>
</ul>
<h4 id="the-correct-producerconsumer-solution">The Correct Producer/Consumer Solution</h4>
<ul>
<li>我们现在有了可用的生产者/消费者方案，但不太通用。我们最后的修改是提高并发和效率。具体来说，增加更多缓冲区槽位，这样在睡眠之前，可以生产多个值。同样，睡眠之前可以消费多个值。单个生产者和消费者时，这种方案因为上下文切换少，提高了效率。多个生产者和消费者时，它甚至支持并发生产和消费，从而提高了并发。幸运的是，和现有方案相比，改动也很小。</li>
<li>第一处修改是缓冲区结构本身，以及对应的 put() 和 get()方法。</li>
</ul>
<pre><code class="language-C">1 int buffer[MAX];
2 int fill_ptr = 0;
3 int use_ptr = 0;
4 int count = 0;
5
6 void put(int value) {
7   buffer[fill_ptr] = value;
8   fill_ptr = (fill_ptr + 1) % MAX;
9   count++;
10 }
11
12 int get() {
13  int tmp = buffer[use_ptr];
14  use_ptr = (use_ptr + 1) % MAX;
15  count--;
16  return tmp;
17 }

</code></pre>
<ul>
<li>我们还稍稍修改了生产者和消费者的检查条件，以便决定是否要睡眠。展示了最终的等待和信号逻辑。生产者只有在缓冲区满了的时候才会睡眠（p2），消费者也只有在队列为空的时候睡眠（c2）。至此，我们解决了生产者/消费者问题。</li>
</ul>
<pre><code class="language-C">1 cond_t empty, fill;
2 mutex_t mutex;
3
4 void *producer(void *arg) {
5   int i;
6   for (i = 0; i &lt; loops; i++) {
7       Pthread_mutex_lock(&amp;mutex); // p1
8       while (count == MAX) // p2
9           Pthread_cond_wait(&amp;empty, &amp;mutex); // p3
10      put(i); // p4
11      Pthread_cond_signal(&amp;fill); // p5
12      Pthread_mutex_unlock(&amp;mutex); // p6
13  }
14 }
15
16 void *consumer(void *arg) {
17  int i;
18  for (i = 0; i &lt; loops; i++) {
19      Pthread_mutex_lock(&amp;mutex); // c1
20      while (count == 0) // c2
21          Pthread_cond_wait(&amp;fill, &amp;mutex); // c3
22      int tmp = get(); // c4
23      Pthread_cond_signal(&amp;empty); // c5
24      Pthread_mutex_unlock(&amp;mutex); // c6
25      printf(&quot;%d\n&quot;, tmp);
26  }
27 }

</code></pre>
<blockquote>
<ul>
<li><strong>提示：对条件变量使用 while（不是 if）</strong></li>
<li>多线程程序在检查条件变量时，使用 while 循环总是对的。if 语句可能会对，这取决于发信号的语义。因此，总是使用 while，代码就会符合预期。</li>
<li>对条件变量使用 while 循环，这也解决了假唤醒（spurious wakeup）的情况。某些线程库中，由于实现的细节，有可能出现一个信号唤醒两个线程的情况。再次检查线程的等待条件，假唤醒是另一个原因。</li>
</ul>
</blockquote>
<h3 id="covering-conditions">Covering Conditions</h3>
<ul>
<li>现在再来看条件变量的一个例子。这段代码摘自 Lampson 和 Redell 关于飞行员的论文，同一个小组首次提出了上述的 Mesa 语义（Mesa semantic，他们使用的语言是 Mesa，因此而得名）。</li>
<li>他们遇到的问题通过一个简单的例子就能说明，在这个例子中，是一个简单的多线程内存分配库。</li>
</ul>
<pre><code class="language-C">1 // how many bytes of the heap are free?
2 int bytesLeft = MAX_HEAP_SIZE;
3
4 // need lock and condition too
5 cond_t c;
6 mutex_t m;
7
8 void *
9 allocate(int size) {
10  Pthread_mutex_lock(&amp;m);
11  while (bytesLeft &lt; size)
12      Pthread_cond_wait(&amp;c, &amp;m);
13  void *ptr = ...; // get mem from heap
14  bytesLeft -= size;
15  Pthread_mutex_unlock(&amp;m);
16  return ptr;
17 }
18
19 void free(void *ptr, int size) {
20  Pthread_mutex_lock(&amp;m);
21  bytesLeft += size;
22  Pthread_cond_signal(&amp;c); // whom to signal??
23  Pthread_mutex_unlock(&amp;m);
24 }

</code></pre>
<ul>
<li>从代码中可以看出，当线程调用进入内存分配代码时，它可能会因为内存不足而等待。相应的，线程释放内存时，会发信号说有更多内存空闲。但是，代码中有一个问题：应该唤醒哪个等待线程（可能有多个线程）？</li>
<li>考虑以下场景。假设目前没有空闲内存，线程 Ta 调用 allocate(100)，接着线程 Tb 请求较少的内存，调用 allocate(10)。Ta 和 Tb 都等待在条件上并睡眠，没有足够的空闲内存来满足它们的请求。这时，假定第三个线程 Tc调用了 free(50)。遗憾的是，当它发信号唤醒等待线程时，可能不会唤醒申请 10 字节的 Tb 线程。而 Ta 线程由于内存不够，仍然等待。因为不知道唤醒哪个（或哪些）线程，所以图中代码无法正常工作。</li>
<li>Lampson 和 Redell 的解决方案也很直接：用 pthread_cond_broadcast()代替上述代码中的 pthread_cond_signal()，唤醒所有的等待线程。这样做，确保了所有应该唤醒的线程都被唤醒。当然，不利的一面是可能会影响性能，因为不必要地唤醒了其他许多等待的线程，它们本来（还）不应该被唤醒。这些线程被唤醒后，重新检查条件，马上再次睡眠。</li>
<li>Lampson 和 Redell 把这种条件变量叫作覆盖条件（covering condition），因为它能覆盖所有需要唤醒线程的场景（保守策略）。成本如上所述，就是太多线程被唤醒。聪明的读者可能发现，在单个条件变量的生产者/消费者问题中，也可以使用这种方法。但是，在这个例子中，我们有更好的方法，因此用了它。一般来说，如果你发现程序只有改成广播信号时才能工作（但你认为不需要），可能是程序有缺陷，修复它！但在上述内存分配的例子中，广播可能是最直接有效的方案。</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>我们看到了引入锁之外的另一个重要同步原语：条件变量。当某些程序状态不符合要求时，通过允许线程进入休眠状态，条件变量使我们能够漂亮地解决许多重要的同步问题，包括著名的（仍然重要的）生产者/消费者问题，以及覆盖条件。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lock of Programming]]></title>
        <id>https://blog.shunzi.tech/post/lock/</id>
        <link href="https://blog.shunzi.tech/post/lock/">
        </link>
        <updated>2021-04-01T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>编程中的锁。</li>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第二篇（Locks），锁。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>编程中的锁。</li>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第二篇（Locks），锁。</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="before">Before</h2>
<ul>
<li>开始之前先简单阐述背景，最近项目测试发现在对于底层对象存储的并发访问过程中由于未进行并发控制而产生了一些问题，所以想趁此机会回顾“锁”的相关基础知识，并在此总结，看看能不能在总结的过程中顺便实现项目里的并发访问控制。</li>
<li>还是和往常一样，会贴很多链接，只是尝试着吸收网络上已经存在的优秀的教程，然后再进行一下转化（讲道理确实没啥新意，但主要是为了之后查阅复习起来比较方便）</li>
</ul>
<h2 id="参考链接">参考链接</h2>
<blockquote>
<ul>
<li>先贴参考链接主要是为了膜拜一下，并以表尊敬，因为绝大数情况下，参考链接里的文献就已经写的很好了，我的下文就其实没啥营养了大概，主要是为了节省大家的时间，看完早点去嗨皮~</li>
<li>参考链接也会持续地更新补充，文章的思路将主要沿袭教科书 Three Easy Pieces（其实我是偷懒顺便把书看了~）</li>
</ul>
</blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/71156910">[1] 知乎：通俗易懂 悲观锁、乐观锁、可重入锁、自旋锁、偏向锁、轻量/重量级锁、读写锁、各种锁及其Java实现！</a></li>
<li><a href="https://pages.cs.wisc.edu/~remzi/OSTEP/threads-locks.pdf">[2] Operating Systems: Three Easy Pieces - Concurrency - Locks </a></li>
<li><a href="https://www.zhihu.com/question/66733477/answer/1267625567">[3] 知乎：如何理解互斥锁、条件锁、读写锁以及自旋锁？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57354304">[4] 知乎：漫画|Linux 并发、竞态、互斥锁、自旋锁、信号量都是什么鬼？</a></li>
<li><a href="http://itmyhome.com/java-concurrent-programming/java-concurrent-programming.pdf">[5] Java 并发编程的艺术</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/194283005">[6] 知乎：2w字 + 40张图带你参透并发编程！</a></li>
<li><a href="https://github.com/xlxing/personalLearn/blob/master/%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF.pdf">[7] 多处理器编程的艺术</a>
<ul>
<li><a href="https://github.com/MottoX/TAOMP">Github Repo</a></li>
</ul>
</li>
<li><a href="https://zhuanlan.zhihu.com/p/138819184">[8] 知乎：volatile 关键字，你真的理解吗？</a> （这个参考可能有点例外，更多是针对缓存一致性的，但在并发场景也常使用）</li>
</ul>
<h2 id="locks">Locks</h2>
<ul>
<li>并发编程中的一个基本问题:我们希望原子地执行一系列指令，但由于单个处理器上出现中断(或多个线程同时在多个处理器上执行)，我们无法执行。我们通过引入所谓的锁来直接解决这个问题。程序员用锁注释源代码，把它们放在临界区周围，从而确保任何这样的临界区都像执行单个原子指令一样执行。</li>
</ul>
<h3 id="locks-the-basic-idea">Locks: The Basic Idea</h3>
<ul>
<li>作为一个例子，假设我们的临界区是这样的，共享变量的规范更新:</li>
</ul>
<pre><code class="language-C">balance = balance + 1;
</code></pre>
<ul>
<li>当然，也可能有其他关键部分，比如向链表添加元素，或者对共享结构进行其他更复杂的更新，但我们现在只讨论这个简单的示例。为了使用锁，我们在临界区周围添加如下代码:</li>
</ul>
<pre><code class="language-C">1 lock_t mutex; // some globally-allocated lock ’mutex’
2 ...
3 lock(&amp;mutex);
4 balance = balance + 1;
5 unlock(&amp;mutex);
</code></pre>
<ul>
<li>锁只是一个变量，因此要使用一个锁，必须声明某种类型的锁变量(比如上面的互斥锁)。这个锁变量(或直接简称“lock”)在任何时刻保持锁的状态。它是可用的(或未锁定的或空闲的)，因此没有线程持有该锁，也没有线程获得该锁(或锁定或持有)，因此只有一个线程持有该锁，并且可能处于临界区。我们也可以存储一些额外的信息在数据类型里，例如哪个线程持有当前锁。或者一个预定锁获取的队列，但是这样的信息对锁的使用者是隐藏的。</li>
<li>lock() 和 unlock() 例程的语义很简单。调用例程lock()试图获取锁;如果没有其他线程持有这个锁(即，它是空闲的)，这个线程将获得这个锁并进入临界区;这个线程有时被称为锁的所有者。如果另一个线程在这个锁变量上调用lock()(在本例中是互斥)，当锁被另一个线程持有时，它不会返回;这样，当持有锁的第一个线程还在临界区中时，其他线程就无法进入临界区。</li>
<li>一旦锁的所有者调用unlock()，锁现在就可用了。如果没有其他线程在等待锁(也就是说，没有其他线程调用了lock()并被困在锁中)，锁的状态就会被更改为free。如果有正在等待的线程(卡在lock()中)，其中一个将(最终)通知(或被告知)锁状态的改变，获取锁，并进入临界区</li>
<li>锁为程序员提供了对调度的最低限度的控制。通常，我们将线程视为程序员创建但由操作系统调度的实体，以操作系统选择的任何方式。锁将部分控制权交还给程序员;通过在一段代码周围加一个锁，程序员可以保证在该代码中不会有超过一个线程处于活动状态。因此，锁有助于将传统操作系统调度的混乱转变为一种更可控的活动</li>
</ul>
<h3 id="pthread-locks">Pthread Locks</h3>
<ul>
<li>POSIX库用于锁的名称是互斥锁，因为它用于提供线程之间的互斥，例如，如果一个线程处于临界区，它将排除其他线程进入，直到它完成临界区。因此，当你看到下面的POSIX线程代码时，你应该明白它正在做和上面一样的事情(我们再次使用我们的包装器在锁定和解锁时检查错误):</li>
</ul>
<pre><code class="language-C">1 pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
2
3 Pthread_mutex_lock(&amp;lock); // wrapper; exits on failure
4 balance = balance + 1;
5 Pthread_mutex_unlock(&amp;lock);

// Keeps code clean; only use if exit() OK upon failure
void Pthread_mutex_lock(pthread_mutex_t *mutex) {
  int rc = pthread_mutex_lock(mutex);
  assert(rc == 0);
}
</code></pre>
<ul>
<li>您可能还注意到，POSIX版本将一个变量传递给lock和unlock，因为我们可能使用不同的锁来保护不同的变量。这样做可以增加并发性:与在访问任何关键部分时使用一个大锁(一种粗粒度的锁定策略)不同，通常使用不同的锁来保护不同的数据和数据结构，从而允许更多的线程同时处于锁定代码中(更细粒度的方法)。</li>
</ul>
<h3 id="building-a-lock">Building A Lock</h3>
<ul>
<li>我们如何建立一个高效的锁?高效的锁以较低的成本提供互斥，还可能获得我们下面讨论的一些其他属性。需要什么硬件支持?操作系统支持什么?</li>
<li>为了构建一个有效的锁，我们需要我们的老朋友——硬件，以及我们的好朋友——操作系统的帮助。多年来，许多不同的硬件原语被添加到各种计算机体系结构的指令集中;虽然我们不会研究这些指令是如何实现的(毕竟，这是计算机架构类的主题)，但我们将研究如何使用它们来构建像锁一样的互斥原语。我们还将研究操作系统是如何参与进来的，从而使我们能够构建一个复杂的锁库。</li>
</ul>
<h3 id="evaluating-locks">Evaluating Locks</h3>
<ul>
<li>在构建任何锁之前，我们应该首先理解我们的目标是什么，因此我们应该询问如何评估特定锁实现的有效性。要评价锁是否有效(以及是否有效)，我们应该建立一些基本的标准。第一个问题是锁是否执行它的基本任务，即提供互斥。基本上，锁是否有效，防止多个线程进入一个临界区?</li>
<li>第二是公平。是否每个争用锁的线程都有机会在锁空闲时获取它?另一种考虑这个问题的方法是检查更极端的情况:是否有任何争用锁的线程在争用锁的时候饿死，从而永远得不到锁?</li>
<li>最后一个标准是性能，特别是使用锁所增加的时间开销。这里有几个不同的情况值得考虑。一种是没有争用的情况;当一个线程正在运行并获取和释放锁时，这样做的开销是什么?另一种情况是多个线程争夺单个CPU上的锁;在这种情况下，是否存在性能问题?最后，当涉及多个cpu，并且每个cpu上的线程都争用该锁时，该锁是如何执行的?通过比较这些不同的场景，我们可以更好地理解使用各种锁定技术对性能的影响，如下所述。</li>
</ul>
<h3 id="controlling-interrupts">Controlling Interrupts</h3>
<ul>
<li>最早用于提供互斥的解决方案之一是禁用临界区中断;这种解决方案是为单处理器系统而发明的。代码如下所示:</li>
</ul>
<pre><code class="language-C">1 void lock() {
2   DisableInterrupts();
3 }
4 void unlock() {
5   EnableInterrupts();
6 }
</code></pre>
<ul>
<li>假设我们运行在这样一个单处理器系统上。通过在进入临界区之前关闭中断(使用某种特殊的硬件指令)，我们可以确保临界区内的代码不会被中断，从而可以像原子一样执行。当我们完成时，我们重新启用中断(同样是通过硬件指令)，这样程序就像往常一样继续运行。这种方法的主要优点是简单。你当然不需要绞尽脑汁来弄明白为什么这是可行的。在没有中断的情况下，线程可以确保它执行的代码将被执行，并且不会有其他线程干扰它。</li>
<li>不幸的是，负面的东西很多。
<ul>
<li>首先，这种方法要求我们允许任何调用线程执行特权操作(打开和关闭中断)，因此相信这种功能不会被滥用。正如你已经知道的，任何时候我们被要求信任一个任意的程序，我们可能会遇到麻烦。在这里，问题表现在许多方面:贪婪的程序可能会在执行开始时调用lock()，从而垄断处理器;更糟糕的是，一个错误的或恶意的程序可能会调用lock()并进入一个无穷循环。在后一种情况下，操作系统永远无法恢复对系统的控制，只有一种方法:重启系统。使用中断禁用作为通用的同步解决方案需要对应用程序有太多的信任。</li>
<li>第二，这种方法不能在多处理器上工作。如果多个线程运行在不同的cpu上，并且每个线程都试图进入相同的临界区，那么是否禁用中断是无关紧要的;线程可以在其他处理器上运行，因此可以进入临界区。由于多处理器现在很普遍，我们的一般解决方案必须做得比这更好。</li>
<li>第三，长时间关闭中断可能会导致中断丢失，从而导致严重的系统问题。例如，想象一下，如果CPU没有注意到磁盘设备已经完成了一个读请求。操作系统如何知道要唤醒等待读取的进程?</li>
<li>最后，也可能是最不重要的，这种方法可能效率低下。与普通的指令执行相比，屏蔽或取消中断的代码在现代cpu中执行得比较慢。</li>
</ul>
</li>
<li>由于这些原因，关闭中断只在有限的上下文中用作互斥原语。例如，在某些情况下，操作系统本身将使用中断屏蔽来保证访问自己的数据结构时的原子性，或者至少防止出现某些混乱的中断处理情况。这种用法是有意义的，因为信任问题在操作系统内部消失了，操作系统无论如何总是信任自己执行特权操作。</li>
</ul>
<h3 id="a-failed-attempt-just-using-loadsstores">A Failed Attempt: Just Using Loads/Stores</h3>
<ul>
<li>要超越基于中断的技术，我们将不得不依赖CPU硬件以及它提供给我们的构建正确锁的指令。让我们首先尝试通过使用单个标记变量来构建一个简单的锁。在这次失败的尝试中，我们将了解构建锁所需的一些基本思想，并(希望如此)了解为什么仅使用单个变量并通过正常的 load 和 store 访问它是不够的。</li>
<li>在如下第一次尝试中，想法非常简单:使用一个简单的变量(标志)来指示某个线程是否拥有锁。进入临界区的第一个线程将调用lock()，它将测试该标志是否等于1(在本例中，它不是)，然后将该标志设置为1，以表明该线程现在持有锁。当临界区结束时，线程调用unlock()并清除标志，从而表明锁不再被持有。</li>
<li>如果另一个线程碰巧在第一个线程处于临界区时调用了lock()，它将在while循环中简单地spin-wait，以便该线程调用unlock()并清除标志。一旦第一个线程这样做了，等待的线程将退出while循环，为自己设置标志为1，并继续进入临界区。</li>
</ul>
<pre><code class="language-C">1 typedef struct __lock_t { int flag; } lock_t;
2
3 void init(lock_t *mutex) {
4   // 0 -&gt; lock is available, 1 -&gt; held
5   mutex-&gt;flag = 0;
6 }
7
8 void lock(lock_t *mutex) {
9   while (mutex-&gt;flag == 1) // TEST the flag
10      ; // spin-wait (do nothing)
11  mutex-&gt;flag = 1; // now SET it!
12 }
13
14 void unlock(lock_t *mutex) {
15  mutex-&gt;flag = 0;
16 }

</code></pre>
<ul>
<li>不幸的是，代码有两个问题:一个是正确性，另一个是性能。一旦您习惯了并行编程，正确性问题就很容易看到。想象一下如果代码交错;假设flag=0开始。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210401172110.png" alt="20210401172110" loading="lazy"></li>
<li>正如您从这个交错中看到的，通过及时(不及时?)中断，我们可以很容易地产生这样一种情况，即两个线程都将标志设置为1，从而两个线程都能够进入临界区。这种行为被专业人士称为“bad”——我们显然没有提供最基本的要求:提供互斥。</li>
<li>性能问题(我们将在后面详细讨论)是这样一个事实:线程等待获取已经持有的锁的方式:它无休止地检查flag的值，这种技术称为旋转等待。旋转等待浪费了等待另一个线程释放锁的时间。在单处理器上，这种浪费是非常高的，服务端正在等待的线程甚至不能运行(至少在发生上下文切换之前)!因此，当我们向前推进并开发更复杂的解决方案时，我们也应该考虑避免这种浪费的方法。</li>
</ul>
<blockquote>
<p>ASIDE: DEKKER’S AND PETERSON’S ALGORITHMS</p>
<ul>
<li>在20世纪60年代，Dijkstra向他的朋友们提出了并发问题，其中一位名叫Theodorus Jozef Dekker的数学家提出了一个解决方案。我们这里讨论的解决方案使用特殊的硬件指令甚至操作系统支持，而Dekker的算法只使用 load 和 store(假设它们彼此之间是原子的，这在早期的硬件上是正确的)。</li>
<li>Dekker的方法后来被Peterson改进了。同样，只使用load和store，这样做是为了确保两个线程不会同时进入临界区。下面是Peterson的算法(适用于两个线程);看看你能不能理解代码。flag 和 turn 的用途是什么? <strong>如下代码其实同时使用了 flag 和 turn 来调度线程，对应线程的 flag 置为了 1 说明该线程即将持有锁，和前面的代码类似，但是还使用了一个 turn 来表明下一个需要调度的线程的 ID，此时两个线程都运行到了 while 处时，条件 1 必定为真，条件二必定其中一个线程在执行时为真，该线程相应地进行自旋</strong></li>
<li><strong>flag 表示哪个线程想要占用临界区状态，就是举手表示想要访问临界区</strong></li>
<li><strong>turn 用于标识当前允许谁进入，就是谁的轮次</strong></li>
</ul>
</blockquote>
<pre><code class="language-C">int flag[2];
int turn;
void init() {
    // indicate you intend to hold the lock w/ ’flag’
    flag[0] = flag[1] = 0;
    // whose turn is it? (thread 0 or 1)
    turn = 0;
}
void lock() {
    // ’self’ is the thread ID of caller
    // 首先举手表示当前线程要访问临界区，eg 线程 1，即 flag[1]=1
    flag[self] = 1;
    // make it other thread’s turn
    // 这里其实有一个“礼让”的逻辑
    // 就是说虽然是当前线程在执行，但是会把轮次让给另外的线程
    // 如果另外的那个线程确实也举手了，
    //        然后另外的线程还没来得及礼让，那么就由另外的线程执行
    //        要是另外的线程也刚好礼让了，那么就由当前线程执行
    turn = 1 - self;
    // 首先看另外的线程举手了没？
    // case0. 没举手那就只有当前线程访问，直接运行就完事了
    // case1. 另外的线程举手了，那这时候就要看该谁的轮次了
    //   case1.0 如果这时候的轮次确实该另外的线程 1-self，那当前线程就自旋等待
    //   case2.0 如果这时候的轮次该自个儿 self，那当前线程就执行
    while ((flag[1-self] == 1) &amp;&amp; (turn == 1 - self))
        ;  // spin-wait while it’s not your turn
}
void unlock() {
    // simply undo your intent
    flag[self] = 0;
}
</code></pre>
<blockquote>
<ul>
<li>由于某些原因，开发不需要特殊硬件支持就能工作的锁在一段时间内变得非常流行，这给理论类型带来了许多问题。当然，当人们意识到假定有一点硬件支持会容易得多(事实上，这种支持早在多处理的早期就存在了)时，这一行的工作就变得毫无用处了。此外，上面的算法不能在现代硬件上运行(由于内存一致性模型不严格)，因此它们比以前更没用了。然而，更多的研究被扔进了历史的垃圾箱</li>
<li>Peterson 算法结合了 LockOne 和 LockTwo 两种算法
<ul>
<li>LockOne 只满足互斥，无法避免死锁。</li>
<li>LockTwo 也只满足互斥，无法避免死锁</li>
</ul>
</li>
<li>https://zh.wikipedia.org/wiki/Peterson%E7%AE%97%E6%B3%95#cite_note-3</li>
<li><a href="https://zhuanlan.zhihu.com/p/125739705">并发编程的艺术02-过滤锁算法</a></li>
</ul>
</blockquote>
<pre><code class="language-Java">class LockOne implements Lock {
    private boolean[] flag = new boolean[2];
    
    // 因为 Lock 本身不互斥，所以两个线程可能同时都将对方的 flag 标识设置为 true
    // 导致两个线程都被阻塞在 while(true) {} 。
    public void lock() {
        int i = ThreadId.get();
        int j = 1 - i;
        flag[i] = true;
        while(flag[j]) {}
    }
    
    public void unlock() {
        int i = ThreadID.get();
        flag[i] = false;
    }
}

class LockTwo implements Lock {
    private volatile int victim;
    
    // 当一个线程先执行，而另一个线程迟迟不执行
    // 这时候第一个线程就被锁住无法进行向下执行
    public void lock() {
        int i = ThreadId.get();

        // 阻塞率先执行 victim = i 的线程（礼让执行）
        victim = i;
        while(victim == i) {}
    }
    
    public void unlock() {
    }
}

class PetersonLock implements Lock {
    private boolean[] flag = new boolean[2];
    private volatile int victim;
    
    public void lock() {
        int i = ThreadId.get();
        int j = 1 - i;
        flag[i] = true;
        victim = i;
       while(flag[j] &amp;&amp; victim == i) {}
    }
    
    public void unlock() {
        int i = ThreadID.get();
        flag[i] = false;
    }
}

class FilterLock implements Lock {
    int[] level;
    int[] victim;
    int n;
    
    public FilterLock(int n) {
        level = new int[n];
        victim = new int[n];
        this.n = n;
        for (int i = 0;i &lt; n; i++) {
            level[i] = 0;
       }
    }
    
    public void lock() {
        int me = ThreadID.get();
        for (int i = 0; i &lt; n; i++) {
            level[me] = i;
            victim[i] = me;
            for (int k = 0; k &lt; n;k++) {
                while ((k != me) &amp;&amp; (level[k] &gt;= i &amp;&amp; victim[i] == me))) {
                    
                }
            }
        }
    }
    
    public void unlock() {
        int me = ThreadID.get();
        level[me] = 0;
    }
}
</code></pre>
<h3 id="building-working-spin-locks-with-test-and-set">Building Working Spin Locks with Test-And-Set</h3>
<blockquote>
<ul>
<li>当使用TAS实现TASLock （Test And Set Lock）测试-设置锁，它的特点是自旋时，每次尝试获取锁时，底层还是使用CAS操作，不断的设置锁标志位的过程会一直修改共享变量的值（回写），会引发缓冲一致性流量发风暴。【因为每一次CAS操作都会发出广播通知其他处理器，从而影响程序的性能。】</li>
<li><a href="https://en.wikipedia.org/wiki/Test-and-set">Wikipedia: Test and Set</a></li>
</ul>
</blockquote>
<ul>
<li>因为禁用中断不能在多个处理器上工作，而且使用load和store(如上所示)的简单方法也不能工作，所以系统设计人员开始发明对锁定的硬件支持。最早的多处理器系统，如20世纪60年代早期的Burroughs B5000，就有这样的支持;如今，所有系统都提供这种类型的支持，即使是单CPU系统。</li>
<li>需要理解的最简单的硬件支持是 test-and-set (或 atomic exchange) 指令。我们通过下面的C代码片段来定义 test-and-set 指令的作用:</li>
</ul>
<pre><code class="language-C">1 int TestAndSet(int *old_ptr, int new) {
2   int old = *old_ptr; // fetch old value at old_ptr
3   *old_ptr = new; // store ’new’ into old_ptr
4   return old; // return the old value
5 }

1 typedef struct __lock_t {
2   int flag;
3 } lock_t;
4
5 void init(lock_t *lock) {
6   // 0: lock is available, 1: lock is held
7   lock-&gt;flag = 0;
8 }
9
10 void lock(lock_t *lock) {
11  while (TestAndSet(&amp;lock-&gt;flag, 1) == 1)
12      ; // spin-wait (do nothing)
13 }
14
15 void unlock(lock_t *lock) {
16  lock-&gt;flag = 0;
17 }
</code></pre>
<ul>
<li><strong>test-and-set 指令的作用如下。它返回旧 ptr 所指向的旧值，并同时将该值更新为new。当然，关键是这个操作序列是以原子的方式执行的。</strong> 之所以称之为 “test-and-set”，是因为它使您能够同时“test”旧值(即返回的值)“set”内存位置为一个新值;事实证明，这个稍微强大一些的指令足以构建一个简单的旋转锁。</li>
<li>我们先弄清楚为什么这把锁可以用。首先想象一下这样一种情况:
<ul>
<li>一个线程调用了lock()，而当前没有其他线程持有该锁;因此，flag应该是0。当线程调用 TestAndSet(flag，1)，线程返回旧的 flag 值，即 0; 因此，正在 testing flag 值的调用线程将不会在 while 循环中被捕获，并将获得锁。线程也会自动地将该值设置为 1，从而表明现在已持有锁。当线程完成它的临界区时，它调用unlock()将标志设为0。</li>
<li>我们可以想象的第二种情况是，当一个线程已经持有锁(即，flag是1)。在这种情况下，这个线程将调用lock()，然后调用 TestAndSet(flag, 1)。这一次，teststandset()将返回旧的flag值，即1(因为锁被持有)，同时再次将其设置为1。只要锁被另一个线程持有，TestAndSet()就会反复返回1，因此这个线程就会不停地旋转，直到锁最终被释放。当标志最终被其他线程设置为0时，该线程将再次调用 TestAndSet()，它现在将返回0，同时原子地将值设置为1，从而获得锁并进入临界区。</li>
</ul>
</li>
<li><strong>通过将 Test(对旧锁的值)和 Set(对新值的值)都设置为单个原子操作，我们确保只有一个线程获得锁。这就是如何构建一个有效的互斥原语</strong>!</li>
<li>现在可能也理解了为什么这种类型的锁通常称为自旋锁。它是需要构建的最简单的锁类型，只是使用CPU周期旋转，直到锁可用为止。为了在单个处理器上正常工作，它需要一个抢占式调度程序(即，为了不时地运行另一个线程，它会通过计时器中断一个线程)。如果没有抢占，旋转锁在单个CPU上没有多大意义，因为在CPU上旋转的线程永远不会放弃它。</li>
<li>汇编代码实现：</li>
</ul>
<pre><code>enter_region:        ; A &quot;jump to&quot; tag; function entry point.

  tsl reg, flag      ; Test and Set Lock; flag is the
                     ; shared variable; it is copied
                     ; into the register reg and flag
                     ; then atomically set to 1.

  cmp reg, #0        ; Was flag zero on entry_region?

  jnz enter_region   ; Jump to enter_region if
                     ; reg is non-zero; i.e.,
                     ; flag was non-zero on entry.

  ret                ; Exit; i.e., flag was zero on
                     ; entry. If we get here, tsl
                     ; will have set it non-zero; thus,
                     ; we have claimed the resource
                     ; associated with flag.

leave_region:
  move flag, #0      ; store 0 in flag
  ret                ; return to caller
</code></pre>
<h3 id="evaluating-spin-locks">Evaluating Spin Locks</h3>
<ul>
<li>有了我们的基本自旋锁，我们现在可以评估它在我们前面描述的轴上的有效性。锁最重要的方面是<strong>正确性</strong>:它是否提供了互斥?答案是肯定的:旋转锁一次只允许一个线程进入临界区。这样，我们就有了一个正确的锁。</li>
<li>下一个轴是<strong>公平</strong>。旋转锁对等待的线程有多公平?你能保证等待的线程会进入临界区吗?不幸的是，这里的答案是坏消息:<strong>旋转锁不提供任何公平性保证。的确，在争用的情况下，在旋转等待的线程可能一直旋转。简单的旋转锁(到目前为止已经讨论过)是不公平的，可能会导致线程饿死</strong>。</li>
<li>最后一个轴是<strong>性能</strong>。使用旋转锁的成本是多少?为了更仔细地分析这个问题，我们建议考虑几个不同的案例。在第一种情况下，想象线程在单个处理器上竞争锁;在第二种情况下，考虑分布在多个cpu上的线程。
<ul>
<li>对于自旋锁，在单一CPU的情况下，性能开销是非常痛苦的;想象一下持有锁的线程在一个临界区中被抢占的情况。调度程序可能会运行其他每一个线程(假设有N−1个其他线程)，每个线程都试图获取锁。在这种情况下，每个线程都将在一个时间片的持续时间内旋转，然后放弃CPU，这是对CPU周期的浪费。</li>
<li>但是，在多个cpu上，旋转锁工作得相当好(如果线程的数量大致等于cpu的数量)。假设CPU 1上的线程A和CPU 2上的线程B都争用一个锁。如果线程(CPU 1)持有锁,线程B试图加锁,B将自旋(CPU 2)。然而,假定关键的部分较短,因此很快锁就变得可用,线程B将获得锁。旋转等待锁在另一个处理器不会浪费很多的周期在本例中,从而可以很有效。</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><strong>更多避免旋转的理由:优先级反转</strong></li>
<li>避免旋转锁的一个很好的理由是性能:正如正文中所描述的，如果一个线程在持有一个锁时被中断，其他使用旋转锁的线程将花费大量的CPU时间来等待锁可用。然而，在某些系统上避免自旋锁还有另一个有趣的原因:正确性。要警惕的问题是所谓的优先级反转。</li>
<li>现在,这个问题。假设T2由于某种原因被阻塞。所以T1运行，抓住旋转锁，进入一个临界区。T2现在变得畅通(可能是因为一个I/O完成了)，CPU调度器会立即调度它(从而重新调度T1)。T2现在尝试获取锁，因为它不能(T1持有锁)，所以它只是继续旋转。因为锁是自旋锁，T2永远自旋，系统就挂了。</li>
<li>不幸的是，仅仅避免使用旋转锁并不能避免反转的问题(唉)。假设有三个线程，T1, T2和T3。T3的优先级最高，T1的优先级最低。现在想象一下，T1抓住了一个锁。然后T3启动，由于它的优先级高于T1，所以它立即运行(抢占T1)。T3试图获取T1所持有的锁，但是由于T1仍然持有锁，所以被困在等待中。如果T2开始运行，它将拥有比T1更高的优先级，因此它将运行。T3的优先级高于T2，它被困在等待T1，由于T2正在运行，T1可能永远不会运行。强大的T3不能运行，而不起眼的T2控制着CPU ?高优先级已经不像以前那么重要了。</li>
<li>您可以通过多种方式解决优先级反转问题。在自旋锁导致问题的特定情况下，可以避免使用自旋锁(后面将详细描述)。一般来说，高优先级线程等待低优先级线程可以暂时提高低优先级线程的优先级，从而使其能够运行并克服反转，这种技术称为优先级继承。最后一个解决方案是最简单的:确保所有线程具有相同的优先级。</li>
</ul>
</blockquote>
<h3 id="compare-and-swap">Compare-And-Swap</h3>
<blockquote>
<ul>
<li>在计算机科学中，比较与交换(CAS)是多线程中用来实现同步的原子指令。它将内存位置的内容与给定值进行比较，只有当它们相同时，才将该内存位置的内容修改为新的给定值。这是作为单个原子操作完成的</li>
<li><a href="https://en.wikipedia.org/wiki/Compare-and-swap">Wikipedia: CAS</a></li>
</ul>
</blockquote>
<ul>
<li>一些系统提供的另一个硬件原语是 Compare-And-Swap 指令(例如，SPARC上这样称呼它)或compare-and-exchange 指令(x86上这样称呼它)。<strong>也就是我们常说的 CAS 操作</strong>。这条指令的C伪代码如下：</li>
</ul>
<pre><code class="language-C">1 int CompareAndSwap(int *ptr, int expected, int new) {
2   int original = *ptr;
3   if (original == expected)
4       *ptr = new;
5   return original;
6 }
</code></pre>
<ul>
<li>其基本思想是通过  compare-and-swap 来测试ptr指定的地址上的值是否等于预期值;如果是，用新值更新ptr所指向的内存位置。如果不是，那就什么都不做。在这两种情况下，返回该内存位置的原始值，从而允许调用 compare-and-swap 的代码知道它是否成功。</li>
<li>CAS 操作其实就比 TAS 多了一个参数，<code>expected</code> 预期值，预期值 E 主要用于和当前值的比对，从而来决定是否更新新的值。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210404172916.png" alt="20210404172916" loading="lazy"></li>
<li>使用 compare-and-swap 指令，我们可以以类似于使用 test-and-set 的方式构建锁。例如，我们可以用以下代码替换上面的lock()例程:</li>
</ul>
<pre><code class="language-C">1 void lock(lock_t *lock) {
2   while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)
3       ; // spin
4 }
</code></pre>
<ul>
<li>其余的代码与上面的测试集示例相同。这段代码的工作原理非常相似;它只是检查标志是否为0，如果是，就自动交换1，从而获得锁。当锁被持有时，试图获取锁的线程将被困在旋转中，直到锁最终被释放。</li>
<li>如果您想了解如何真正制作一个 c 可调用的x86版本的 compare-and-swap，看以下链接：<br>
https://github.com/remzi-arpacidusseau/ostep-code/tree/master/threads-locks</li>
</ul>
<pre><code class="language-C">#include &lt;stdio.h&gt;

int global = 0;

char compare_and_swap(int *ptr, int old, int new) {
    unsigned char ret;
    // Note that sete sets a ’byte’ not the word
    __asm__ __volatile__ (
	&quot; lock\n&quot;
	&quot; cmpxchgl %2,%1\n&quot;
	&quot; sete %0\n&quot;
	: &quot;=q&quot; (ret), &quot;=m&quot; (*ptr)
	: &quot;r&quot; (new), &quot;m&quot; (*ptr), &quot;a&quot; (old)
	: &quot;memory&quot;);
    return ret;
}

int main(int argc, char *argv[]) {
    printf(&quot;before successful cas: %d\n&quot;, global);
    int success = compare_and_swap(&amp;global, 0, 100);
    printf(&quot;after successful cas: %d (success: %d)\n&quot;, global, success);
    
    printf(&quot;before failing cas: %d\n&quot;, global);
    success = compare_and_swap(&amp;global, 0, 200);
    printf(&quot;after failing cas: %d (old: %d)\n&quot;, global, success);

    return 0;
}
</code></pre>
<ul>
<li>最后，您可能已经感觉到，compare_and_swap 指令比 test-and-set 指令更强大。我们将在以后简要地研究诸如无锁同步等主题时使用这种能力。然而，如果我们只是用它构建一个简单的自旋锁，它的行为与我们上面分析的自旋锁相同。</li>
</ul>
<h3 id="load-linked-and-store-conditional">Load-Linked and Store-Conditional</h3>
<ul>
<li>一些平台提供了一对协同工作的指令来帮助构建关键部分。例如，在MIPS架构中，load-linked 和 store-conditional 指令可以串联使用来构建锁和其他并发结构。这些指令的 C 伪代码如下。Alpha、PowerPC和ARM提供了类似的说明</li>
</ul>
<pre><code class="language-C">1 int LoadLinked(int *ptr) {
2   return *ptr;
3 }
4
5 int StoreConditional(int *ptr, int value) {
6   if (no update to *ptr since LoadLinked to this address) {
7       *ptr = value;
8       return 1; // success!
9   } else {
10      return 0; // failed to update
11  }
12 }
</code></pre>
<ul>
<li>load-linked的操作与典型的load指令非常相似，它只是从内存中获取一个值并将其放入寄存器中。关键的不同在于store-conditional，它<strong>只在没有对该地址进行存储的情况下才成功(并更新存储在刚刚进行load-linked的地址上的值)</strong>。在成功的情况下，store-conditional返回1并将ptr处的值更新为value;如果失败，则ptr处的值不会更新，返回0。</li>
<li>lock()代码是唯一有趣的部分。首先，线程旋转，等待标志被设置为0(从而表明锁没有被持有)。一旦这样，线程试图通过store条件获取锁;如果成功，线程就自动地将标志的值更改为1，因此可以进入临界区</li>
<li>请注意 store-conditional 是如何发生故障的。一个线程调用lock()并执行LoadLinked，当锁未被持有时返回0。在它尝试 store-conditional 之前，它被中断，另一个线程进入 lock代码，也执行 load-linked 指令，得到一个0，然后继续。此时，两个线程分别执行了load-linked，并且都准备尝试store-conditional。这些指令的<strong>关键特性是，只有其中一个线程会成功地将标志更新为1</strong>，从而获得锁;第二个尝试 store-conditional 的线程将失败(因为另一个线程更新了它的load-linked和store-conditional之间的标志值)，因此必须再次尝试获取锁。</li>
</ul>
<pre><code class="language-C">1 void lock(lock_t *lock) {
2   while (1) {
3       while (LoadLinked(&amp;lock-&gt;flag) == 1)
4           ; // spin until it’s zero
5       if (StoreConditional(&amp;lock-&gt;flag, 1) == 1)
6           return; // if set-it-to-1 was a success: all done
7                   // otherwise: try it all over again
8   }
9 }
10
11 void unlock(lock_t *lock) {
12  lock-&gt;flag = 0;
13 }
</code></pre>
<ul>
<li>简化一下如上代码</li>
</ul>
<pre><code class="language-C">1 void lock(lock_t *lock) {
2   while (LoadLinked(&amp;lock-&gt;flag) ||
3           !StoreConditional(&amp;lock-&gt;flag, 1))
4       ; // spin
5 }
</code></pre>
<h3 id="fetch-and-add">Fetch-And-Add</h3>
<ul>
<li>最后一个硬件原语是“Fetch-And-Add”指令，它在返回特定地址的旧值时自动增加一个值。Fetch-And-Add指令的C伪代码是这样的:</li>
</ul>
<pre><code class="language-C">1 int FetchAndAdd(int *ptr) {
2   int old = *ptr;
3   *ptr = old + 1;
4   return old;
5 }
</code></pre>
<ul>
<li>在本例中，我们将使用取加(fetch-and-add)来构建一个更有趣的 ticket lock，。锁定和解锁代码如下：</li>
</ul>
<pre><code class="language-C">1 typedef struct __lock_t {
2   int ticket;
3   int turn;
4 } lock_t;
5
6 void lock_init(lock_t *lock) {
7   lock-&gt;ticket = 0;
8   lock-&gt;turn = 0;
9 }
10
11 void lock(lock_t *lock) {
12  int myturn = FetchAndAdd(&amp;lock-&gt;ticket);
13  while (lock-&gt;turn != myturn)
14      ; // spin
15 }
16
17 void unlock(lock_t *lock) {
18  lock-&gt;turn = lock-&gt;turn + 1;
19 }

</code></pre>
<ul>
<li>这个解决方案不是使用单个值，而是组合使用 ticket 和 turn 变量来构建一个锁。基本操作非常简单:当线程希望获取锁时，它首先对票据值执行原子的取加操作;这个值现在被认为是这个线程的“回合”(myturn)。然后使用全局共享的 lock-&gt;turn 来确定线程的回合;当(myturn == turn)对于一个给定的线程，它是线程进入临界区的回合。解锁可以通过增加回合数来实现，这样下一个等待线程(如果有的话)就可以进入临界区。</li>
<li>注意这个解决方案与我们之前尝试的一个重要区别:它确保了所有线程的执行。一旦给一个线程分配了它的票证值，它将在未来的某个时间点被调度(一旦它前面的线程通过了临界区并释放了锁)。在我们以前的尝试中，并不存在这样的保证;在 test-and-set 上自旋的线程(例如)可能会永远自旋，即使其他线程获得和释放锁。</li>
</ul>
<h3 id="too-much-spinning-what-now">Too Much Spinning: What Now?</h3>
<ul>
<li>我们的简单的基于硬件的锁很简单(只有几行代码)，而且它们可以工作(如果您愿意，甚至可以通过编写一些代码来证明这一点)，这是任何系统或代码的两个优秀特性。然而，在某些情况下，这些解决方案可能非常低效。假设您在一个处理器上运行两个线程。现在，假设有一个线程(线程0)处于临界区，因此持有一个锁，不幸的是被中断。第二个线程(线程1)现在尝试获取锁，但发现锁被持有。因此，它开始自旋，接着自旋。</li>
<li>然后它继续自旋。最后，时钟中断产生，线程0再次运行，释放锁，最后(比如，下一次运行时)，线程1不必自选了，可以获得锁。因此，任何时候一个线程在这种情况下被捕获，它都浪费了整个时间片，自旋只检查一个不会改变的值! 当有N个线程争用一个锁时，这个问题会变得更糟;N−1个时间片也可能以类似的方式浪费，只是自旋并等待一个线程释放锁。因此，我们的下一个问题:<strong>我们怎样才能开发出一个不需要浪费时间在CPU上自旋的锁呢</strong>？</li>
<li>仅靠硬件支持无法解决这个问题。我们也需要操作系统的支持!现在让我们来看看它是如何工作的。</li>
</ul>
<h3 id="a-simple-approach-just-yield-baby">A Simple Approach: Just Yield, Baby</h3>
<ul>
<li>硬件支持让我们走得很远:工作锁，甚至(就像票据锁的情况)获取锁的公平性。然而，我们仍然有一个问题:当临界区发生上下文切换，并且线程开始无休止地旋转，等待被中断时，该怎么办让(持有锁的)线程再次运行?</li>
<li>我们的第一个尝试是一个简单而友好的方法:当要自旋时，将CPU让给另一个线程。如下演示了这种方法：</li>
</ul>
<pre><code class="language-C">1 void init() {
2   flag = 0;
3 }
4
5 void lock() {
6   while (TestAndSet(&amp;flag, 1) == 1)
7       yield(); // give up the CPU
8 }
9
10 void unlock() {
11  flag = 0;
12 }

</code></pre>
<ul>
<li>在这种方法中，我们假设一个操作系统原语 <code>yield()</code>，当线程想要放弃CPU并让另一个线程运行时，可以调用它。线程可以处于以下三种状态中的一种(运行、准备或阻塞); <code>yield</code>只是一个系统调用，它将调用者从运行状态移动到就绪状态，从而使另一个线程运行。因此，<strong>yield 的线程本质上是对自己进行了重新调度</strong>。</li>
<li>考虑一个CPU上有两个线程的例子;在这种情况下，我们基于 yield 的方法非常有效。如果一个线程碰巧调用了lock()，并发现一个锁被持有，它就会释放CPU，这样另一个线程就会运行并完成它的临界区。在这个简单的例子中，yielding 方法很有效。</li>
<li>现在让我们考虑一下有许多线程(比如100个)反复争夺一个锁的情况。在这种情况下，如果一个线程获得了锁，并在释放它之前被抢占，其他99个线程将调用lock()，找到所持有的锁，并交出CPU。假设有某种轮询调度器，在持有锁的线程再次运行之前，这99个调度器中的每一个都将执行这个run-and-yield模式。虽然比我们的旋转方法更好(这将浪费99个时间切片旋转)，<strong>但这种方法仍然是昂贵的;上下文切换的成本可能很高，因此会产生大量的浪费</strong>。</li>
<li>更糟糕的是，我们根本没有解决饥饿问题。当其他线程反复进入和退出临界区时，一个线程可能会陷入一个无穷 yield 循环。我们显然需要一种直接解决这一问题的方法。</li>
</ul>
<h3 id="using-queues-sleeping-instead-of-spinning">Using Queues: Sleeping Instead Of Spinning</h3>
<ul>
<li>我们之前方法的真正问题是，它们留给了太多的机会。调度程序决定下一个线程运行;如果调度程序做出了错误的选择，那么运行的线程要么必须自旋，等待锁(我们的第一种方法)，要么立即 yield CPU (我们的第二种方法)。无论哪种方式，都有浪费的可能，而且无法防止饥饿。</li>
<li>因此，在当前持有者释放锁之后，必须显式地对下一个获得锁的线程施加一些控制。为此，我们需要更多的操作系统支持，以及<strong>一个队列来跟踪哪些线程正在等待获取锁</strong>。</li>
<li>为简单起见，我们将使用 Solaris 提供的支持，包括两种调用： <strong><code>park()</code> 将调用线程置于睡眠状态，<code>unpark(threadID)</code> 唤醒由threadID指定的特定线程。</strong> 这两个例程可以一起使用来构建一个锁，如果调用者试图获取一个被占用的锁，那么这个锁将使调用者处于休眠状态，当锁空闲时将其唤醒。如下代码所示</li>
</ul>
<pre><code class="language-C">1 typedef struct __lock_t {
2   int flag;
3   int guard;
4   queue_t *q;
5 } lock_t;
6
7 void lock_init(lock_t *m) {
8   m-&gt;flag = 0;
9   m-&gt;guard = 0;
    // 初始化锁的等待队列
10  queue_init(m-&gt;q);
11 }
12
13 void lock(lock_t *m) {
    // 尝试获取锁，将 guard 设置为 1，如果已经为 1 自旋等待
14  while (TestAndSet(&amp;m-&gt;guard, 1) == 1)
15      ; //acquire guard lock by spinning
16  if (m-&gt;flag == 0) {
        // 获取到了锁，将 guard 置为 0
17      m-&gt;flag = 1; // lock is acquired
18      m-&gt;guard = 0;
19  } else {
        // 未获取到锁，加入该锁的等待队列中
20      queue_add(m-&gt;q, gettid());
        // 将 guard 置为 0
21      m-&gt;guard = 0;
        // 将该线程休眠，等待唤醒
22      park();
23  }
24 }
25
26 void unlock(lock_t *m) {
    // 将 guard 设置为 1，如果已经为 1 自选等待
27  while (TestAndSet(&amp;m-&gt;guard, 1) == 1)
28      ; //acquire guard lock by spinning
29  if (queue_empty(m-&gt;q))
        // 如果该锁的等待队列为空，那就标记锁为空闲态
30      m-&gt;flag = 0; // let go of lock; no one wants it
31  else
        // 如果该锁的等待队列不为空，相应的出队一个线程，唤醒该线程去获取锁
32      unpark(queue_remove(m-&gt;q)); // hold lock
33                                  // (for next thread!)
    // 将 guard 设置为 0
34  m-&gt;guard = 0;
35 }

</code></pre>
<ul>
<li>在这个例子中，我们做了一些有趣的事情。首先，我们将旧的 test-and-set 思想与一个显式的锁等待队列相结合，以获得更高效的锁。其次，我们<strong>使用队列来帮助控制谁下一个获得锁，从而避免饥饿</strong>。</li>
<li>您可能会注意到 <strong>Guard 是如何使用的，它基本上是围绕着锁所使用的标志和队列操作的自旋锁</strong>。因此，<strong>这种方法不能完全避免旋转等待;一个线程可能在获取或释放锁时被中断，从而导致其他线程旋转——等待这个线程再次运行。然而，旋转所花费的时间非常有限</strong>(锁定和解锁代码中只有几条指令，而不是用户定义的关键部分)，因此这种方法可能是合理的。</li>
<li>您可能还会注意到，在lock()中，当线程无法获得锁(它已经被持有)时，我们会小心地将自己添加到队列中(通过调用gettid()函数来获取当前线程的线程ID)，将guard设置为0，并 yield CPU。给读者一个问题:如果在park()之后，而不是之前，释放守卫锁会发生什么?提示: something bad。（<strong>park之后设置guard的话，就会出现guard一直为1的情况，然后一直陷入自旋等待</strong>）</li>
<li>您还可以进一步检测到，当另一个线程被唤醒时，该标志没有被设回0。这是为什么呢?好吧，这不是错误，而是必须的!当一个线程被唤醒时，它将像从park()返回一样;然而，它在代码的那一点上并没有保持保护，因此甚至不能尝试将标志设置为1。因此，我们只需将释放锁的线程直接传递给获取锁的下一个线程;flag在两者之间没有设置为0</li>
<li>最后，您可能会注意到解决方案中在调用park()之前出现了可感知的竞争条件。在错误的时间（比如 park 调用之前），一个线程将会被暂停，假设它应该休眠，直到锁不再被持有。此时切换到另一个线程(例如，持有锁的线程)可能会导致问题，例如，如果该线程随后释放了锁。第一个线程之后的park 将永远休眠(潜在的)，这个问题有时被称为wakeup/waiting race。为了解决这个问题，需要做一些额外的工作。（<strong>其实就是在无限等待 park</strong>）</li>
<li>Solaris通过添加第三个<strong>系统调用 setpark() 解决了这个问题。通过调用这个例程，线程可以指示它将要 park。如果它恰好被中断了，刚好另一个线程被调度，并且另一个线程在实际调用 park 之前调用unpark，那么随后的 park 将立即返回，而不是休眠</strong>。lock() 内部的代码修改非常小</li>
</ul>
<pre><code class="language-C">1 queue_add(m-&gt;q, gettid());
2 setpark(); // new code
3 m-&gt;guard = 0;
</code></pre>
<ul>
<li>另一种解决方案可以将 guard 传递到内核中。在这种情况下，内核可以采取预防措施，原子地释放锁并使正在运行的线程退出队列。</li>
</ul>
<h3 id="different-os-different-support">Different OS, Different Support</h3>
<ul>
<li>到目前为止，我们已经看到了操作系统为了在线程库中构建更有效的锁而提供的一种支持。其他操作系统也提供了类似的支持;细节有所不同。</li>
<li>例如，Linux提供了一个futex，它类似于Solaris接口，但提供了更多的内核功能。具体来说，每个futex都有一个特定的物理内存位置，以及一个每个futex的内核队列。调用者可以根据需要使用futex调用(如下所述)来睡眠和唤醒。</li>
<li>具体来说，有两个调用可用。对 futex_wait(address, expected) 的调用将调用线程置于睡眠状态，假设 address 上的值等于 expected。如果不相等，则调用立即返回。对例程futex_wake(address) 的调用唤醒一个正在队列中等待的线程。这些调用在 Linux 互斥中的用法如下所示：</li>
</ul>
<pre><code class="language-C">1 void mutex_lock (int *mutex) {
2   int v;
3   /* Bit 31 was clear, we got the mutex (the fastpath) */
    // 自旋锁
4   if (atomic_bit_test_set (mutex, 31) == 0)
5       return;
    // 请求锁，相应的 mutex+1
6   atomic_increment (mutex);
7   while (1) {
        // 被unlock唤醒了！！获取锁然后维护等待队列长度
8       if (atomic_bit_test_set (mutex, 31) == 0) {
9           atomic_decrement (mutex);
10          return;
11      }
12      /* We have to waitFirst make sure the futex value
13         we are monitoring is truly negative (locked). */
14      v = *mutex;
        // 判断整数的正负，正则为被持有
15      if (v &gt;= 0)
16          continue;
        // 为负，即该锁将被持有，会有竞争发生，休眠线程
        // 原子性的检查 mutex 中计数器的值是否为val,如果是则让进程休眠，
        // 直到FUTEX_WAKE或者超时(time-out)。也就是把进程挂到 mutex 相对应的等待队列上去。
17      futex_wait (mutex, v);
18  }
19 }
20
21 void mutex_unlock (int *mutex) {
22  /* Adding 0x80000000 to counter results in 0 if and
23  only if there are not other interested threads */
    // 解锁，如果等待队列长度是0就不用唤醒！
    // 不把这个逻辑放futex_wake是为了减少sys call的开销。
24  if (atomic_add_zero (mutex, 0x80000000))
25      return;
26
27  /* There are other threads waiting for this mutex,
28     wake one of them up. */
    // 唤醒等待 mutex 的进程
29  futex_wake (mutex);
30 }
</code></pre>
<ul>
<li>这段来自nptl库(gnu libc库的一部分)中的 <code>lowlevellock.h</code> 的代码片段很有趣，原因有几个。首先，它使用单个整数来跟踪锁是否被持有(整数的高位)和锁上的等待者数量(所有其他位)。因此，如果锁是负的，它将被持有(因为设置了高位，该位决定了整数的符号)。</li>
<li>其次，代码片段展示了如何针对常见情况进行优化，特别是当没有争用锁的时候;只有一个线程获取和释放锁，完成的工作很少(获取锁时 TestAndSet 原子位运算，释放锁原子位加法)。</li>
<li>看看您能否解开这个“现实世界”锁的其余部分，以理解它是如何工作的。做到这一点，成为Linux锁的大师，或者至少是当一本书告诉你要做什么的时候倾听的人。</li>
</ul>
<h3 id="two-phase-locks">Two-Phase Locks</h3>
<ul>
<li>最后一点:Linux方法有一种老方法，这种方法已经断断续续地使用了多年，至少可以追溯到 Dahm 锁在1960年代早期，现在被称为两阶段锁。两阶段锁意识到自旋是有用的，特别是当锁即将被释放的时候。所以在<strong>第一阶段，锁会自旋一段时间，希望它能获得锁</strong>。</li>
<li>但是，<strong>如果在第一个旋转阶段没有获得锁，那么就会进入第二个阶段，在这个阶段中调用者被置于睡眠状态，直到后来锁被释放时才会被唤醒</strong>。上面的Linux锁就是这种锁的一种形式，但它只旋转一次;<strong>更常见的方式是在循环中自旋固定的次数，然后使用 futex 睡眠。</strong></li>
<li>两阶段锁是混合方法的另一个实例，在这种方法中，结合两个好的想法可能会产生一个更好的想法。当然，是否这样做很大程度上取决于许多因素，包括硬件环境、线程数量和其他工作负载细节。像往常一样，制作一个适用于所有可能用例的通用锁是一个很大的挑战。</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>上面的方法展示了目前如何构建真正的锁:一些硬件支持(以更强大的指令的形式)加上一些操作系统支持(例如，在Solaris上以park()和unpark()原语的形式，或在Linux上以futex的形式)。当然，细节是不同的，执行这种锁定的确切代码通常是经过高度调整的。如果您想了解更多细节，请查看Solaris或Linux代码库;它们是一本引人入胜的读物。也可以参阅David等人关于现代多处理器上的锁策略比较的优秀工作。</li>
</ul>
<h2 id="lock-based-concurrent-data-structures">Lock-based Concurrent Data Structures</h2>
<ul>
<li>对于特定数据结构，如何加锁才能让该结构功能正确？进一步，如何对该数据结构加锁，能够保证高性能，让许多线程同时访问该结构，即并发访问（concurrently）？</li>
<li>这里会简单介绍一些并发的数据结构，后期补充一个 <strong>并发跳表</strong>。</li>
</ul>
<h3 id="concurrent-counters">Concurrent Counters</h3>
<ul>
<li>如下先定义一个非并发的计数器，操作很简单，主要三种操作。+ / - / r</li>
</ul>
<pre><code class="language-C">1 typedef struct counter_t {
2   int value;
3 } counter_t;
4
5 void init(counter_t *c) {
6   c-&gt;value = 0;
7 }
8
9 void increment(counter_t *c) {
10  c-&gt;value++;
11 }
12
13 void decrement(counter_t *c) {
14  c-&gt;value--;
15 }
16
17 int get(counter_t *c) {
18  return c-&gt;value;
19 } 
</code></pre>
<ul>
<li>为了实现并发的效果，简单粗暴地加锁就完事了。其实很好理解，就是在相应的非线程安全函数执行之前对应的加锁和释放锁就 OK。显然简单粗暴多半就意味着性能是有一定问题的。</li>
</ul>
<pre><code class="language-C">1 typedef struct counter_t {
2   int value;
3   pthread_mutex_t lock;
4 } counter_t;
5
6 void init(counter_t *c) {
7   c-&gt;value = 0;
8   Pthread_mutex_init(&amp;c-&gt;lock, NULL);
9 }
10
11 void increment(counter_t *c) {
12  Pthread_mutex_lock(&amp;c-&gt;lock)
13  c-&gt;value++;
14  Pthread_mutex_unlock(&amp;c-&gt;lock);
15 }
16
17 void decrement(counter_t *c) {
18  Pthread_mutex_lock(&amp;c-&gt;lock);
19  c-&gt;value--;
20  Pthread_mutex_unlock(&amp;c-&gt;lock);
21 }
22
23 int get(counter_t *c) {
24  Pthread_mutex_lock(&amp;c-&gt;lock);
25  int rc = c-&gt;value;
26  Pthread_mutex_unlock(&amp;c-&gt;lock);
27  return rc;
28 } 

</code></pre>
<ul>
<li>为了理解简单方法的性能代价，我们运行了一个基准测试，其中每个线程更新一个共享计数器固定次数;然后我们改变线程的数量。下图显示了在一到四个线程处于活动状态时所花费的总时间;每个线程更新计数器一百万次。这个实验是在装有四个Intel 2.7 GHz i5 cpu的iMac上进行的;随着更多的cpu处于活动状态，我们希望在单位时间内完成更多的工作</li>
<li>从图上方的曲线（标为“Precise”）可以看出，同步的计数器扩展性不好。单线程完成 100 万次更新只需要很短的时间（大约 0.03s），而两个线程并发执行，每个更新 100 万次，性能下降很多（超过 5s！）。线程更多时，性能更差。</li>
<li>理想情况下，你会看到多处理上运行的多线程就像单线程一样快。达到这种状态称为完美扩展（perfect scaling）。虽然总工作量增多，但是并行执行后，完成任务的时间并没有增加。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210404221747.png" alt="20210404221747" loading="lazy"></li>
</ul>
<h4 id="scalable-counting">Scalable Counting</h4>
<ul>
<li>令人吃惊的是，关于如何实现可扩展的计数器，研究人员已经研究了多年。更令人吃惊的是，最近的操作系统性能分析研究表明，可扩展的计数器很重要。没有可扩展的计数，一些运行在 Linux 上的工作在多核机器上将遇到严重的扩展性问题。</li>
<li>尽管人们已经开发了多种技术来解决这一问题，我们将介绍一种特定的方法。这个方法是最近的研究提出的，称为懒惰计数器（sloppy counter）</li>
<li>懒惰计数器通过多个局部计数器和一个全局计数器来实现一个逻辑计数器，其中每个CPU 核心有一个局部计数器。具体来说，在 4 个 CPU 的机器上，有 4 个局部计数器和 1 个全局计数器。除了这些计数器，还有锁：每个局部计数器有一个锁，全局计数器有一个。</li>
<li>懒惰计数器的基本思想是这样的。如果一个核心上的线程<strong>想增加计数器，那就增加它的局部计数器，访问这个局部计数器是通过对应的局部锁同步的</strong>。因为每个 CPU 有自己的局部计数器，不同 CPU 上的线程不会竞争，所以计数器的更新操作可扩展性好。但是，<strong>为了保持全局计数器更新（以防某个线程要读取该值），局部值会定期转移给全局计数器，方法是获取全局锁，让全局计数器加上局部计数器的值，然后将局部计数器置零</strong>。</li>
<li>这种局部转全局的频度，取决于一个阈值，这里称为 S（表示 sloppiness）。S 越小，懒惰计数器则越趋近于非扩展的计数器。S 越大，扩展性越强，但是全局计数器与实际计数的偏差越大。我们可以抢占所有的局部锁和全局锁（以特定的顺序，避免死锁），以获得精确值，但这种方法没有扩展性。</li>
<li>为了弄清楚这一点，来看一个例子（见表）。在这个例子中，阈值 S 设置为 5，4 个 CPU 上分别有一个线程更新局部计数器 L1,…, L4。随着时间增加，全局计数器 G 的值也会记录下来。每一段时间，局部计数器可能会增加。如果局部计数值增加到阈值 S，就把局部值转移到全局计数器，局部计数器清零。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405134342.png" alt="20210405134342" loading="lazy"></li>
<li>上面的曲线图中接近 X 轴的是阈值 S 为 1024 时懒惰计数器的性能。性能很高，4 个处理器更新 400 万次的时间和一个处理器更新 100 万次的几乎一样。</li>
<li>下图展示了了阈值 S 的重要性，在 4 个 CPU 上的 4 个线程，分别增加计数器 100 万次。如果 S 小，性能很差（但是全局计数器精确度高）。如果 S 大，性能很好，但是全局计数器会有延时。懒惰计数器就是在准确性和性能之间折中。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405153232.png" alt="20210405153232" loading="lazy"></li>
<li>下面代码就是懒惰计数器的基本实现。</li>
</ul>
<pre><code class="language-C">1 typedef struct counter_t {
2   int global; // global count
3   pthread_mutex_t glock; // global lock
4   int local[NUMCPUS]; // local count (per cpu)
5   pthread_mutex_t llock[NUMCPUS]; // ... and locks
6   int threshold; // update frequency
7 } counter_t;
8
9  // init: record threshold, init locks, init values
10 // of all local counts and global count
11 void init(counter_t *c, int threshold) {
12  c-&gt;threshold = threshold;
13
14  c-&gt;global = 0;
15  pthread_mutex_init(&amp;c-&gt;glock, NULL);
16
17  int i;
18  for (i = 0; i &lt; NUMCPUS; i++) {
19      c-&gt;local[i] = 0; 
20      pthread_mutex_init(&amp;c-&gt;llock[i], NULL);
21  }
22 }
23
24 // update: usually, just grab local lock and update local amount
25 // once local count has risen by 'threshold', grab global
26 // lock and transfer local values to it
27 void update(counter_t *c, int threadID, int amt) {
28  pthread_mutex_lock(&amp;c-&gt;llock[threadID]);
29  c-&gt;local[threadID] += amt; // assumes amt &gt; 0
30  if (c-&gt;local[threadID] &gt;= c-&gt;threshold) { // transfer to global
31      pthread_mutex_lock(&amp;c-&gt;glock);
32      c-&gt;global += c-&gt;local[threadID];
33      pthread_mutex_unlock(&amp;c-&gt;glock);
34      c-&gt;local[threadID] = 0;
35  }
36  pthread_mutex_unlock(&amp;c-&gt;llock[threadID]);
37 }
38
39 // get: just return global amount (which may not be perfect)
40 int get(counter_t *c) {
41  pthread_mutex_lock(&amp;c-&gt;glock);
42  int val = c-&gt;global;
43  pthread_mutex_unlock(&amp;c-&gt;glock);
44  return val; // only approximate!
45 } 
</code></pre>
<h3 id="concurrent-linked-lists">Concurrent Linked Lists</h3>
<ul>
<li>接下来看一个更复杂的数据结构，链表。同样，我们从一个基础实现开始。简单起见，<br>
我们只关注链表的插入操作，其他操作比如查找、删除等就交给读者了。</li>
</ul>
<pre><code class="language-C">1 // basic node structure
2 typedef struct node_t {
3   int key;
4   struct node_t *next;
5 } node_t;
6
7 // basic list structure (one used per list)
8 typedef struct list_t {
9   node_t *head;
10  pthread_mutex_t lock;
11 } list_t;
12
13 void List_Init(list_t *L) {
14  L-&gt;head = NULL; 
15  pthread_mutex_init(&amp;L-&gt;lock, NULL);
16 }
17  
    // 从链表头插入
18 int List_Insert(list_t *L, int key) {
19  pthread_mutex_lock(&amp;L-&gt;lock);
20  node_t *new = malloc(sizeof(node_t));
21  if (new == NULL) {
22      perror(&quot;malloc&quot;);
23      pthread_mutex_unlock(&amp;L-&gt;lock);
24      return -1; // fail
25  }
26  new-&gt;key = key;
27  new-&gt;next = L-&gt;head;
28  L-&gt;head = new;
29  pthread_mutex_unlock(&amp;L-&gt;lock);
30  return 0; // success
31 }
32
33 int List_Lookup(list_t *L, int key) {
34  pthread_mutex_lock(&amp;L-&gt;lock);
35  node_t *curr = L-&gt;head;
36  while (curr) {
37      if (curr-&gt;key == key) {
38          pthread_mutex_unlock(&amp;L-&gt;lock);
39          return 0; // success
40      }
41      curr = curr-&gt;next;
42  }
43  pthread_mutex_unlock(&amp;L-&gt;lock);
44  return -1; // failure
45 }
</code></pre>
<ul>
<li>从代码中可以看出，代码插入函数入口处获取锁，结束时释放锁。如果 malloc 失败（在极少的时候），会有一点小问题，在这种情况下，代码在插入失败之前，必须释放锁。事实表明，这种异常控制流容易产生错误。最近一个 Linux 内核补丁的研究表明，有40%都是这种很少发生的代码路径（实际上，这个发现启发了我们自己的一些研究，我们从 Linux 文件系统中移除了所有内存失败的路径，得到了更健壮的系统）。</li>
<li>因此，挑战来了：我们能够重写插入和查找函数，保持并发插入正确，但避免在失败情况下也需要调用释放锁吗？</li>
<li>在这个例子中，答案是可以。具体来说，我们调整代码，<strong>让获取锁和释放锁只环绕插入代码的真正临界区。</strong> 前面的方法有效是因为部分工作实际上不需要锁，<strong>假定 malloc()是线程安全的，每个线程都可以调用它，不需要担心竞争条件和其他并发缺陷。只有在更新共享列表时需要持有锁</strong>。</li>
</ul>
<pre><code class="language-C">1 void List_Init(list_t *L) {
2   L-&gt;head = NULL;
3   pthread_mutex_init(&amp;L-&gt;lock, NULL);
4 }
5
6 void List_Insert(list_t *L, int key) {
7   // synchronization not needed
8   node_t *new = malloc(sizeof(node_t));
9   if (new == NULL) {
10      perror(&quot;malloc&quot;);
11      return;
12  }
13  new-&gt;key = key;
14
15  // just lock critical section
16  pthread_mutex_lock(&amp;L-&gt;lock);
17  new-&gt;next = L-&gt;head;
18  L-&gt;head = new;
19  pthread_mutex_unlock(&amp;L-&gt;lock);
20 }
21
22 int List_Lookup(list_t *L, int key) {
23  int rv = -1;
24  pthread_mutex_lock(&amp;L-&gt;lock);
25  node_t *curr = L-&gt;head;
26  while (curr) {
27      if (curr-&gt;key == key) {
28          rv = 0;
29          break;
30      }
31      curr = curr-&gt;next;
32  }
33  pthread_mutex_unlock(&amp;L-&gt;lock);
34  return rv; // now both success and failure
35 } 
</code></pre>
<h4 id="scaling-linked-lists">Scaling Linked Lists</h4>
<ul>
<li>尽管我们有了基本的并发链表，但又遇到了这个链表扩展性不好的问题。研究人员发现的增加链表并发的技术中，有一种叫作过手锁（hand-over-hand locking，也叫作锁耦合，lock coupling）。</li>
<li>原理也很简单。每个节点都有一个锁，替代之前整个链表一个锁。遍历链表的时候，首先抢占下一个节点的锁，然后释放当前节点的锁。</li>
<li>从概念上说，过手锁链表有点道理，它增加了链表操作的并发程度。但是实际上，在遍历的时候，每个节点获取锁、释放锁的开销巨大，很难比单锁的方法快。即使有大量的线程和很大的链表，这种并发的方案也不一定会比单锁的方案快。也许某种杂合的方案（一定数量的节点用一个锁）值得去研究。</li>
</ul>
<blockquote>
<ul>
<li>如果方案带来了大量的开销（例如，频繁地获取锁、释放锁），那么高并发就没有什么意义。如果简单的方案很少用到高开销的调用，通常会很有效。增加更多的锁和复杂性可能会适得其反。话虽如此，有一种办法可以获得真知：实现两种方案（简单但少一点并发，复杂但多一点并发），测试它们的表现。毕竟，你不能在性能上作弊。结果要么更快，要么不快。</li>
<li>有一个通用建议，对并发代码和其他代码都有用，即注意控制流的变化导致函数返回和退出，或其他错误情况导致函数停止执行。因为很多函数开始就会获得锁，分配内存，或者进行其他一些改变状态的操作，如果错误发生，代码需要在返回前恢复各种状态，这容易出错。因此，最好组织好代码，减少这种模式。</li>
</ul>
</blockquote>
<h3 id="concurrent-queues">Concurrent Queues</h3>
<ul>
<li>你现在知道了，总有一个标准的方法来创建一个并发数据结构：添加一把大锁。对于一个队列，我们将跳过这种方法。我们来看看 Michael 和 Scott 设计的、更并发的队列。</li>
<li>仔细研究这段代码，你会发现有两个锁，一个负责队列头，另一个负责队列尾。这两个锁使得入队列操作和出队列操作可以并发执行，因为入队列只访问 tail 锁，而出队列只访问  head 锁。</li>
<li>Michael 和 Scott 使用了一个技巧，添加了一个假节点（在队列初始化的代码里分配的）。该假节点分开了头和尾操作。研究这段代码，或者输入、运行、测试它，以便更深入地理解它。</li>
<li>队列在多线程程序里广泛使用。然而，这里的队列（只是加了锁）通常不能完全满足这种程序的需求。更完善的有界队列，在队列空或者满时，能让线程等待。这是下一章探讨条件变量时集中研究的主题。读者需要看仔细了！</li>
</ul>
<pre><code class="language-C">1 typedef struct __node_t {
2   int value;
3   struct __node_t *next;
4 } node_t;
5
6 typedef struct __queue_t {
7   node_t *head;
8   node_t *tail;
9   pthread_mutex_t head_lock, tail_lock;
10 } queue_t;
11
12 void Queue_Init(queue_t *q) {
13  node_t *tmp = malloc(sizeof(node_t));
14  tmp-&gt;next = NULL;
15  q-&gt;head = q-&gt;tail = tmp;
16  pthread_mutex_init(&amp;q-&gt;head_lock, NULL);
17  pthread_mutex_init(&amp;q-&gt;tail_lock, NULL);
18 }
19
20 void Queue_Enqueue(queue_t *q, int value) {
21  node_t *tmp = malloc(sizeof(node_t));
22  assert(tmp != NULL);
23  tmp-&gt;value = value;
24  tmp-&gt;next = NULL;
25
26  pthread_mutex_lock(&amp;q-&gt;tail_lock);
27  q-&gt;tail-&gt;next = tmp;
28  q-&gt;tail = tmp;
29  pthread_mutex_unlock(&amp;q-&gt;tail_lock);
30 }
31
32 int Queue_Dequeue(queue_t *q, int *value) {
33  pthread_mutex_lock(&amp;q-&gt;head_lock);
34  node_t *tmp = q-&gt;head;
35  node_t *new_head = tmp-&gt;next;
36  if (new_head == NULL) {
37      pthread_mutex_unlock(&amp;q-&gt;head_lock);
38      return -1; // queue was empty
39  }
40  *value = new_head-&gt;value;
41  q-&gt;head = new_head;
42  pthread_mutex_unlock(&amp;q-&gt;head_lock);
43  free(tmp);
44  return 0;
45 }

</code></pre>
<h3 id="concurrent-hash-table">Concurrent Hash Table</h3>
<ul>
<li>我们讨论最后一个应用广泛的并发数据结构，散列表。我们只关注不需要调整大小的简单散列表。支持调整大小还需要一些工作，留给读者作为练习。</li>
<li>本例的散列表使用我们之前实现的并发链表，性能特别好。每个散列桶（每个桶都是一个链表）都有一个锁，而不是整个散列表只有一个锁，从而支持许多并发操作。</li>
</ul>
<pre><code class="language-C">1 #define BUCKETS (101)
2
3 typedef struct __hash_t {
4   list_t lists[BUCKETS];
5 } hash_t;
6
7 void Hash_Init(hash_t *H) {
8   int i;
9   for (i = 0; i &lt; BUCKETS; i++)
10      List_Init(&amp;H-&gt;lists[i]);
11 }
12
13 int Hash_Insert(hash_t *H, int key) {
14  return List_Insert(&amp;H-&gt;lists[key % BUCKETS], key);
15 }
16
17 int Hash_Lookup(hash_t *H, int key) {
18  return List_Lookup(&amp;H-&gt;lists[key % BUCKETS], key);
19 }

</code></pre>
<ul>
<li>下图展示了并发更新下的散列表的性能（同样在 4 CPU 的 iMac，4 个线程，每个线程分别执行 1 万～5 万次并发更新）。同时，作为比较，我们也展示了单锁链表的性能。可以看出，这个简单的并发散列表扩展性极好，而链表则相反。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405160416.png" alt="20210405160416" loading="lazy"></li>
</ul>
<blockquote>
<ul>
<li><strong>建议：避免不成熟的优化（Knuth 定律）</strong></li>
<li>实现并发数据结构时，先从最简单的方案开始，也就是加一把大锁来同步。这样做，你很可能构建了正确的锁。如果发现性能问题，那么就改进方法，只要优化到满足需要即可。正如 Knuth 的著名说法“不成熟的优化是所有坏事的根源。”</li>
<li>许多操作系统，在最初过渡到多处理器时都是用一把大锁，包括 Sun 和 Linux。在 Linux 中，这个锁甚至有个名字，叫作 BKL（大内核锁，big kernel lock）。这个方案在很多年里都很有效，直到多 CPU 系统普及，内核只允许一个线程活动成为性能瓶颈。终于到了为这些系统优化并发性能的时候了。Linux 采用了简单的方案，把一个锁换成多个。Sun 则更为激进，实现了一个最开始就能并发的新系统，Solaris。读者可以通过 Linux 和 Solaris 的内核资料了解更多信息。</li>
</ul>
</blockquote>
<h3 id="summary-2">Summary</h3>
<ul>
<li>我们已经介绍了一些并发数据结构，从计数器到链表队列，最后到大量使用的散列表。同时，我们也学习到：控制流变化时注意获取锁和释放锁；增加并发不一定能提高性能；有性能问题的时候再做优化。关于最后一点，避免不成熟的优化（premature optimization），对于所有关心性能的开发者都有用。我们让整个应用的某一小部分变快，却没有提高整体性能，其实没有价值。</li>
<li>当然，我们只触及了高性能数据结构的皮毛。Moir 和 Shavit 的调查提供了更多信息，包括指向其他来源的链接。特别是，你可能会对其他结构感兴趣（比如 B 树），那么数据库课程会是一个不错的选择。你也可能对根本不用传统锁的技术感兴趣。这种非阻塞数据结构是有意义的，在常见并发问题的章节中，我们会稍稍涉及。但老实说这是一个广泛领域的知识，远非本书所能覆盖。感兴趣的读者可以自行研究。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Series One of Basic of Concurrency - Concurrency and Threads]]></title>
        <id>https://blog.shunzi.tech/post/basic-of-concurrency-three/</id>
        <link href="https://blog.shunzi.tech/post/basic-of-concurrency-three/">
        </link>
        <updated>2021-03-28T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第一篇（Concurrency and Threads），并发和线程。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第一篇（Concurrency and Threads），并发和线程。</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="chapter-index">Chapter Index</h2>
<ul>
<li><a href="">Series One of Basic of Concurrency - Concurrency and Threads</a></li>
<li><a href="../lock/">Series Two of Basic of Concurrency - Lock</a></li>
<li><a href="">Series Three of Basic of Concurrency - Condition Variables</a></li>
<li><a href="">Series Four of Basic of Concurrency - Semaphores</a></li>
<li><a href="">Series Five of Basic of Concurrency - Bugs and Event-Based Concurrency</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage]]></title>
        <id>https://blog.shunzi.tech/post/SpanDB/</id>
        <link href="https://blog.shunzi.tech/post/SpanDB/">
        </link>
        <updated>2021-03-11T12:07:09.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>FAST 2021 的文章《SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage》</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>FAST 2021 的文章《SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage》</li>
</ul>
</blockquote>
<!--more-->
<ul>
<li>SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage</li>
<li><a href="https://blog.shunzi.tech/post/SpanDB/">Elvis Zhang's Blog</a></li>
</ul>
<h2 id="abstract">Abstract</h2>
<ul>
<li>键值(KV)存储支持许多关键的应用程序和服务。它们在内存中执行的速度很快，但仍然经常受到I/O性能的限制。最近出现的高速商业NVMe ssd推动了利用其超低延迟和高带宽优势的新kv系统设计。与此同时，转换到全新的数据布局并将整个数据库扩展到高端 SSD 需要大量的投资。</li>
<li>作为一种折衷方案，我们提出了SpanDB，一种基于 LSM 树的 KV 存储，适应流行的RocksDB 系统，利用高速 SSD 选择性部署。SpanDB 允许用户将大量数据存储在更便宜、更大的SSD上，同时将预写日志(WAL)和 LSM 树的顶层重新定位到更小、更快的NVMe SSD上。为了更好地利用这个快速磁盘，SpanDB通过SPDK提供了高速、并行的WAL写入，并支持异步请求处理，以减轻线程间同步开销，并有效地使用基于轮询的I/O工作。我们的测试结果显示，SpanDB 同时将 RocksDB 的吞吐量提高了8.8x，并将延迟降低了9.5% - 58.3%。与专为高端 SSD 设计的 KVell 系统相比，SpanDB的吞吐量达到 96-140%，延迟 2.3-21.6x 更低，存储配置更便宜。</li>
<li><a href="https://github.com/SpanDB/SpanDB">Github Repo</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>LSM 树应用广泛，它仍然具有吸引力，因为生产KV环境经常被发现是写密集型的，特别是由于积极的内存缓存。正如最近的系统所证明的那样，最近快速的、商业NVMe SSD可以显著提高KV性能，如 KVell、KVSSD，通过丢弃为硬盘设计的LSM-tree数据结构或将KV数据管理卸载到专门的硬件，这些系统提供了高吞吐量和可伸缩性，整个数据集托管在高端设备上。</li>
<li>相反，这项工作的目标是将主流基于lsm树的KV设计适应于快速NVMe ssd和I/O接口，并特别关注在生产环境中具有成本效益的部署，因为我们发现当前基于 LSM 树的 KV 存储未能充分挖掘 NVMe SSD 的潜力（对比 NVMe SSD 和 SATA SSD），特别是，常见的KV存储设计的I/O路径严重地未充分利用超低延迟NVMe ssd，特别是对于小的写操作（对比 ext4 和 SPDK）</li>
<li>这对写前日志记录(WAL)尤为不利，WAL 它对数据持久性和事务原子性至关重要，位于写的关键路径上，是主要瓶颈。其次，现有的KV请求处理采用慢速设备，如果切换到快速的基于轮询的I/O，工作流设计将引入高软件开销或浪费CPU周期</li>
<li>此外，新的NVMe接口附带了访问约束(例如要求绑定整个设备以进行SPDK访问，或者建议将线程固定在core上)。这使得使用高端 SSD 来处理不同类型的KV I/O的KV设计变得复杂，并降低了当前的常见实践(如同步请求处理)的效率。</li>
<li>像Optane这样的顶级ssd对于大规模部署来说是昂贵的。由于大型的、写密集型的KV存储不可避免地拥有大量的冷数据，在这些相对较小和昂贵的设备上托管所有数据可能超出用户或云数据库服务提供商的预算。</li>
<li>针对这些挑战，我们提出了SpanDB，一个基于LSM tree的KV系统，采用高端NVMe ssd进行部分部署，对使用 SPDK I/O 的 KV 系统的瓶颈和挑战进行了全面分析。
<ul>
<li>它通过合并一个相对较小但速度较快的磁盘(SD)来扩大对最新数据的所有读写处理，同时将数据存储扩展到一个或多个更大、更便宜的磁盘(CD)上。</li>
<li>它可以通过SPDK实现快速并行访问，从而更好地利用SD，绕过Linux I/O堆栈，特别是允许高速WAL写入。(据作者描述，这是研究KV存储的SPDK支持的第一个工作。)</li>
<li>它设计了一个适合基于轮询的I/O的异步请求处理管道，它消除了不必要的同步，积极地将I/O等待与内存中处理重叠，并自适应地协调前台/后台I/O。</li>
<li>它根据实际的KV工作负载有策略地自适应地划分数据，积极地将CD的I/O资源，特别是带宽纳入其中，以帮助共享当代KV系统中常见的写放大</li>
</ul>
</li>
<li>SpanDB重新设计了RocksDB的KV请求处理、存储管理和组WAL写，利用快速SPDK接口，保留了RocksDB的lsm树组织、后台I/O机制、事务支持等数据结构和算法。 因此，它的设计与RocksDB的许多其他优化方案是互补的。添加SD后，可以将现有的RocksDB数据库迁移到SpanDB</li>
<li>我们使用YCSB和LinkBench进行的评估显示，SpanDB在所有测试用例中(吞吐量、平均延迟和尾部延迟)的性能都显著优于RocksDB，特别是在写密集型测试用例中。相对于最近设计用来利用高端ssd的KVell系统，SpanDB在大多数情况下提供了更高的吞吐量(只占KVell延迟的一小部分)，而不牺牲事务支持。</li>
</ul>
<h2 id="background-motivation">Background &amp; Motivation</h2>
<h3 id="lsm-tree-based-kv-stores">LSM-tree based KV Stores</h3>
<ul>
<li>整体架构不再赘述。</li>
<li>几个观点：
<ul>
<li>WAL 仍然是面向客户的数据库的一个组成部分，并且位于处理写请求的关键路径上</li>
<li>读操作可能会在 LSM 的很多层上产生随机数据访问，尽管有很多系统采用内存缓存来优化读性能，但是对于数据量较大的情况或者局部性较差的情况还是会产生 I/O 操作，严重增加了尾延迟，影响整体性能。</li>
<li>flush 和 compaction 操作都会产生较大的顺序 I/O，它对前台请求处理的影响体现在 I/O 争用和写停滞(当用户写需要等待刷新以清空MemTable空间时)。</li>
<li>RocksDB和LevelDB通过用户可配置的flush/compaction线程数量来控制后台I/O速率，当有后台I/O任务时，它们被激活，否则休眠。研究人员已经注意到后台线程设置对性能的影响，并提出了相关的优化 （SILK）。然而，现有的解决方案仍然保留了后台线程设计，假定I/O速度较慢和基于中断的同步，这在新的基于轮询的I/O接口(将在下面讨论)中不能很好地工作。</li>
</ul>
</li>
</ul>
<h3 id="group-wal-writes">Group WAL Writes</h3>
<ul>
<li>
<p>当前写 WAL 的常见做法是分组日志记录(group logging)，即对一次日志数据写入批量处理多个写请求。在很多常见的数据库系统中都使用了该技术，除了容错之外，组日志还通过促进顺序写操作，在速度较慢的存储设备(其中随机访问往往更慢)上提供了更好的写性能。</p>
</li>
<li>
<p>图 1 展示了批量写 WAL 的流程。WAL 的写入过程是顺序的，任何时候最多只有一个 group 在写 log。</p>
<ul>
<li>当有一个正在进行中的写操作时，处理写请求的 workers 线程通过加入一个共享队列组成一个新的组，第一个进入队列的线程被指定为组的 leader。图示 1 - 3</li>
<li>leader 从 peers 中收集日志项，直到前一个组的 leader (刚刚完成写入)通知它继续执行。这将关闭当前组的大门，随后到达的线程将启动一个新的组。</li>
<li>leader 以单个同步 I/O 步骤(使用fsync/fdatasync, 4)将日志条目写入持久存储。然后，leader 唤醒组成员，启动 MemTables 中的更新，使这样的写入对后续的请求可见 5 - 6</li>
<li>通过将最后一个可见序列推进到其条目中的最新序列号来完成组提交 7</li>
<li>解散组 8，然后唤醒下一个 leader<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301214350.png" alt="20210301214350" loading="lazy"></li>
</ul>
</li>
<li>
<p>使用高端 NVMe SSD 的话，上图中的步骤 4 的时间可以大幅减少。与此同时批处理写操作合并了小 IO。因此，同步组日志导致的软件开销增加，导致大多数线程将时间浪费在不同类型的等待上 （步骤 1-3 和 7）</p>
</li>
<li>
<p>例如，我们测量了RocksDB在通过ext4访问的SATA SSD上的这4个步骤上平均花费了68.1%的写请求处理时间，而在Optane上通过 SPDK 则增长到了81.0%。（因为总花费的时间减少了，等待的时间所占据的比重就增加了）</p>
</li>
<li>
<p><strong>如果上面的 Group Logging 流程不够清晰，可以看下面这个图</strong></p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/161797527">图源：知乎 bluesky - RocksDB 写入流程</a><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210311152453.png" alt="20210311152453" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="high-performance-ssds-interfaces">High-Performance SSDs Interfaces</h3>
<ul>
<li>高性能 SSD Intel Optane、Toshiba XL-Flash、Samsung Z-SSD 提供了更低的延迟和更高的吞吐量。考虑到Linux内核I/O堆栈开销在总I/O延迟中不再是可以忽略的，Intel 开发了 SPDK，一组用户态的库/工具来访问高速的 NVMe 设备。<strong>SPDK 把驱动移动到了用户空间，避免了系统调用并支持了零拷贝访问。它轮询硬件完成，而不是使用中断，并避免I/O路径中的锁</strong>。在这里我们总结了在本研究中发现的与KV存储相关的SPDK性能行为和策略限制</li>
</ul>
<h4 id="spdk-整体性能">SPDK 整体性能</h4>
<ul>
<li>Intel Optane P4800X and P4610</li>
<li>使用不同类型和不同大小的 IO 组合来模拟 I/O，下图展示了 P4800X 的结果，P4160 展示的测试结果趋势类似。ext4 使用了 write 调用，每一个都进行了 fdatasync，SPDK 使用了内建的 perf tool (spdk_nvme_perf)</li>
<li>结果表明：（<strong>此处无随机写测试</strong>）
<ul>
<li><strong>顺序读</strong>下 ext4 和 SPDK 接近</li>
<li>通过ext4进行的4KB<strong>顺序写</strong>操作(WAL-style)只实现了硬件潜力的一小部分，延迟时间比SPDK高4.05x(IOPS相应降低)。</li>
<li>4KB的<strong>随机读</strong>和64KB的<strong>顺序写</strong>测试可以看到这两个极端之间的ext4-SPDK差距</li>
</ul>
</li>
<li>这些结果突出表明，SPDK可以显著改善KV I/O，特别是日志和写密集型工作负载<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301220514.png" alt="20210301220514" loading="lazy"></li>
</ul>
<h4 id="spdk-concurrency">SPDK concurrency</h4>
<ul>
<li>为了评估SPDK服务并发顺序写的能力，我们分析了各个SPDK请求，并发现7-8µs的单线程延迟中的大部分确实被busy-wait占用，由于争用下的I/O变慢，busy-wait随着并发写的线程增多而增加。</li>
<li>然后我们设计了一个 pipeline 方案，其中每个线程管理多个并发的SPDK请求。它允许“窃取”I/O等待时间来发出新请求，并检查未完成请求的完成状态(每个请求的完成时间低于1µs)。</li>
<li>图3 给出了Intel上的延迟和吞吐量结果P4610 (N)和Intel Optane (O) ssd硬盘。我们改变线程的数量(“3-N”有3个线程写入SSD N)和每个线程并发请求的上限(“CR=2”，每个线程最多发出2个请求)。NVMe ssd提供了目前RocksDB/LevelDB single-WAL-writer设计所没有的并行性。特别是，Optane (O)显示出比P4610 (N)更高的并发性，在更多的写入的情况下，具有更低的延迟和更快的吞吐量增长。然而，即使使用O，超过3个并发写入也不能提供更高的SPDK IOPS:使用3个loggers，每个CR=3似乎可以提供峰值的WAL速度，我们表示为3L3R。另一方面，N在2L4R处饱和。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301220356.png" alt="20210301220356" loading="lazy"></li>
</ul>
<h4 id="spdk-access-restrictions">SPDK access restrictions</h4>
<ul>
<li>快速使用spdk访问高端NVMe ssd带来的性能好处体现在：
<ul>
<li>一旦某个SSD被一个进程绑定到SPDK上，其他进程(无论是通过SPDK还是通过Linux I/O堆栈)都无法访问它。这简化了与用户级访问相关的工作负载间隔离，但也禁止将文件系统部分部署到通过SPDK访问的SSD上。</li>
<li>此外，建议用户将spdk access线程绑定到特定的内核。我们验证了不这样做会带来显著的I/O性能损失。这一点，再加上基于轮询的I/O模式，使得使用后台刷新/压缩线程的常见做法不适合SPDK访问:未绑定线程的I/O很慢，而绑定线程在空闲时不能轻松释放核心资源</li>
</ul>
</li>
</ul>
<h2 id="spandb-overview">SpanDB Overview</h2>
<h3 id="design-rationale">Design rationale</h3>
<ul>
<li>我们提出了SpanDB，一个高性能，经济有效的基于lsm树的KV存储使用异构存储设备。SpanDB提倡使用小型、快速且通常更昂贵的NVMe SSD作为快速磁盘(SD)，同时部署更大、更慢、更便宜的SSD(或此类设备的阵列)作为容量磁盘(CD)。SpanDB使用SD有两个目的:<strong>(1)对WAL进行写操作(2)存储RocksDB的LSM-tree的顶层</strong>。</li>
<li>由于WAL的处理成本是用户可见的，并且直接影响延迟，我们预留了足够的资源(内核和并发的SPDK请求，以及足够的SPDK队列)，以最大化其性能。同时，WAL数据只需要维护到相应的flush操作，通常需要GBs的空间，而今天的“小型”高端ssd，如Optane，提供超过300GB的空间，这促使SpanDB将RocksDB的LSM-tree的顶层移至SD。这还会从CD上卸载大量的刷新/压缩流量，CD上驻留着大量较冷的数据</li>
</ul>
<h3 id="spandb-architecture">SpanDB architecture</h3>
<ul>
<li>下图展示了 SpanDB 的架构。在 DRAM 中保留了 RocksDB Memtable 的相关设计，使用一个可变和多个不可变memtable。SpanDB 几乎不会对 RocksDB 的 KV 数据结构、算法或者操作语义进行修改。这里的主要区别在于它的异步处理模型，以减少同步开销和自适应调度任务。</li>
<li>磁盘上的数据分布在 CD 和 SD 上，两个物理存储分区。
<ul>
<li>SD 进一步被分区，一个小部分为 WAL 区域，另一个部分为数据区域。SpanDB 通过 SPDK 将 SD 当作裸设备管理并重新设计了 RocksDB 的 WAL 批量写入，以实现快速、并行的日志记录，将日志记录带宽提高 10x。为了最小化 RocksDB 的修改，SpanDB 实现了一个轻量级的自带缓存的文件系统 <em>TopFS</em>，允许简单和动态的 LSM levels 重定位。</li>
<li>CD 分区同时存储了 tree stump，经常包含较冷的大部分数据。SpanDB 的管理还是与RocksDB一样，通过文件系统访问，并借助操作系统页面缓存进行辅助。</li>
</ul>
</li>
<li>下图还对 SpanDB 不同类型的流量进行了区分。WAL 区域只进行 log 的操作，SD 的数据区域接受所有 flush 操作。此外，SD 数据区和 CD 都可以容纳用户读和压缩读/写，SpanDB 执行额外的优化，以支持在两个分区上同时进行压缩，并自动协调前台/后台任务。最后，SpanDB 能够基于对两个分区的实时带宽监控实现动态 tree level 布局。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301222754.png" alt="20210301222754" loading="lazy"></li>
</ul>
<h3 id="sources-of-performance-benefits">Sources of performance benefits</h3>
<ul>
<li>通过采用一个通过SPDK访问的小而快的SD，它加快了WAL的速度，并行WAL写入。</li>
<li>通过将SD也用于数据存储，它优化了这种快速ssd的带宽利用率</li>
<li>通过启用工作负载自适应的SD-CD数据分发，它积极地聚合设备间可用的I/O资源(而不是仅仅将CD用作“溢出层”)。</li>
<li>虽然主要是通过将I/O卸载到SD来优化写，但它减少了读密集型工作负载的尾部延迟</li>
<li>通过减少同步和主动平衡前台/后台I/O需求，它利用了快速轮询I/O，同时节省CPU资源</li>
</ul>
<h3 id="limitations">Limitations</h3>
<ul>
<li>由于前面提到的SPDK访问约束，SD需要绑定到一个进程，这使得共享资源 SD 变得困难</li>
<li>对于全读的工作负载，SpanDB在异步处理中产生的速度提升很小，并带来轻微的开销。</li>
</ul>
<h2 id="design-and-implementation">Design and Implementation</h2>
<h3 id="asynchronous-request-processing">Asynchronous Request Processing</h3>
<ul>
<li>很多 KV 系统都使用了嵌入式的 DB 处理，所有前端任务都被假设成客户端，每次同步处理一个KV请求。这样的处理通常是 I/O 限制(特别是WAL写)，用户通常通过线程过度配置获得更高的总体吞吐量(每秒请求数)，拥有比 cores 更多的客户端线程。具有快速NVMe ssd和SPDK等接口，线程同步(比如睡眠和唤醒)很容易花费比 I/O 请求更长的时间。在这种情况下，线程过度配置不仅增加了延迟，还会降低总体资源利用率，从而降低吞吐量。</li>
<li>此外，使用基于轮询的SPDK I/O，让线程在相同的 cores 上共存就失去了在 I/O 等待期间提高 CPU 利用率的吸引力。这也适用于使用后台线程管理LSM-tree刷新/压缩任务的常见实践。特别是，由于SPDK I/O的“fsync”涉及到忙等待，现有的 RocksDB 设计可能会释放许多后台线程，这会对其他线程造成巨大的破坏，并浪费 CPU 周期。</li>
<li>认识到这些，SpanDB采用异步请求处理。在 n 核机器上，用户将客户端线程的数量配置为N client，每个线程占用一个核。剩下的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>−</mo><msub><mi>N</mi><mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n - N_{client}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 个核心处理 SpanDB 内部服务线程，相应地被分为两个角色：<em>loggers</em> and <em>workers</em>。这些线程都被绑定到他们分配的核心上。 Loggers 主要执行 WAL 写入，Workers 处理后台任务处理（flush/compaction）以及一些非 I/O 任务，例如 Memtables 的写入和更新，WAL 日志项的准备，相关事务的加锁/同步。根据观察到的写强度，head-server 线程自动自适应地决定logger 的数量，这些 logger 被绑定到分配了 SPDK 队列的核上，从而保证 WAL 的写带宽<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210302114726.png" alt="20210302114726" loading="lazy"></li>
</ul>
<h4 id="asynchronous-apis">Asynchronous APIs</h4>
<ul>
<li>SpanDB 提供了简单、直观的异步 api。对于现有的 RocksDB 同步 get 和 put 操作，它增加了异步对等对象A_get和A_put，再加上A_check来检查请求状态。类似的API扩展适用于扫描和删除。相应地，SpanDB扩展了RocksDB的 status 枚举。</li>
<li>如下代码示例大致描述了异步请求的流程。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210302143805.png" alt="20210302143805" loading="lazy"></li>
</ul>
<h4 id="spandb-request-processing">SpanDB request processing</h4>
<ul>
<li>SpanDB管理前台请求处理的各个阶段，以及多个队列中的后台刷新/压缩任务。这些队列在线程之间传递子任务，还提供关于某个系统组件的压力级别的反馈。根据这些反馈，SpanDB可以调节客户端请求发出率(通过上述IsBusy接口)，也可以动态调整内部 workers 的任务分配</li>
<li>图 5 描绘了 SpanDB 中的相关队列。flush 和 compaction 队列本身就是来自于 RocksDB 的设计，即便 SpanDB 修改了实际的操作来使用 SPDK I/O。SPDK 还额外加了四个队列，读队列、以及写相关的三个队列（ProLog、Log、EpiLog）</li>
<li>对于<strong>异步读</strong>，当请求不需要进行 I/O 时，SpanDB 保持了原有 RocksDB 的同步模型，在KV应用程序中，由于具有典型的局部性，许多读操作都是从MemTable中进行的，特别是在今天由 DRAM 支持的更大的 MemTable 的场景下。给定一个 Key，能够快速在检查 Memtables 是否命中，如果命中直接完成读请求。命中的情况下大概只花费 4-6 微秒，相反即使在争用的情况下从 Optane 上读取也只花费 30 微妙，相应地会把读请求插入到读请求等待队列中然后返回。之后会有 worker 从队列中取出该请求，完成剩下的和 RocksDB 相同的读任务并设置他的完成状态。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210302114726.png" alt="20210302114726" loading="lazy"></li>
<li>对于<strong>异步写</strong>，SpanDB 将处理过程划分成了三个部分。
<ul>
<li>
<ol>
<li>客户端先将请求 dump 到 ProLog 队列中，由 worker 来处理。</li>
</ol>
</li>
<li>
<ol start="2">
<li>worker 相应地生成 WAL 日志项，按顺序传递到 Log 队列中。这两个队列都旨在促进批处理日志：一个 worker/logger 在这些队列中收集所有的日志项。除了批处理之外，logger 还将写日志流水线化，最大限度地提高了SPDK写并发性</li>
</ol>
</li>
<li>
<ol start="3">
<li>当将一个组写入到了 SD 之后，logger 把相应的请求添加到 EpiLog 队列中，以便 worker 能够完成最终的处理。主要包括实际 MemTable 的更新。像读操作一样，这里的任务需要单独的注意力，不可能通过批量执行来加速。</li>
</ol>
</li>
</ul>
</li>
<li><strong>如上图所示，ProLog 和 Log 队列都是 flat 的无锁队列，也就很容易地抓取所有请求进行出队。 Read、Epilog 都是环形队列，只在出队的时候要求加锁。</strong></li>
</ul>
<h4 id="task-scheduling">Task scheduling</h4>
<ul>
<li>上述SpanDB队列为调整内部资源分配提供了自然反馈。我们的SPDK基准测试结果表明 NVMe SSD 提供了并行性但是可能会被每个核心发出好几个并发请求使其饱和。因此 SpanDB 从一个 logger 开始，根据当前的写入强度在 1 到 3 之间增加和减少这个分配。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301220356.png" alt="20210301220356" loading="lazy"></li>
<li>对于 Worker 因为需要处理所有其他的队列，所以比较灵活。在三个前端任务队列之间，SpanDB 根据队列的实际长度和每个任务的处理时间的加权来执行负载均衡。在前端队列和后端队列之间，SpanDB 以处理前台任务优先，使用一个合适的阈值来监控后台队列长度，从而主动执行清理，特别是在写密集型工作负载下。</li>
</ul>
<h4 id="transaction-support">Transaction support</h4>
<ul>
<li>SpanDB 完整支持了事务并提供了一个异步提交接口 <code>A_commit</code>，对 RocksDB 做了一些小改动。注意在 RocksDB 的事务模式，写操作将在一个内部 Buffer 中生成 WAL 项，只会在提交的时候才会被写入。而 SpanDB 异步提交操作则是将相应的写任务插入到了 ProLog 队列。</li>
</ul>
<h3 id="high-speed-logging-via-spdk">High-speed Logging via SPDK</h3>
<h4 id="enabling-parallelism-and-pipelining">Enabling parallelism and pipelining</h4>
<ul>
<li>SpanDB 保留了 group logging 机制，但也允许多个 WAL 流并发写入。它不是让一个客户端作为 leader (并迫使 follower 等待)，而是使用专用的 logger，并发地发出批处理写操作。每个 logger 抓取它在 QLog 中看到的所有请求，并将这些 WAL 条目聚合为尽可能少的 4KB 块。它通过为一个请求窃取 SPDK 繁忙等待时间准备/检查其他请求来执行 pipeline。例如，对于 2L4R，有多达 8 个未完成的 WAL 写组</li>
</ul>
<h4 id="log-data-management">Log data management</h4>
<ul>
<li>并行的 WAL 写入让日志数据的管理变得复杂，特别是针对没有文件系统的裸设备。幸运的是，使用原子的 4KB SPDK 写操作，协调并发的 WAL 流只增加了很少的开销。</li>
<li>SpanDB 首先在 SD 上分配可配置数量的 Pages 给相应的 WAL 区域（测试中分配了 10GB），每个都会有一个逻辑页号 LPN。这些页中的一个会被设置为元数据页，在任何时候只会有一个可变的 Memtable，其对应的日志文件会不断增大。我们会分配固定数量的逻辑页 groups，每个组会包含连续的页，而且足够大可以装下一个 Memtable 的日志数据。被使用了的日志页和对应的 Memtable 给组织在一起：SpanDB 重用了 RocksDB Memtable 的日志文件号 LFN 作为日志标签号 LTN，嵌入在所有日志页的开头以便进行故障恢复。</li>
<li>下图给了一个包含四个 Memtable 的例子，一个 mutable 三个 immutable，对应元数据页中的 Active 和 InActive 状态。</li>
<li>在一个 Memtable 被 flush 之后，日志页的整个分片将被回收，保证 Memtable 日志的连续性。对于每个 Immutable Memtable，元数据页都记录了对应日志页的起始和终止 LPN。假设典型的 KV 存储使用少量 memtable，一个页面就足够容纳这样的元数据了。</li>
<li>Logger 发出并发请求，每个 Logger 提供一个 WAL 数据缓冲区和大小，唯一的同步点是日志页面的分配。我们使用比较和交换(CAS)操作实现了轻量级的原子页面分配。下图中的淡蓝色部分显示了三个请求的日志页分配过程，分别分配了 1，3，2 个页面，可以并行地进行分配。这些 WAL 写入不会修改元数据页，在元数据页中，只有当MemTable变得不可变时，per-MemTable end LPN 才会被追加。</li>
<li>在日志 Page 中，logger 首先记录当前 LTN，然后是一组日志条目，每个条目都有其大小的注释。图7中的放大部分描述了这种布局，包括每个条目的校验和(已经在RocksDB中计算过了)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210302153627.png" alt="20210302153627" loading="lazy"></li>
</ul>
<h4 id="correctness">Correctness</h4>
<ul>
<li>SpanDB的并行WAL写设计保留了RocksDB一致性语义。它不会改变用于协调和排序客户机请求的并发控制机制。因此，带有happened-before限制的事务在日志页面中不会出现顺序错误，如下面的简要解释。RocksDB 默认的隔离保证是 READ COMMITTED，也会检查写与写操作之间的冲突并序列化两个并发事务（假定两个更新相同 KV 项）。对于任何两个更新事务T1 和 T2，使用这两个隔离保证，READ COMMITTED 表明如果 T1 发生在 T2 之前，即 T2 看到了 T1 的效果，那么 T1 在 T2 开始执行之前必须提交。通过RocksDB group WAL写协议的设计，可以看出T1和T2的日志记录应该分两批出现，其中T1的批提交早于T2的批提交，在T2的批提交之前完成。当批量日志并行写入到 SpanDB 时，传递一个序列化的point 以便进行原子的页分配。因此 T1 的批任务仍然保证了比 T2 获得的序列号更小，后者就能看见前者的更新。相似的，从 WAL 中恢复的时候，SpanDB 总是按序列号升序执行 redo。</li>
</ul>
<h4 id="log-recovery">Log recovery</h4>
<ul>
<li>恢复操作首先读元数据页，检索出 log page groups 的数量以及相应的页地址范围。实际的从 log page group 恢复和从 RocksDB 的日志文件中恢复是一致的。同样，每个页面中的LTN号帮助标识 Active 日志 log group 的“结束”。</li>
<li>一个难点是 SpanDB 回收日志页 groups 时，有的 group 包含一些老的日志页，在恢复过程中 SpanDB 需要知道当前组中哪些页已经被重写了。RocksDB 依赖文件系统进行故障恢复：读取数据是否包含在活跃的 log file 中。没有文件系统，SpanDB 在回收之前可以持久化一个单独的元数据更新或者将老的日志页 wipe out(写 0)。会影响 SD 的 WAL 写带宽，降低到一半。而我们重用了每个 Memtable 的 LTN 作为日志页的 color。因为 SSD 可以保证 4K 原子写，LTN 通常又都是在页的开始写的，这些页已经暗含了最后一次成功写的位置信息了。元数据页维护了当前活跃的 LTN，在这个组中，具有过时LTN的页面还没有从当前MemTable中被覆盖写。</li>
</ul>
<h3 id="offloading-lsm-tree-levels-to-sd">Offloading LSM-tree Levels to SD</h3>
<ul>
<li>为了 KV 服务器的持续均衡的运行，SpanDB将RocksDB的lsm树的顶层迁移到SD，为用户提供更多的硬件投资回报。下面我们讨论所涉及的主要挑战和解决办法</li>
</ul>
<h4 id="data-area-storage-organization">Data area storage organization</h4>
<ul>
<li>在NVMe SSD上使用SPDK的一个限制是，整个设备必须与本机内核驱动程序解除绑定，并且不能通过传统的I/O堆栈访问。因此不能对SD进行分区，只能使用SPDK将WAL写入一个区域，在另一个区域安装文件系统。</li>
<li>SpanDB 实现的 TopFS，这是一个精简的文件系统，在SPDK I/O之上提供熟悉的文件接口包装器。SST 文件的仅追加和此后不可变的特性，加上它们的单 writer 访问模式，简化了TopFS设计。比如文件在创建的时候就知道了大小或者有有一个已知的大小限制。而且每个 SST 只会被单个线程写一次。大多数场景下，输入的数据不会被删除直到 SST 文件写入成功完成。此外，TopFS 保证了文件 close 时的数据的一致性。从而支持了每个文件连续的 LPN 范围分配，和前面提到的日志 groups 相似。元数据管理也比较简单：一个哈希表，按文件名进行索引，存储文件的起始和结束 LPN。TopFS 使用一个 LPN 空闲列表管理空闲空间，相应的连续的 LPN 范围被合并。</li>
</ul>
<h4 id="ensuring-wal-write-priority">Ensuring WAL write priority</h4>
<ul>
<li>flush/Compaction 操作是会影响前台负载的，但他们的延迟很多时候都对用户是透明的，因此 SD 可以完全更充分利用其带宽，比如在 WAL 写上，因为 WAL 的写入延迟用户时能感知到的。SPDK 提供了足够的 NVMe Queue Pairs，这允许对不同的请求类型进行单独管理。不幸的是，没有一个现有的通用 SSD 实现对这些队列的优先级管理。此外，这些队列提供的操作非常有限:用户只能发出请求和检查完成状态。</li>
<li>因此除了前后端任务在队列上的协调以外，SpanDB 还对 WAL 写请求赋予了优先级。我们发现他们的优先级可以有效地保护通过以下步骤：
<ul>
<li>(1) 分配到每个日志请求 slot 专用队列(例如,8 个 L2R4 队列)</li>
<li>(2) 减少 flush/compaction 的 I/O 请求的大小从 1MB 到 64KB 来减小有 WAL 时的 I/O 争用</li>
<li>(3) 限制执行 flush/compaction 的 worker 线程数量</li>
</ul>
</li>
</ul>
<h4 id="spandb-spdk-cache">SpanDB SPDK cache</h4>
<ul>
<li>SPDK bypass 了 OS page cache，如果无人看管，这会带来出色的原始 I/O，但会带来灾难性的应用程序 I/O 性能。为了克服这个问题，我们在 TopFS 上实现了 SpanDB 自己的缓存。注意，对于SPDK I/O，传递的所有数据缓冲区必须通过 <code>spdk_dma_malloc()</code> 在固定内存中分配。SpanDB重用这样的缓冲区作为缓存，从而节省额外的内存复制。</li>
<li>在SpanDB初始化时，它在hugepage中分配一个大内存缓存(大小可配置)。在创建SST文件时，SpanDB在缓存中保留适当数量的连续64KB缓冲区(文件大小或大小限制是已知的)。SpanDB使用另一个哈希表来管理这个缓存，同样以RocksDB SST文件名作为键。value字段是一个数组，存储每个文件块的缓存项，如果块被缓存，则存储相应的内存地址，否则为空。块大小配置显然涉及到缓存数据粒度和元数据开销之间的权衡。我们的评估使用SpanDB默认块大小64KB，对100GB的数据库产生&lt;500KB的元数据开销。</li>
</ul>
<h4 id="dynamic-level-placement">Dynamic level placement</h4>
<ul>
<li>通过上述所有机制，我们可以动态地调整SD上驻留的树级别的数量。最初，我们使用一个分析模型来直接计算最佳SD-CD级别分区，从而最大限度地提高整个系统的吞吐量。然而，我们无法找到与我们的测量结果一致的精确的 LSM-tree 写放大模型。特别是，最先进的工作在这前端的似乎没有考虑写速度和它的变化。我们的测试表明，这些因素会严重影响暂态的“树状结构”(最高层在不同程度上超出其大小限制)，进而影响写/读放大级别。</li>
<li>因此，SpanDB 使用了临时的动态分区，通过观察 SD 和 CD 之间持续的资源利用率的不平衡，它的 head-server 线程监视 SD 带宽使用情况，并在低于 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi>B</mi><mi>W</mi></mrow><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">{BW}_L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 时触发SST文件重定位，直到达到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi>B</mi><mi>W</mi></mrow><mi>H</mi></msub></mrow><annotation encoding="application/x-tex">{BW}_H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 或 SD 满，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi>B</mi><mi>W</mi></mrow><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">{BW}_L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi>B</mi><mi>W</mi></mrow><mi>H</mi></msub></mrow><annotation encoding="application/x-tex">{BW}_H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是两个可配置的阈值。</li>
<li><strong>由于SST文件不断地进行合并，SpanDB并没有在SD和CD之间迁移数据，而是通过将文件创建重定向到不同的目标，逐步“提升”或“降级”整个级别</strong>。它有一个指针，指示当前哪些级别应该转到快速NVMe设备。例如，指针 3 覆盖了所有的前3个级别。然而，该指针仅确定新SST文件的目的地。因此，在SD上有一个新的L3文件，在CD上有一个旧的L2文件是可能的，尽管这种“倒置”是罕见的，因为顶层更小，他们的文件更新更频繁。</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210312113832.png" alt="20210312113832" loading="lazy"></figure>
<h2 id="evaluation">Evaluation</h2>
<p><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210312113934.png" alt="20210312113934" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210302200522.png" alt="20210302200522" loading="lazy"></p>
<h2 id="related-work">Related Work</h2>
<h3 id="tiered-storage">Tiered storage</h3>
<ul>
<li>NVMFS, Strata, Ziggurat:  File system based on heterogeneous devices</li>
<li>HiLSM, MatrixKV: Hybrid storage devices for KV</li>
<li>Mutant: ranks SST files by popularity and places them on different cloud storage devices</li>
<li>PrismDB: makes LSM-trees “read-aware” by pinning hot objects to fast devices.</li>
</ul>
<h3 id="kv-stores-optimizations-for-fast-homogeneous-storage">KV stores optimizations for fast, homogeneous storage</h3>
<h4 id="homogeneous-storage">homogeneous storage</h4>
<ul>
<li>UniKV</li>
<li>LSM-trie</li>
<li>SlimDB</li>
<li>FloDB</li>
<li>PebblesDB</li>
<li>KVell</li>
<li>SplinterDB</li>
</ul>
<h4 id="fpga">FPGA</h4>
<ul>
<li><strong>X-Engine</strong></li>
<li><strong>KVSSD</strong></li>
<li><strong>ATC20 PinK</strong></li>
</ul>
<h4 id="pm">PM</h4>
<ul>
<li>HiKV</li>
<li>NoveLSM</li>
<li>NVMRocks</li>
<li>Bullet</li>
<li>SLM-DB</li>
<li>FlatStore</li>
<li>MyNVM</li>
<li>MyRocks</li>
</ul>
<h3 id="logging-optimizations">Logging optimizations</h3>
<ul>
<li>NVLogging</li>
<li>NVWAL</li>
</ul>
<h3 id="other-related-work">Other related work</h3>
<ul>
<li>SILK</li>
<li>Monkey</li>
<li>ElasticBF</li>
<li>TRIAD</li>
<li>WiscKey</li>
<li>HashKV</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux Software Management]]></title>
        <id>https://blog.shunzi.tech/post/linux-software-mangement/</id>
        <link href="https://blog.shunzi.tech/post/linux-software-mangement/">
        </link>
        <updated>2021-02-26T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Linux 软件包管理安装。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Linux 软件包管理安装。</li>
</ul>
</blockquote>
<!--more-->
<h2 id="参考书目链接">参考书目/链接</h2>
<ul>
<li><a href="https://www.linuxprobe.com/linux-basic-manage.html">Linux 就该这么学：Linux软件包管理基本操作入门</a></li>
<li><a href="https://segmentfault.com/a/1190000011325357">segmentfault: Linux系统中软件的“四”种安装原理详解</a></li>
<li><a href="https://www.cnblogs.com/zwj-linux/p/11643033.html">博客园：Linux下rpm、yum和源码三种安装方式简介</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-rpm1/index.html">IBM Developer - 构建和分发包</a></li>
</ul>
<h2 id="软件包">软件包</h2>
<h3 id="windows">Windows</h3>
<ul>
<li>开始介绍 Linux 软件包之前，先大致回忆一下 Windows 场景下的我们常常使用的软件包形态。</li>
<li>Win 环境下：
<ul>
<li>大量的软件应用程序都是使用诸如 .exe/.msi 等类型的文件进行安装，直接运行该安装程序，按照步骤勾选相应的可选配置就能安装对应的软件。
<ul>
<li>MSI 就是 microsoft installer 的简写，msi 文件就是 window installer 的数据包，把所有和安装文件相关的内容封装在一个包里。</li>
<li>exe 是一个安装引导程序。主要是用于检查安装的环境，当检查成功后，会自动再安装 msi 文件。</li>
</ul>
</li>
<li>除了安装程序以外，常常还有一些压缩包的形式来安装软件（也就是所谓的绿色版），本质就是把程序的全部数据以及依赖的环境给压缩到一个文件夹中，而下载该文件夹就可以直接执行其中保存的可执行程序。</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210225220304.png" alt="20210225220304" loading="lazy"></figure>
<h3 id="linux">Linux</h3>
<ul>
<li>大多数现代类 Unix 操作系统都提供了一个集中的软件包管理机制，以帮助用户搜索、安装和管理软件。而软件通常以「包」的形式存储在仓库「repository」中，对软件包的使用和管理被称为包管理。</li>
<li>Linux 包的基本组成部分通常有：共享库、应用程序、服务和文档。</li>
<li>Linux 软件包分类:
<ul>
<li>源码包</li>
<li>二进制包（RPM/DEB包）</li>
<li>yum/apt 源在线安装</li>
<li>脚本安装包（本质还是源码包和二进制包）</li>
</ul>
</li>
</ul>
<h2 id="linux-软件包管理">Linux 软件包管理</h2>
<h3 id="软件包管理系统">软件包管理系统</h3>
<ul>
<li>大多数包管理系统是建立在包文件上的集合，包文件通常包含编译好的二进制文件和其它资源组成的：软件、安装脚本、元数据及其所需的依赖列表。</li>
<li>因为 Linux 有很多发行版，各自的管理系统也有一定的差异</li>
</ul>
<table>
<thead>
<tr>
<th>系统</th>
<th>格式</th>
<th>工具</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debian</td>
<td>.deb</td>
<td>apt, apt-cache、apt-get、dpkg</td>
</tr>
<tr>
<td>Ubuntu</td>
<td>.deb</td>
<td>apt、apt-cache、apt-get、dpkg</td>
</tr>
<tr>
<td>CentOS</td>
<td>.rpm</td>
<td>yum</td>
</tr>
<tr>
<td>Fedora</td>
<td>.rpm</td>
<td>dnf</td>
</tr>
</tbody>
</table>
<ul>
<li>Debian 及其衍生产品如：Ubuntu、Linux Mint 和 Raspbian 的包格式为.deb文件，APT 是最常见包操作命令，可：搜索库、安装包及其依赖和管理升级。而要直接安装现成.deb包时需要使用dpkg命令。</li>
<li>CentOS、Fedora 及 Red Hat 系列 Linux 使用RPM包文件，并使用yum命令管理包文件及与软件库交互。在最新的 Fedora 版本中，yum命令已被dnf取代进行包管理。</li>
</ul>
<h3 id="常见操作">常见操作</h3>
<ul>
<li>更新本地包数据库列表：
<ul>
<li>apt-get update</li>
<li>yum check-update</li>
<li>dnf check-update</li>
</ul>
</li>
<li>升级已安装的包：
<ul>
<li>apt-get upgrade</li>
<li>apt-get dist-upgrade</li>
<li>yum update /dnf upgrade</li>
</ul>
</li>
<li>查找/搜索软件包：
<ul>
<li>apt-cache search xxx</li>
<li>yum search xxx</li>
<li>yum search all xxx</li>
<li>dnf search xxx</li>
<li>dnf search all xxx</li>
</ul>
</li>
<li>查看某个软件包信息:
<ul>
<li>apt-cache show [pkg_name]</li>
<li>dpkg -s [pkg_name] (显示包的安装状态)</li>
<li>yum info [pkg_name]</li>
<li>yum deplist [pkg_name] （列出包的依赖）</li>
<li>dnf info  [pkg_name]</li>
<li>dnf repoquery –requires [pkg_name]</li>
</ul>
</li>
<li>安装包：
<ul>
<li>apt-get install xxx</li>
<li>yum instal xxx</li>
<li>dnf install xxx</li>
</ul>
</li>
<li>移除包：
<ul>
<li>apt-get remove xxx</li>
<li>apt-get autoremove (自动移除已知不需要的包)</li>
<li>yum remove xxx</li>
<li>dnf erase xxx</li>
</ul>
</li>
</ul>
<h3 id="本地文件系统安装">本地文件系统安装</h3>
<ul>
<li>首先简要介绍本地安装软件包的场景和主要命令，然后以 RPM 为例详细说明。</li>
</ul>
<h4 id="从本地文件系统直接安装包">从本地文件系统直接安装包</h4>
<ul>
<li>很多时候，我们在进行测试或从某个地方直接拿到软件包之后需要从本地文件系统直接安装包。（特别是针对一些无公网的环境，无法直接使用包管理工具直接安装相关依赖）</li>
<li>Debian 及衍生系统可以使用 dpkg 进行安装，CentOS 和 Fedora 系统使用 yum 和 dnf 命令进行安装。</li>
</ul>
<h5 id="本地安装命令">本地安装命令</h5>
<ul>
<li>dpkg -i [pkg_name].deb</li>
<li>apt-get install -y gdebi&amp;&amp; sudo gdebi [pkg_name].deb （使用gdebi检索缺少的依赖关系）</li>
<li>yum install [pkg_name].rpm</li>
<li>dnf install [pkg_name].rpm</li>
</ul>
<h3 id="rpm">RPM</h3>
<ul>
<li>RPM命名“RedHat Package Manager”，简称则为RPM。这个机制最早由Red Hat这家公司开发出来的，后来实在很好用，因此很多distributons就使用这个机制来作为软件安装的管理方式，包括Fedora，CentOS，SuSE等知名的开发商都是用它。</li>
<li>RPM最大的特点就是需要安装的软件已经编译过，并已经打包成RPM机制的安装包，通过里头默认的数据库记录这个软件安装时需要的依赖软件。当安装在你的Linux主机时，RPM会先依照软件里头的数据查询Linux主机的依赖属性软件是否满足，若满足则予以安装，若不满足则不予安装。</li>
</ul>
<h4 id="构建流程">构建流程</h4>
<p>要构建 RPM，必须：</p>
<ul>
<li>依照 rpmbuild 规范设定一个目录结构。</li>
<li>将源代码和附带文件放在目录中合适的位置。</li>
<li>创建 spec 文件。</li>
<li>编译 RPM。可以选择编译源 RPM，以与其他人共享您的源代码。</li>
</ul>
<h5 id="目录结构">目录结构</h5>
<ul>
<li><strong>BUILD</strong>。BUILD 用作实际编译软件的暂存空间。</li>
<li><strong>RPMS</strong>。RPMS 包含 rpmbuild 所编译的二进制 RPM。</li>
<li><strong>SOURCES</strong>。SOURCES 存储源代码。</li>
<li><strong>SPECS</strong>。SPECS 包含您的 spec 文件，您想要构建的一个 RPM 对应一个 spec 文件。</li>
<li><strong>SRPMS</strong>。SRPMS 包含在这个过程中构建的源 RPM。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210226153713.png" alt="20210226153713" loading="lazy"></figure>
<h5 id="准备文件">准备文件</h5>
<ul>
<li><strong>step1</strong>. 将源代码（理想情况下应捆绑为一个 tarball 压缩文件）复制到 SOURCES 目录</li>
<li><strong>step2</strong>. 创建 spec 文件。spec 文件只是一个具有特殊语法的文本文件。
<ul>
<li>此处举一个简单例子 wget 打包</li>
</ul>
</li>
</ul>
<pre><code class="language-shell"># This is a sample spec file for wget

# 定义了五个变量 
%define _topdir     /home/strike/mywget
%define name            wget 
%define release     1
%define version     1.12
%define buildroot %{_topdir}/%{name}-%{version}-root
 
# 设置若干关键参数
BuildRoot:  %{buildroot}
Summary:        GNU wget
License:        GPL
Name:           %{name}
Version:        %{version}
Release:        %{release}
Source:         %{name}-%{version}.tar.gz
Prefix:         /usr
Group:          Development/Tools

# 简单明了地描述软件。这一行将在用户运行 rpm -qi 来查询 RPM 数据库时显示。您可以说明包的用途，描述任何警告或额外的配置说明等。
%description
The GNU wget program downloads files from the Internet using the command-line.

# 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行
# 准备源代码
# %setup -q 是一个 %prep 宏，用于自动解压 Source 中的特定 tarball 压缩文件
%prep
%setup -q

# 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行
# 手动配置和启动构建过程的步骤 
%build
./configure
make

# 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行
%install
make install prefix=$RPM_BUILD_ROOT/usr

# 列出应该捆绑到 RPM 中的文件，还可以设置权限和其他信息。
%files
%defattr(-,root,root)
/usr/local/bin/wget

# %doc 告诉 RPM 该文件为一个文档文件，所以如果用户使用 --excludedocs 安装包，将不会安装该文件。
%doc %attr(0444,root,root) /usr/local/share/man/man1/wget.1
</code></pre>
<ul>
<li>再来看 tcmu-runner.spec 的例子</li>
</ul>
<pre><code class="language-spec">%global _hardened_build 1

# 运行 rpmbuild 时读取的一些条件参数
# without rbd dependency
# if you wish to exclude rbd handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without rbd
%bcond_without rbd

# without glusterfs dependency
# if you wish to exclude glfs handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without glfs
%bcond_without glfs

# without qcow dependency
# if you wish to exclude qcow handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without qcow
%bcond_without qcow

# without zbc dependency
# if you wish to exclude zbc handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without zbc
%bcond_without zbc

# without file backed optical dependency
# if you wish to exclude fbo handlers in RPM, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without fbo
%bcond_without fbo

# without tcmalloc dependency
# if you wish to exclude tcmalloc, use below command
# rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without tcmalloc
%bcond_without tcmalloc

# 设置若干关键参数
Name:          tcmu-runner
Summary:       A daemon that handles the userspace side of the LIO TCM-User backstore
Group:         System Environment/Daemons
License:       ASL 2.0 or LGPLv2+
Version:       1.0
URL:           https://github.com/open-iscsi/tcmu-runner

Release:       0%{dist}
BuildRoot:     %(mktemp -udp %{_tmppath}/%{name}-%{version})
Source:       %{name}-%{version}.tar.gz
ExclusiveOS:   Linux

BuildRequires: cmake make gcc
BuildRequires: libnl3-devel glib2-devel zlib-devel kmod-devel

# 针对不同条件参数下的依赖处理
%if %{with rbd}
BuildRequires: librbd1-devel librados2-devel
Requires(pre): librados2, librbd1
%endif

%if %{with glfs}
BuildRequires: glusterfs-api-devel
Requires(pre): glusterfs-api
%endif

%if %{with tcmalloc}
BuildRequires: gperftools-devel
Requires:      gperftools-libs
%endif

# 基本依赖
Requires(pre): kmod, zlib, libnl3, glib2, logrotate, rsyslog
Requires:      libtcmu = %{version}-%{release}

# 软件描述
%description
A daemon that handles the userspace side of the LIO TCM-User backstore.

LIO is the SCSI target in the Linux kernel. It is entirely kernel code, and
allows exported SCSI logical units (LUNs) to be backed by regular files or
block devices. But, if we want to get fancier with the capabilities of the
device we're emulating, the kernel is not necessarily the right place. While
there are userspace libraries for compression, encryption, and clustered
storage solutions like Ceph or Gluster, these are not accessible from the
kernel.

The TCMU userspace-passthrough backstore allows a userspace process to handle
requests to a LUN. But since the kernel-user interface that TCMU provides
must be fast and flexible, it is complex enough that we'd like to avoid each
userspace handler having to write boilerplate code.

tcmu-runner handles the messy details of the TCMU interface -- UIO, netlink,
pthreads, and DBus -- and exports a more friendly C plugin module API. Modules
using this API are called &quot;TCMU handlers&quot;. Handler authors can write code just
to handle the SCSI commands as desired, and can also link with whatever
userspace libraries they like.

# 生成的 RPM 包 libtcmu
%package -n libtcmu
Summary:       A library supporting LIO TCM-User backstores processing
Group:         Development/Libraries

%description -n libtcmu
libtcmu provides a library for processing SCSI commands exposed by the
LIO kernel target's TCM-User backend.

# 生成的 RPM 包 libtcmu-devel
%package -n libtcmu-devel
Summary:       Development headers for libtcmu
Group:         Development/Libraries
Requires:      %{name} = %{version}-%{release}
Requires:      libtcmu = %{version}-%{release}

%description -n libtcmu-devel
Development header(s) for developing against libtcmu.

%global debug_package %{nil}

# 准备源代码
%prep
%setup -n %{name}-%{version}

# 手动配置和启动构建
%build
%{__cmake} \
 -DSUPPORT_SYSTEMD=ON -DCMAKE_INSTALL_PREFIX=%{_usr} \
 %{?_without_rbd:-Dwith-rbd=false} \
 %{?_without_zbc:-Dwith-zbc=false} \
 %{?_without_qcow:-Dwith-qcow=false} \
 %{?_without_glfs:-Dwith-glfs=false} \
 %{?_without_fbo:-Dwith-fbo=false} \
 %{?_without_tcmalloc:-Dwith-tcmalloc=false} \
 .
%{__make}

%install
%{__make} DESTDIR=%{buildroot} install
%{__rm} -f %{buildroot}/etc/tcmu/tcmu.conf.old
%{__rm} -f %{buildroot}/etc/logrotate.d/tcmu-runner.bak/tcmu-runner

# 列出应该捆绑到 RPM 中的文件
%files
%{_bindir}/tcmu-runner
%dir %{_sysconfdir}/dbus-1/
%dir %{_sysconfdir}/dbus-1/system.d
%config %{_sysconfdir}/dbus-1/system.d/tcmu-runner.conf
%dir %{_datadir}/dbus-1/
%dir %{_datadir}/dbus-1/system-services/
%{_datadir}/dbus-1/system-services/org.kernel.TCMUService1.service
%{_unitdir}/tcmu-runner.service
%dir %{_libdir}/tcmu-runner/
%{_libdir}/tcmu-runner/*.so
%{_mandir}/man8/*
%doc README.md LICENSE.LGPLv2.1 LICENSE.Apache2
%dir %{_sysconfdir}/tcmu/
%config %{_sysconfdir}/tcmu/tcmu.conf
%config(noreplace) %{_sysconfdir}/logrotate.d/tcmu-runner
%ghost %attr(0644,-,-) %{_sysconfdir}/tcmu/tcmu.conf.old
%ghost %attr(0644,-,-) %{_sysconfdir}/logrotate.d/tcmu-runner.bak/tcmu-runner

%files -n libtcmu
%{_libdir}/libtcmu*.so.*

%files -n libtcmu-devel
%{_libdir}/libtcmu*.so

</code></pre>
<h5 id="构建-rpm">构建 RPM</h5>
<ul>
<li>例如构建 wget:</li>
</ul>
<blockquote>
<p><code>rpmbuild -v -bb --clean SPECS/wget.spec</code></p>
</blockquote>
<p>此命令使用指定的 spec 文件构建一个二进制包（-bb 表示 “构建二进制包”），还会生成详细的输出（-v）。构建实用程序在生成包之后删除构建树（--clean）。如果还希望构建源 RPM，指定 -ba（“构建所有包”）来代替 -bb。（查看 rpmbuild 清单页面，了解完整的选项列表。）</p>
<ul>
<li>rpmbuild 执行以下步骤：
<ul>
<li>读取并解析 wget.spec 文件。</li>
<li>运行 %prep 节，将源代码解压到临时目录。在这里，临时目录为 BUILD。</li>
<li>运行 %build 节，编译代码。</li>
<li>运行 %install 节，将代码安装到构建机器上的目录中。</li>
<li>从 %files 节读取文件列表，将它们收集到一起，然后创建一个二进制 RPM（和源 RPM 文件，如果已选择）。</li>
</ul>
</li>
</ul>
<h4 id="安装流程">安装流程</h4>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210226151530.png" alt="20210226151530" loading="lazy"></figure>
<ul>
<li><code>rpm -ivh package-name</code>
<ul>
<li>-i：install的意思，安装</li>
<li>-v：查看更详细的安装信息画面（provide more detailed output）</li>
<li>-h：以安装信息栏显示安装进度</li>
</ul>
</li>
</ul>
<h5 id="错误处理">错误处理</h5>
<ul>
<li>安装rpm包时提示错误：依赖检测失败: <code>--nodeps --force</code></li>
</ul>
<pre><code class="language-cmd">[root@localhost ~]# rpm -ivh tcmu-runner-2.0.1-0.el7.x86_64.rpm 
错误：依赖检测失败：
	libhcs_obj_util.so()(64bit) 被 tcmu-runner-2.0.1-0.el7.x86_64 需要

[root@localhost ~]# rpm -ivh tcmu-runner-2.0.1-0.el7.x86_64.rpm --nodeps --force
准备中...                          ################################# [100%]
正在升级/安装...
   1:tcmu-runner-2.0.1-0.el7          ################################# [100%]
[root@localhost ~]# systemctl status tcmu-runner
● tcmu-runner.service - LIO Userspace-passthrough daemon
   Loaded: loaded (/usr/lib/systemd/system/tcmu-runner.service; disabled; vendor preset: disabled)
   Active: inactive (dead)
     Docs: man:tcmu-runner(8)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reading Group Notes]]></title>
        <id>https://blog.shunzi.tech/post/ReadingGroup/</id>
        <link href="https://blog.shunzi.tech/post/ReadingGroup/">
        </link>
        <updated>2021-02-19T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>参加 Systems Reading Group 记录的论文笔记。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>参加 Systems Reading Group 记录的论文笔记。</li>
</ul>
</blockquote>
<!--more-->
<h1 id="system-reading-group">System Reading Group</h1>
<ul>
<li>Group 介绍以及 Presentation 安排： https://learn-sys.github.io/cn/reading/</li>
<li>THU AOS 2020: http://os.cs.tsinghua.edu.cn/oscourse/AOS2020</li>
</ul>
<h2 id="week-1-operating-system">Week 1: Operating System</h2>
<h3 id="course-notes">Course Notes</h3>
<ul>
<li>Video: THU AOS P7 - P11</li>
</ul>
<h4 id="os-architecture-structure">OS Architecture &amp; Structure</h4>
<ul>
<li><strong>OS Structure</strong>:
<ul>
<li>Simple kernel</li>
<li>Monolithic kernel</li>
<li>Micro kernel</li>
<li>Exokernel</li>
<li>VMM(Virtual Machine Monitor), etc...</li>
</ul>
</li>
<li><strong>Monolithic kernel</strong>
<ul>
<li>UNIX Arch<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219163131.png" alt="20210219163131" loading="lazy"></li>
<li>Linux Arch<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219163153.png" alt="20210219163153" loading="lazy"></li>
</ul>
</li>
<li><strong>Micro kernel</strong>
<ul>
<li><strong>微内核：功能相对较少的内核，只提供某些核心功能。从而相比于单体内核，把很多单体内核中的事情放到用户空间去做，解耦了内核的各个 features，让整个系统的稳定性和灵活性得到了提升。但也就因为 IPC 的开销导致性能表现不尽如人意。</strong></li>
<li>Kernel with minimal features</li>
<li>Moves as much from the kernel into user space
<ul>
<li>Address spaces</li>
<li>Interprocess communication (IPC)</li>
<li>Scheduling</li>
</ul>
</li>
<li>Benefits
<ul>
<li>Flexibility</li>
<li>Safety</li>
<li>Modularity</li>
</ul>
</li>
<li>Detriments (Poor Performance)
<ul>
<li>Address spaces</li>
<li>Interprocess communication (IPC)</li>
<li>Scheduling</li>
</ul>
</li>
<li><strong>Mach</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165141.png" alt="20210219165141" loading="lazy"></li>
<li><strong>L4</strong> - Microkernel– L4Second generation microkernel
<ul>
<li>synchronous IPCs –&gt; async IPCs (like epoll in Linux)</li>
<li>smaller, Mach 3(330 KB) –&gt; L4 (12KB)</li>
<li>IPC security checks moved to user process</li>
<li>IPC is hardware dependent</li>
</ul>
</li>
</ul>
</li>
<li><strong>Exokernel</strong>
<ul>
<li><strong>Exokernel 要做的事情其实是把内核也近乎给 PASS 掉，尽可能减少抽象层次，允许应用程序直接访问硬件，而ExoKernel只负责保护和分配系统资源。说白了就是把硬件资源都直接交给应用程序自己来组织了，因为有大量的应用程序想要自己独立控制可管理硬件，而不需要你操作系统层面的过多干涉。</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165849.png" alt="20210219165849" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210219165610.png" alt="20210219165610" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="paper-1-the-multikernel-a-new-os-architecture-for-scalable-multicore-systems">Paper 1: The Multikernel: A new OS architecture for scalable multicore systems</h3>
<ul>
<li><strong>SOSP09</strong></li>
<li><strong>SIGOPS Hall of Fame Award 2020</strong></li>
</ul>
<h4 id="abstract">Abstract</h4>
<ul>
<li>普通计算机系统包含越来越多的处理器核心，并呈现出越来越多的架构权衡，包括内存层次结构、互连、指令集和变体，以及IO配置。 以前的高性能计算系统在特定情况下进行了扩展，但是现代客户机和服务器工作负载的动态特性，加上不可能针对所有工作负载和硬件变体静态地优化操作系统，对操作系统结构构成了严重的挑战。</li>
<li>我们认为，迎接未来多核硬件挑战的最好方法是拥抱机器的网络化本质，重新思考使用来自分布式系统的思想的操作系统架构。我们研究了一种新的操作系统结构，即 Multikernel，它将机器视为一个由独立核心组成的网络，假定在最低层次上没有核间共享，并将传统的操作系统功能转移到一个通过消息传递进行通信的分布式进程系统。</li>
<li>我们已经实现了一个多内核操作系统来证明这种方法是有前途的，并且我们描述了操作系统的传统的可伸缩性问题(如内存管理)是如何通过消息有效地重新解决的，以及如何利用分布式系统和网络的洞察力。在多核系统上对我们的原型的评估表明，即使在现在的机器上，多内核的性能也可以与传统的相媲美，并且可以更好地扩展以支持未来的硬件。</li>
</ul>
<h4 id="problems">Problems</h4>
<ul>
<li>随着不断变化的技术对摩尔定律的限制，处理器架构变得越来越多样化，且逐渐转向异构化，并向可扩展的架构发展，以适应高性能的应用。传统的单体操作系统在解决可伸缩性问题和针对不同硬件结构进行优化方面面临着巨大的挑战。
<ul>
<li>Systems are increasingly diverse</li>
<li>Cores are increasingly diverse</li>
<li>The interconnect matters</li>
<li>Messages cost less than shared memory</li>
<li>Cache coherence is not a panacea</li>
<li>Messages are getting easier</li>
</ul>
</li>
<li>本文作者尝试通过在内核之间使用显式消息传递和在内核之间复制内核状态来解决这个问题，而不是使用共享内存模型。他们的另一个主要目标是使这个操作系统与硬件无关，不针对任何机器架构。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210220221342.png" alt="20210220221342" loading="lazy"></li>
</ul>
<h4 id="contributions">Contributions</h4>
<ul>
<li>多内核操作系统的主要贡献嵌入在它们的三个设计原则中：
<ul>
<li>通过消息传递显式地实现内核间通信</li>
<li>使操作系统结构与硬件无关</li>
<li>在内核之间复制内核状态</li>
</ul>
</li>
<li>该系统侧重于非共享内存模型，通过消息的显式通信来维护缓存的一致性。基于以上原则，设计实现了 MultiKernel 原型 Barrelfish<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210224113033.png" alt="20210224113033" loading="lazy"></li>
</ul>
<h2 id="week-2-virtualization">Week 2: Virtualization</h2>
<h3 id="course-notes-2">Course Notes</h3>
<ul>
<li>Video
<ul>
<li>THU AOS P12 - P21</li>
<li>IPADS MOS P70 - P87</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MapReduce: Simplified Data Processing on Large Clusters]]></title>
        <id>https://blog.shunzi.tech/post/MapReduce/</id>
        <link href="https://blog.shunzi.tech/post/MapReduce/">
        </link>
        <updated>2021-02-08T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。</li>
</ul>
</blockquote>
<!--more-->
<h2 id="before-beginning">Before Beginning</h2>
<ul>
<li>为什么要读这篇论文呢？其实这篇文章之前也已经简单看过了，只是最近开始刷 MIT6.824，本来是想直接做相关 Lab 的，但是发现还是有整理不清楚的思路，觉得还是有必要回顾一下，那就多花点时间继续研读吧~</li>
<li>网上关于 6.824 以及 MapReduce 的资料很多了，我在这里只是做一些简单的记录，如果有发现其他大佬做的比较好的笔记，也会贴在这里，以供膜拜学习。我的 6.824 系列的的博客介绍大抵都是如此。</li>
<li>话不多说，学习开始~</li>
</ul>
<h2 id="abstract">Abstract</h2>
<ul>
<li><strong>大致流程</strong>: 用户指定一个 map 函数来处理键/值对以生成一组中间键/值对，以及一个 reduce 函数来合并与同一中间键相关的所有中间值</li>
<li><strong>为什么这么做？</strong> 是想充分利用不同机器的并行性来处理大量的数据，分别执行 map 和 reduce 任务来完成大数据任务，提高每个 host 的利用率。（也就是分布式系统的原型）</li>
<li><strong>需要解决的问题：</strong>
<ul>
<li>数据输入的切分</li>
<li>不同机器上执行的任务的调度</li>
<li>机器故障处理</li>
<li>机器间通信的管理</li>
</ul>
</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>根本矛盾</strong>：少数据量时单机能够直接运行简单的任务来完成相关计算，但面对大数据量的情况下，引入多计算实例组成的系统的复杂性和本身计算任务的简单性之间的矛盾。</li>
<li><strong>思想起源</strong>：来自于 Lisp 语言的函数式编程思想中的 map/reduce 函数。
<ul>
<li>在 lisp 语言中，map 作为一个输入函数接受一个序列，然后处理每个序列中 value 值，然后 reduce 将最终的 map 计算出来的结果整理成最终程序输出。</li>
</ul>
</li>
</ul>
<h2 id="programming-model">Programming Model</h2>
<ul>
<li>MapReduce 本质是一种编程模型
<ul>
<li>Map: 由用户编写，接受一个输入对并生成一组中间键/值对</li>
</ul>
</li>
</ul>
<pre><code class="language-Java">// map (k1,v1) → list(k2,v2)
map(String key, String value): 
    // key: document name 
    // value: document contents 
    for each word w in value: 
        EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<ul>
<li>Reduce: 也由用户编写，接受一个中间键和该键的一组值。它将这些值合并在一起，形成一个可能更小的值集</li>
</ul>
<pre><code class="language-Java">// reduce (k2,list(v2)) → list(v2)
reduce(String key, Iterator values): 
    // key: a word 
    // values: a list of counts 
    int result = 0; 
    for each v in values: 
        result += ParseInt(v); 
    Emit(AsString(result));
</code></pre>
<ul>
<li><strong>应用实例</strong>：
<ul>
<li>Distributed Grep</li>
<li>Count of URL Access Frequency：map &lt;URL, 1&gt;, reduce &lt;URL, total count&gt;</li>
<li>Reverse Web-Link Graph: map &lt;target, source&gt;, reduce &lt;target, list(source)&gt;</li>
<li>Term-Vector per Host</li>
<li>Inverted Index: map &lt;word, document ID&gt;, reduce &lt;word, list(document ID)&gt;</li>
<li>Distributed Sort</li>
</ul>
</li>
</ul>
<h2 id="implementation">Implementation</h2>
<ul>
<li>Map 函数分布在多个机器上，相应地自动将输入数据划分为 M 份，然后可以由分布了 Map 函数的机器并行处理这些数据。而对于 Reduce 则是将中间数据划分为 R 份，通常需要使用一个分割函数，常见的就是 <code>hash(key) mod R</code> 来将中间 Key 进行区分。</li>
<li>下图演示了整个 MapReduce 的流程，当用户程序调用 MapReduce 函数时将按照以下顺序执行：
<ul>
<li>
<ol>
<li>MapReduce Library 首先将输入文件划分为 M 个分片，每个分片大小通常为 16MB or 64MB，可以由用户控制，然后开始将程序拷贝到各个机器上，也就是图中的 <strong>fork</strong> 过程</li>
</ol>
</li>
<li>
<ol start="2">
<li>fork 的过程中会有一个特殊的情况，即 master 节点上运行的程序。剩下的 worker 对应执行的任务都是由 master 分配的，有 M 个 map task 和 R 个 reduce task 需要分配，master 选择空闲的 worker 来执行 map 或者 reduce task。</li>
</ol>
</li>
<li>
<ol start="3">
<li>被分配到 map task 的 worker 首先读取分片的数据内容，它从输入数据中解析键/值对，并将每对键/值传递给用户定义的 Map 函数，然后由 Map 产生的中间键值对将被缓冲在内存中；</li>
</ol>
</li>
<li>
<ol start="4">
<li>缓冲在内存中的中间数据将定期执行刷回操作写到磁盘，然后再由用户定义的分割函数执行将中间数据分割为 R 个区域，这些原本缓冲在内存中的数据持久化到磁盘之后的地址将传递给 master，然后 master 负责告诉 reduce task worker 这些数据在哪里。</li>
</ol>
</li>
<li>
<ol start="5">
<li>执行 reduce task 的 worker 在接收到来自 master 的数据地址的通知之后，使用 RPC 来从 map worker 的本地磁盘中读取数据，当一个 reducer 读取到了所有的中间数据之后，就可以根据中间键对它进行排序，以便将所有出现的相同键组合在一起。之所以需要排序，是因为通常有许多不同的键映射到同一个reduce任务。如果中间数据量太大，无法装入内存，则使用外部排序。</li>
</ol>
</li>
<li>
<ol start="6">
<li>reduce worker 迭代已排序的中间数据，对于遇到的每个惟一的中间键，它将键和相应的中间值集传递给用户的 reduce 函数。Reduce 函数的输出被追加到这个 Reduce 分区的最终输出文件中；</li>
</ol>
</li>
<li>
<ol start="7">
<li>所有的 map 任务和 reduce 任务都完成后，master 唤醒用户程序。此时，用户程序中的MapReduce 调用返回到用户程序。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210208135434.png" alt="20210208135434" loading="lazy"></li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="master-data-structures">Master Data Structures</h3>
<ul>
<li>master 节点保存了几个数据结构：
<ul>
<li>对于每个 map 和 reduce task，它存储了 task 对应的状态（idle, in-progress, completed）</li>
<li>worker machine 的标识（非空闲任务运行所在的机器）</li>
</ul>
</li>
<li>master 是一个管道，通过它将中间文件区域的位置从 map task 传播到 reduce task。因此，对于每个完成的 map task，master 存储了 map task 产生的 R 个中间文件区域的位置和大小，当 map task 完成时，master 将接收对该位置和大小信息的更新。信息被递增地推送给正在进行 reduce task 的 worker。</li>
</ul>
<h3 id="fault-tolerance">Fault Tolerance</h3>
<ul>
<li>由于 MapReduce 库被设计用来帮助处理使用成百上千台机器的大量数据，所以这个库必须能够优雅地容忍机器故障。</li>
</ul>
<h4 id="worker-failure">Worker Failure</h4>
<ul>
<li>master 周期地 ping 每个 worker，如果在确定时间内未收到对应的响应，则认为该 worker 宕机，标记该 worker 为 failed，由 worker <strong>已经完成</strong>的任何 map task 都将被重置回初始的空闲 <em>idle</em> 状态，因此有资格对其他 worker 进行重新调度，类似地，在一个失败的 worker 上<strong>正在进行</strong>的任何 map task 或reduce task 也会被重置为空闲，并可以重新调度。</li>
<li>在发生故障时，完成的 map task 将被重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。<strong>已完成的 reduce 任务</strong>不需要重新执行，因为它们的输出存储在全局文件系统中。</li>
<li>假设一个 map task A 一开始由 A 执行，之后由 B 执行（因为 A failed），所有正在执行 reduce task 的 workers 将被通知重新执行，任何尚未从 worker A 读取数据的 reduce task 都将从 worker B 读取数据。</li>
</ul>
<h4 id="master-failure">Master Failure</h4>
<ul>
<li>让 master 定期对上面描述的 master 节点存取的数据结构做 checkpoint 很容易。如果 master task 失效，可以从最后一个检查点状态启动一个新的副本。然而，考虑到只有一个主机，它的失败是不太多见，因此，如果 master 失败，我们当前的实现将终止 MapReduce 计算。客户端可以检查这种情况，如果他们愿意，可以重试 MapReduce 操作。</li>
</ul>
<h4 id="semantics-in-the-presence-of-failures">Semantics in the Presence of Failures</h4>
<ul>
<li>当用户提供的 map 和 reduce 操作符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的非故障顺序执行产生的输出相同。</li>
<li>我们依赖 map 和 reduce task 输出的原子提交来实现该属性。每个正在进行的任务都将其输出写入私有临时文件。一个 reduce task 生成一个这样的文件，map task 生成 R 个这样的文件(每个 reduce task 一个)。当 map task 完成时，worker 向 master 发送一条消息，并在消息中包含 R 临时文件的名称，如果 master 接收到一个<strong>已经完成</strong>的 map task 的完成消息，它将忽略该消息。否则，它将在主数据结构中记录 R 文件的名称。</li>
<li>当一个 reduce task 完成之后，reduce worker 自动地将其临时输出文件重命名为最终输出文件。如果在多台机器上执行相同的 reduce 任务，那么将对相同的最终输出文件执行多个 rename 调用。我们依赖于底层文件系统提供的原子重命名操作，以确保最终文件系统状态只包含一次执行 reduce 任务所产生的数据。</li>
<li>我们的 map 和 reduce 操作符绝大多数都是确定性的，在这种情况下，我们的语义等价于顺序执行，这使得程序员可以很容易地推断他们的程序行为。当 map 或 reduce 操作符是不确定的时，我们提供较弱但仍然合理的语义。在存在非确定性操作符的情况下，特定 reduce 任务 R1 的输出等价于由非确定性程序的顺序执行产生的 R1 的输出。然而，不同 reduce 任务 R2 的输出可能对应于非确定性程序的不同顺序执行产生的 R2 输出。</li>
<li>考虑 map 任务 M 和 reduce 任务 R1 和 R2。设 e(Ri) 是所承诺的 Ri 的执行(只有一个这样的执行)。由于 e(R1) 可能读取了 M 的一次执行产生的输出，而 e(R2) 可能读取了 M 的另一次执行产生的输出，所以语义较弱。</li>
</ul>
<h3 id="locality">Locality</h3>
<ul>
<li>在我们的计算环境中，网络带宽是一个相对稀缺的资源。通过利用输入数据(由 GFS 管理)存储在组成集群的机器的本地磁盘这一事实，我们节约了网络带宽。GFS 将每个文件划分为64 MB的块，并在不同的机器上存储每个块的多个副本(通常是3个副本)，MapReduce master 将输入文件的位置信息考虑在内，并尝试在包含相应输入数据副本的机器上调度map任务。如果失败，它将尝试调度靠近该任务输入数据副本的 map 任务(例如，在与包含数据的机器在同一网络交换机上的工作机器上)。当在集群中相当一部分 worker 上运行大型MapReduce 操作时，大部分输入数据都是在本地读取的，不会消耗网络带宽</li>
</ul>
<h3 id="task-granularity">Task Granularity</h3>
<ul>
<li>如上所述，我们将 map 阶段细分为 M 个部分，将 reduce 阶段细分为 R 个部分。理想情况下，M 和 R 应该远远大于工作机器的数量。让每个 worker 执行许多不同的任务可以改善动态负载平衡，并在 worker 失败时加快恢复速度:它完成的许多 map 任务可以分散到所有其他 worker 机器上。</li>
<li>在我们的实现中，M 和 R 的大小最多有多大是有实际限制的，因为如上所述，master 必须做出 O(M + R) 调度决策，并在内存中保持 O(M*R) 状态。(内存使用的常量是很小的:状态的 O(M∗R) 部分由每个 map任务/reduce任务对大约一个字节的数据组成。)</li>
<li>此外，R 常常受到用户的限制，因为每个 reduce 任务的输出都以单独的输出文件结束。在实践中，我们倾向于选择 M，以便每个单独的任务大约有 16MB 到 64MB 的输入数据(以便上面描述的局部性优化最有效)，并且我们将 R 设为预期使用的工作机器数量的小倍数。我们经常使用 2000 台 worker 机器进行 M = 200000 和 R = 5000 的 MapReduce 计算。</li>
</ul>
<h3 id="backup-tasks">Backup Tasks</h3>
<ul>
<li>导致 MapReduce 操作总时间延长的一个常见原因是“掉线”(straggler)。一种需要异常长时间才能完成计算过程中最后几个 map 或 reduce 任务之一的机器。掉队者出现的原因有很多。例如，磁盘有问题的机器可能会经常出现可纠正错误，导致读性能从 30MB/s 降至 1MB/s。集群调度系统可能已经调度了机器上的其他任务，由于 CPU、内存、本地磁盘或网络带宽的竞争，导致它执行 MapReduce 代码的速度变慢。我们最近遇到的一个问题是，机器初始化代码中的一个bug导致了处理器缓存被禁用:受影响机器的计算速度降低了 100 倍以上。</li>
<li>我们有一个一般性的机制来缓解掉队者的问题。当 MapReduce 操作接近完成时，master 会对剩余的正在执行的任务进行备份。只要主执行或备份执行完成，任务就被标记为完成。我们已经调优了这种机制，因此它通常不会增加操作使用的计算资源超过几个百分点。我们发现，这大大减少了完成大型 MapReduce 操作的时间。以5.3中所述的排序程序为例，关闭备份机制后，排序程序完成的时间会增加 44%。</li>
</ul>
<h2 id="refinements">Refinements</h2>
<h3 id="partitioning-function">Partitioning Function</h3>
<ul>
<li>MapReduce 的用户指定他们想要的 reduce 任务/输出文件的数量(R)，使用中间键上的分区函数在这些任务之间对数据进行分区。提供了一个默认的分区函数，使用哈希(例如&quot; hash(key) mod R &quot;)。这往往会导致相当平衡的分区。然而，在某些情况下，通过键的其他函数来分区数据是有用的。例如，有时输出键是url，我们希望单个主机的所有条目都在同一个输出文件中结束。为了支持这种情况，MapReduce 库的用户可以提供一个特殊的分区函数。例如，使用&quot; hash(Hostname(urlkey)) mod R &quot;作为分区函数会导致来自同一主机的所有 url 最终出现在同一个输出文件中。</li>
</ul>
<h3 id="ordering-guarantees">Ordering Guarantees</h3>
<ul>
<li>我们保证在给定的分区中，中间键/值对按键的递增顺序进行处理。这种排序保证使得为每个分区生成有序的输出文件变得很容易，当输出文件格式需要支持按键进行有效的随机访问查找，或者输出的用户发现对数据进行排序很方便时，这很有用。</li>
</ul>
<h3 id="combiner-function">Combiner Function</h3>
<ul>
<li>在某些情况下，每个 map 任务产生的中间键有显著的重复，并且用户指定的 Reduce 函数是可交换的和关联的。单词计数示例就是一个很好的例子。由于单词频率倾向于遵循 Zipf 分布，每个 map 任务将产生数百或数千个 &lt;the, 1&gt; 形式的记录。所有这些计数将通过网络发送到一个 reduce 任务，然后由 reduce 函数相加产生一个数字。我们允许用户指定一个可选的Combiner函数，该函数在通过网络发送数据之前对数据进行部分合并。</li>
<li>Combiner 函数在每一个执行 map task 上的机器执行，通常使用相同的代码来实现 combiner 和 reduce 函数，reduce 函数和 combiner 函数之间的唯一区别是 MapReduce 库如何处理函数的输出。reduce 函数的输出被写入最终的输出文件。combiner 函数的输出被写入一个中间文件，该文件将被发送到reduce 任务。</li>
<li>部分 Combine 大大加快了 MapReduce 操作的某些类。</li>
</ul>
<h3 id="input-and-output-types">Input and Output Types</h3>
<ul>
<li>MapReduce 库支持以几种不同的格式读取输入数据。text 模式的输入将每一行视为键/值对：键是文件中的偏移量，值是行内容。另一种常见的支持格式存储按键排序的键/值对序列。每个输入类型实现都知道如何将自己分割成有意义的范围，以便作为单独的 map 任务进行处理(例如，文本模式的范围分割确保范围分割只发生在行边界)。用户可以通过提供一个简单的 <em>reader</em> 接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义输入类型中的一种。</li>
<li><em>reader</em> 并不一定需要提供从文件中读取的数据。例如，很容易定义从数据库或映射在内存中的数据结构中读取记录的 <em>reader</em>。</li>
<li>以类似的方式，我们支持一组输出类型来生成不同格式的数据，并且用户代码很容易添加对新输出类型的支持。</li>
</ul>
<h3 id="side-effects">Side-effects</h3>
<ul>
<li>在某些情况下，MapReduce 的用户发现从他们的 map 或 reduce 操作生成辅助文件作为额外的输出是很方便的。我们依靠应用程序 writer 使这些副作用具有原子性和幂等性。通常，应用程序会写入一个临时文件，并在完全生成该文件后自动重命名该文件。</li>
<li>我们不支持单个任务生成的多个输出文件的原子两阶段提交。因此，产生具有跨文件一致性要求的多个输出文件的任务应该是确定的。这种限制在实践中从来就不是问题。</li>
</ul>
<h3 id="skipping-bad-records">Skipping Bad Records</h3>
<ul>
<li>有时，用户代码中的错误会导致 Map 或 Reduce 函数在特定记录上崩溃。此类 bug 会导致 MapReduce 操作无法完成。通常的做法是修复 bug，但有时这是不可行的;这个 bug 可能存在于源代码不可用的第三方库中。此外，有时忽略一些记录是可以接受的，例如在对一个大数据集进行统计分析时。我们提供了一个可选的执行模式，MapReduce 库会检测哪些记录导致确定性崩溃，并跳过这些记录以便继续前进。</li>
<li>每个工作进程都安装一个信号处理程序来捕获分割违规和总线错误。在调用 Map 或 Reduce 操作之前，MapReduce 库会将参数的序号存储在全局变量中。如果用户代码产生信号，信号处理器发送一个包含序列号的“最后一口气” UDP 包给 MapReduce master。当主服务器在一个特定的记录上看到多个失败时，它指示在下一次重新执行对应的 Map 或 Reduce 任务时应该跳过该记录。</li>
</ul>
<h3 id="local-execution">Local Execution</h3>
<ul>
<li>在 Map 或 Reduce 函数中调试问题可能会很棘手，因为实际的计算发生在分布式系统中，通常在几千台机器上，由 master 动态地做出工作分配决策。为了方便调试、分析和小规模测试，我们开发了 MapReduce 库的替代实现，在本地机器上顺序执行 MapReduce 操作的所有工作。控件提供给用户，以便计算可以限制到特定的映射任务。用户可以用一个特殊的标志来调用他们的程序，然后可以很容易地使用任何他们认为有用的调试或测试工具(例如gdb)。</li>
</ul>
<h3 id="status-information">Status Information</h3>
<ul>
<li>主服务器运行一个内部HTTP服务器，并导出一组状态页面供人们使用。状态页面显示了计算的进度，例如有多少任务已经完成，有多少任务正在进行，输入字节数，中间数据字节数，输出字节数，处理速率等。这些页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算将花费多长时间，以及是否应该向计算中添加更多的资源。这些页面还可以用于计算何时会比预期的慢得多。</li>
<li>此外，顶级状态页面显示哪些 worker 失败了，以及当他们失败时正在处理哪些 map 和 reduce 任务。当试图诊断用户代码中的错误时，此信息非常有用。</li>
</ul>
<h3 id="counters">Counters</h3>
<ul>
<li>MapReduce 库提供了一个计数器来计算各种事件的发生次数。例如，用户代码可能需要计算已处理的字的总数或索引的德文文档的数量，等等。</li>
<li>要使用这个功能，用户代码创建一个命名的计数器对象，然后在 Map 或 Reduce 函数中适当地增加计数器。例如:</li>
</ul>
<pre><code class="language-C++">Counter* uppercase; 
uppercase = GetCounter(&quot;uppercase&quot;);
map(String name, String contents): 
  for each word w in contents: 
    if (IsCapitalized(w)): 
      uppercase-&gt;Increment(); 
    EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<ul>
<li>来自各个 worker 机器的计数器值定期传播到 master (在 ping 响应中附带)。master 聚合成功的 map 和 reduce 任务的计数器值，并在 MapReduce 操作完成时将它们返回给用户代码。当前计数器值也显示在 master 状态页面上，以便人们可以观看实时计算的进度。在聚合计数器值时，master 消除了重复执行同一个 map 或 reduce 任务的影响，以避免重复计算。(重复执行可能是由于我们使用了备份任务以及由于失败而重新执行任务引起的。)</li>
<li>一些计数器值由 MapReduce 库自动维护，例如处理的输入键/值对的数量和产生的输出键/值对的数量。</li>
<li>用户已经发现 counter 工具对于检查 MapReduce 操作的行为是非常有用的。例如，在一些 MapReduce 操作中，用户代码可能希望确保生成的输出对的数量恰好等于处理的输入对的数量，或者确保处理的德文文档的比例在处理的文档总数的某个可容忍的比例内。</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>MapReduce编程模型已经在谷歌上成功地用于许多不同的目的。我们认为这种成功有几个原因。首先，该模型易于使用，即使对于没有并行和分布式系统经验的程序员也是如此，因为它隐藏了并行化、容错、局部性优化和负载平衡的细节。其次，大量的问题可以通过MapReduce计算很容易地表达出来。例如，MapReduce被用于谷歌生产web搜索服务的数据生成、排序、数据挖掘、机器学习以及许多其他系统。第三，我们开发了一个MapReduce的实现，它可以扩展到由数千台机器组成的大型机器集群。该实现有效地利用了这些机器资源，因此适合用于在谷歌中遇到的许多大型计算问题。</li>
<li>我们从这项工作中学到了一些东西。首先，对编程模型的限制使得并行化和分布式计算变得容易，并使这些计算具有容错性。其次，网络带宽是一种稀缺资源。因此，我们系统中的许多优化都旨在减少通过网络发送的数据量:局部性优化允许我们从本地磁盘读取数据，将中间数据的单个副本写入本地磁盘可以节省网络带宽。第三，冗余执行可以用来减少慢速机器的影响，并处理机器故障和数据丢失。</li>
</ul>
<pre><code class="language-C++">#include &quot;mapreduce/mapreduce.h&quot;
// 用户实现map函数
class WordCounter : public Mapper {
 public:
    virtual void Map(const MapInput&amp; input) {
      const string&amp; text = input.value();
      const int n = text.size();
      for (int i = 0; i &lt; n; ) {
        // 跳过前导空格
        while ((i &lt; n) &amp;&amp; isspace(text[i]))
             i++;
         // 查找单词的结束位置
         int start = i;
         while ((i &lt; n) &amp;&amp; !isspace(text[i]))
              i++;
         if (start &lt; i)
            Emit(text.substr(start,i-start),&quot;1&quot;);
        }
 
     }
 
};
 
REGISTER_MAPPER(WordCounter);
// 用户实现reduce函数
class Adder : public Reducer {
    virtual void Reduce(ReduceInput* input) {
              // 迭代具有相同key的所有条目,并且累加它们的value
              int64 value = 0;
              while (!input-&gt;done()) {
                     value += StringToInt(input-&gt;value());
                     input-&gt;NextValue();
              }
              // 提交这个输入key的综合
              Emit(IntToString(value));
       }
 
};
REGISTER_REDUCER(Adder);
int main(int argc, char** argv) {
       ParseCommandLineFlags(argc, argv);
       MapReduceSpecification spec;
       // 把输入文件列表存入&quot;spec&quot;
       for (int i = 1; i &lt; argc; i++) {
              MapReduceInput* input = spec.add_input();
              input-&gt;set_format(&quot;text&quot;);
              input-&gt;set_filepattern(argv[i]);
              input-&gt;set_mapper_class(&quot;WordCounter&quot;);
       }
        //指定输出文件:
       // /gfs/test/freq-00000-of-00100
       // /gfs/test/freq-00001-of-00100
      // ...
       MapReduceOutput* out = spec.output();
       out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);
       out-&gt;set_num_tasks(100);
       out-&gt;set_format(&quot;text&quot;);
       out-&gt;set_reducer_class(&quot;Adder&quot;);
       // 可选操作:在map任务中做部分累加工作,以便节省带宽
       out-&gt;set_combiner_class(&quot;Adder&quot;);
       // 调整参数: 使用2000台机器,每个任务100MB内存
       spec.set_machines(2000);
       spec.set_map_megabytes(100);
       spec.set_reduce_megabytes(100);
       // 运行它
       MapReduceResult result;
       if (!MapReduce(spec, &amp;result)) abort();
       // 完成: 'result'结构包含计数,花费时间,和使用机器的信息
       return 0;
</code></pre>
<hr>
<ul>
<li>论文的部分到此结束，后面展开讲一下 MapReduce 的其他东西。</li>
</ul>
<h2 id="other">Other</h2>
<ul>
<li>MapReduce 最重要的贡献：MR takes care of, and hides, all aspects of distribution!</li>
</ul>
<h3 id="problems">Problems</h3>
<ul>
<li><strong>What if the master gives two workers the same Map() task?</strong>
<ul>
<li>Perhaps the master incorrectly thinks one worker died. it will tell Reduce workers about only one of them.</li>
</ul>
</li>
<li><strong>What if the master gives two workers the same Reduce() task?</strong>
<ul>
<li>they will both try to write the same output file on GFS! atomic GFS rename prevents mixing; one complete file will be visible.</li>
</ul>
</li>
<li><strong>What if a single worker is very slow -- a &quot;straggler&quot;?</strong>
<ul>
<li>perhaps due to flakey hardware. master starts a second copy of last few tasks.</li>
</ul>
</li>
<li><strong>What if a worker computes incorrect output, due to broken h/w or s/w?</strong>
<ul>
<li>too bad! MR assumes &quot;fail-stop&quot; CPUs and software.</li>
</ul>
</li>
</ul>
<h3 id="current-status">Current status</h3>
<ul>
<li>Hugely influential (Hadoop, Spark, &amp;c).</li>
<li>Probably no longer in use at Google.
<ul>
<li>Replaced by Flume / FlumeJava (see paper by Chambers et al).</li>
<li>GFS replaced by Colossus (no good description), and BigTable.</li>
</ul>
</li>
</ul>
<h3 id="conclusion-2">Conclusion</h3>
<ul>
<li>MapReduce single-handedly made big cluster computation popular.
<ul>
<li>-Not the most efficient or flexible.</li>
<li>+Scales well.</li>
<li>+Easy to program -- failures and data movement are hidden.</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is license for source code?]]></title>
        <id>https://blog.shunzi.tech/post/license/</id>
        <link href="https://blog.shunzi.tech/post/license/">
        </link>
        <updated>2021-02-06T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。</li>
</ul>
</blockquote>
<!--more-->
<h3 id="开源许可长啥样">开源许可长啥样？</h3>
<ul>
<li>我们常常在 Github 上看到关于 License 的信息，仿佛 NB 点的项目都挂了个 License（啊没有不挂就不 NB 的意思），一般都长成下面这样，只是可能协议啥的会有区别。如 RocksDB 的 <a href="https://github.com/facebook/rocksdb">repo</a>，使用了 <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GPLv2</a> 和 <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a> 协议，然后 repo 内也有相应的协议文件与之对应，COPYING 和 LICENSE.Apache<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206203249.png" alt="20210206203249" loading="lazy"></li>
</ul>
<h3 id="到底啥是开源许可">到底啥是开源许可？</h3>
<ul>
<li>License 可能大家听说的相对于 Copyright 少一点。那么不妨先说啥是 Copyright。</li>
</ul>
<h4 id="copyrightc">Copyright(C)</h4>
<ul>
<li><strong>Copyright</strong>：中文译作版权，大家在一些数字媒体软件上可能感受的真切一点，比如某首歌只有部分音乐公司拥有其版权，那为什么有的公司没有版权就不能提供相关音乐的播放和下载服务呢，不妨拆词解义 copy + right，即复制的权利，说的简单点就是没有某个产品 Copyright 的公司就无法对该产品进行复制，就更别说进行修改发布了。</li>
</ul>
<blockquote>
<p>百度百科：版权是对计算机程序、文学著作、音乐作品、照片、游戏，电影等的复制权利的合法所有权。除非转让给另一方，版权通常被认为是属于作者的。大多数计算机程序不仅受到版权的保护，还受软件许可证的保护。版权只保护思想的表达形式，而不保护思想本身。算法、数学方法、技术或机器的设计均不在版权的保护之列。</p>
</blockquote>
<ul>
<li>如果有去公司实习或者工作过的同学应该就知道，往往在公司的项目里写相关代码的时候往往会有一条编程规范的限制，即 Copyright 的声明，许多 IDE 也有相关 Copyright 模板和自动生成插件的提供。如下为 RocksDB 源代码中关于 CopyRight 的声明。Copyright 约定了版权归属谁，并归定了这个软件的使用许可证方式。</li>
</ul>
<pre><code class="language-Java">// Copyright (c) 2011-present, Facebook, Inc.  All rights reserved.
//  This source code is licensed under both the GPLv2 (found in the
//  COPYING file in the root directory) and Apache 2.0 License
//  (found in the LICENSE.Apache file in the root directory).

package org.rocksdb;

public abstract class Cache extends RocksObject {
  protected Cache(final long nativeHandle) {
    super(nativeHandle);
  }
}
</code></pre>
<ul>
<li>Copyright 是作者或者创建者因为其原创性的工作，所拥有的复制，分发，出售以及其他一系列的排他性权利，如上所示代码声明了“这段代码的版权归属于 Facebook”，拥有一切版权保护的相关权利。</li>
</ul>
<h4 id="copyleftɔ">Copyleft(Ɔ)</h4>
<ul>
<li>其实还有个东西叫 <strong>Copyleft(Ɔ)</strong>，
<ul>
<li>“Copyleft”最初是为反对商业软件而生，但它并不是放弃版权。反对软件一切权利归作者私有，保护知识共享、权利共享。</li>
<li>软件的版权归原作者所有，其它一切权利归任何人所有。用户和软件的作者享有除版权外的完全同等的权利，包括复制软件和重新发布修改过的软件的权利。</li>
<li>自由软件在承认著作权的基础上，可以通过许可协议，与公众共享作品的其它权利</li>
</ul>
</li>
</ul>
<blockquote>
<p>百度百科：著佐权（Copyleft）是一个由自由软件运动所发展的概念，是一种利用现有著作权体制来保护所有用户和二次开发者的自由的授权方式。在自由软件授权方式中增加著佐权条款之后，该自由软件除了允许使用者自由使用、散布、修改之外，著佐权许可证更要求使用者修改后的衍生作品必须要以同等的授权方式（除非许可证或者版权声明里面例外条款所规定的外）释出以回馈社会。</p>
</blockquote>
<ul>
<li>所以正是因为 <strong>Copyleft(Ɔ)</strong> 的思想，才逐渐衍生出后来的 <strong>License</strong>。可以简单理解为 <code>Copyleft = Copyright+GPL</code></li>
</ul>
<h4 id="license">License</h4>
<ul>
<li>版权法默认禁止共享，也就是说，没有许可证的软件，就等同于保留版权，虽然开源了，用户只能看看源码，不能用，一用就会侵犯版权。所以软件开源的话，必须明确地授予用户开源许可证。</li>
<li>License 是 Copyright 拥有者授予其他人处置其原创性成果的权利，如上代码版权声明所示，“Facebook 授予了这段代码 GPLv2 和 Apache2.0 的许可”。</li>
<li>开放源码许可证是符合开放源码定义的许可证——简而言之，它们允许“<strong>自由</strong>”地使用、修改和共享软件。这里的自由其实是相对的，相应地需要遵守对应 License 下的规定。</li>
<li>软件许可是告诉其他人，他们能够对您的代码做什么，不能做什么。</li>
<li>大多数人将其许可文件放在仓库根目录的文件 <code>LICENSE.txt</code>（或 <code>LICENSE.md</code>）中，如 RocksDB 中的 COPYING 和 LICENSE.Apache。</li>
<li>GPLv2 COPYING</li>
</ul>
<pre><code class="language-txt">                    GNU GENERAL PUBLIC LICENSE
                       Version 2, June 1991

 Copyright (C) 1989, 1991 Free Software Foundation, Inc.,
 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.
 ...
</code></pre>
<ul>
<li>Apache2.0 LICENSE.Apache</li>
</ul>
<pre><code class="language-txt">

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.
   ...
</code></pre>
<ul>
<li>上面其实介绍的都是开源 license，但其实在软件市场中还是有大量的<strong>商业 license</strong> 的，毕竟也是要恰饭的嘛。主要就是一些商业软件的使用，常常需要大家购买对应软件的 license 才能使用，而且很多 license 大多都是有时间期限的，例如 IDEA 可能还有一些 license server 的机制，但总体思想都是通过销售 license 来获取盈利。
<ul>
<li>BTW，很多盗版软件其实就是尝试着去碰撞出一个可能有效的 license，甚至有人共享对应的 license 来进行多用户使用。</li>
</ul>
</li>
</ul>
<h3 id="有哪些开源许可">有哪些开源许可</h3>
<ul>
<li>https://www.gnu.org/licenses/license-list.zh-cn.html</li>
<li>几种常见的开源许可：
<ul>
<li><a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GPL（GNU General Public License）</a>：GNU 通用公共许可协议，免费使用、引用、修改代码，但不能用在闭源软件中发布及销售。“传染性” 表示如果一个软件使用了 GPL 协议的开源代码，那么这个软件也必须开源，仍然免费使用。不能用于商业产品。</li>
<li><a href="https://www.gnu.org/licenses/lgpl-3.0.html">LGPL（GNU Lesser General Public License）</a>：宽松GPL，规定：如果A项目采用LGPL许可证，那么基于A开发出来的B项目也必须采用LGPL，即必须也开源，但是，如果B项目不是基于A开发出来的，而仅仅调用了A的接口，那么B项目可不必开源，倘若换做GPL的话，那么B项目也是要开源的（所以叫做宽松的GPL）。</li>
<li><a href="https://en.wikipedia.org/wiki/BSD_licenses">BSD License（original BSD license、FreeBSD license、Original BSD license）</a>：伯克利软件套装，规定：如果A项目采用BSD许可证，那么基于A开发出来的B项目可以选择闭源，即私有化、商业化，但是必须注明B项目采用了A这个开源项目。<strong>主要限制在于不能用开源代码的作者或机构进行商品推广。</strong></li>
<li><a href="https://en.wikipedia.org/wiki/MIT_License">MIT(The MIT License)</a>：麻省理工学院许可证，规定：这是一个自由度很高的开源许可证，几乎同意了可以随意使用一个开源项目（使用、复制、修改、合并、出版发行、散布、再授权、贩售软件及软件的副本），只要在你的项目中包含或提及原开源项目的MIT许可证。<strong>至于你会不会通过它进行商品推广，作者并不关心，只想保留版权。</strong></li>
<li><a href="https://www.apache.org/licenses/">Apache Licence</a>：Apache软件基金会，规定：大致上和BSD许可证类似，只是有一点细微差别，它除了需要注明B项目源于开源项目A，也要在每个修改过的A项目的文件注明此文件已被修改，并且原文件是A开源项目中的哪个文件。<strong>相对于 MIT，如果修改了源代码，需要进行说明</strong>。</li>
</ul>
</li>
<li>不推荐用于商业产品的协议：GPL (eg. Linux), LGPL, MPL</li>
<li>适用于商业产品的协议：BSD, MIT, Apache (eg. RocksDB)</li>
<li><strong>Dual-Licensed</strong>: 但是我们在如上的 RocksDB 中的例子观察到，RocksDB 中包含了两个开源许可，一个是不推荐用于商业产品的 GPLv2，一个是推荐用于商业产品的 Apache2.0，而在 RocksDB 关于 License 的介绍中我们发现本身该项目就是基于两个开源协议的，而其他软件开发者可以根据自己的实际需求来决定使用哪一个 License。</li>
</ul>
<h4 id="other">Other</h4>
<ul>
<li>可能还会有小伙伴看过这样的版权例子，但大多都是一些知识产品，比如博客、slides、文档以及网页等等。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206230938.png" alt="20210206230938" loading="lazy"></li>
<li><strong>CC License</strong>: Creative Commons license，简称CC许可，是一种公共版权许可协议，知识共享许可协议，其允许分发受版权保护的作品。一个创作共用许可，用于一个作者想给他人分享、使用、甚至创作派生作品的权利。创作共用提供给作者灵活性（例如，他们可以选择允许非商业用途使用他们的作品），保护使用或重新分配他人作品的人，所以他们只要遵守由作者指定的条件，不必担心侵犯版权。</li>
</ul>
<blockquote>
<p>百度百科：知识共享（Creative Commons，简称CC，台湾译名创用CC）是一个非营利组织，也用是一种创作的授权方式。此组织的主要宗旨是增加创意作品的流通可及性，作为其它人据以创作及共享的基础，并寻找适当的法律以确保上述理念。</p>
</blockquote>
<ul>
<li><strong>CC-BY-NC-SA</strong> 本质是几种权利的组合：
<ul>
<li><strong>CC</strong>：创作共用</li>
<li><strong>BY</strong>：署名：您（用户）可以复制、发行、展览、表演、放映、广播或通过信息网络传播本作品；您必须按照作者或者许可人指定的方式对作品进行署名。</li>
<li><strong>NC</strong>：非商业性使用（英语：Noncommercial，nc）您可以自由复制、散布、展示及演出本作品；您不得为商业目的而使用本作品。</li>
<li><strong>SA</strong>：相同方式共享（英语：ShareAlike，sa）您可以自由复制、散布、展示及演出本作品；若您改变、转变或更改本作品，仅在遵守与本作品相同的许可条款下，您才能散布由本作品产生的派生作品。（参见copyleft。）</li>
</ul>
</li>
<li>除此以外还包含一种权利：
<ul>
<li><strong>ND</strong>：禁止演绎（英语：No Derivative Works，nd)，您可以自由复制、散布、展示及演出本作品；您不得改变、转变或更改本作品。</li>
</ul>
</li>
</ul>
<h3 id="怎么选开源许可">怎么选开源许可</h3>
<ul>
<li>如何选 License: https://www.gnu.org/licenses/license-recommendations.html</li>
<li>千言万语一大堆，不如一张图。图源阮一峰博客 http://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206213620.png" alt="20210206213620" loading="lazy"></li>
<li>https://choosealicense.com/<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210206220305.png" alt="20210206220305" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous Merging]]></title>
        <id>https://blog.shunzi.tech/post/Dostoevsky/</id>
        <link href="https://blog.shunzi.tech/post/Dostoevsky/">
        </link>
        <updated>2021-01-12T14:50:15.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based<br>
Key-Value Stores via Adaptive Removal of Superfluous Merging</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based<br>
Key-Value Stores via Adaptive Removal of Superfluous Merging</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>无论是学术界还是工业界，所有主流的基于 LSM 树的键值存储都在更新的 I/O 成本和查询和存储空间的 I/O 成本之间进行了权衡。因为在所有的 LSM Tree 的级别上都需要执行 Compaction 操作来限制查询遍历的 runs，并删除 obsolete 的数据项来腾出存储空间。即便是最先进的 LSM Tree 设计，来自 LSM Tree 所有层此的合并操作（除了最大的层此）减少的点查询成本、大范围查询成本和存储空间，减少的效果可以忽略不计；与此同时还增加了更新操作的平摊开销。</li>
<li>为了解决这个问题，我们提出了 Lazy Leveling，一种新的设计，从除开最大层以外的所有 level 中删除合并操作。同时提出了 Fluid LSM-tree，一种可以涵盖整个 LSM-tree 设计领域的通用设计，可以参数化以假设任何现有的设计。相对于 Lazy level, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。</li>
<li>Dostoevsky，一种键值存储，通过基于应用程序工作负载和硬件来动态调整的弹性的 LSM-tree 设计，自适应地消除多余的合并。基于 RocksDB 实现，测试表明无论是性能还是存储空间方面都优于目前最先进的设计。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>LSM-tree 将要插入/更新的条目缓冲在内存中，并在缓冲区填满时将缓冲区作为 sorted run 刷新到次要存储。LSM-tree 稍后对这些 runs 进行排序合并，以限制查找必须扫描的run 数量，并删除过时的条目。LSM-tree 将运行组织成指数级增长的容量，更大的级别包含更老的运行。当条目被替换更新时，点查找通过从最小到最大的级别查找条目的最新版本，并在查找目标键时终止查找。另一方面，范围查找必须在所有级别的所有 run 中访问相关的键范围，并从结果集中删除过时的条目。为了提高单个 run 的查询速度，设计中常常包含了两个额外的内存中的数据结构。首先，对于每个 run，都有一组包含每个 run 块的第一个键的 fence 指针，这允许查找在一个 run 中只使用一个 I/O 就可以访问特定的键；第二，每个 run 会有一个 BoolmFilter，这允许点查询跳过不包含目标键的 runs。这个设计被应用到了大量的现代 KV 存储中，如 LevelDB、BigTable 等。</li>
<li><strong>问题</strong>：LSM-tree 中的合并操作的频率控制了在 更新的 I/O 成本 和 查询和存储空间放大的 I/O 成本之间的 trade-off，另外的问题就是现有的设计在这些指标之间的 trade-off 并不理想。下图表示了指标之间的权衡关系，虽然这些 y 轴指标具有不同的单位，但它们相对于 x 轴的权衡曲线具有相同的形状。两个极端分别是 log 和 sorted array。LSM-tree在完全不合并或尽可能多地合并时，分别退化为这些边缘点。我们将主流系统放置在这些边缘点之间的顶部曲线上，基于它们的默认合并频率，我们为 Monkey 绘制了一个优越的权衡曲线，我们证明了存在一个甚至比Monkey更好的权衡曲线。现有的设计放弃了大量的性能和/或存储空间，因为没有沿着这条底部曲线设计。</li>
<li><strong>问题来源</strong>：通过分析最先进的 LSM 树的设计空间，我们指出了问题的根源，即最坏情况下的更新代价、点查询代价、范围查询代价和空间放大在不同的层次上产生不同的结果。
<ul>
<li>Update: 更新的 I/O 成本稍后通过更新条目参与的合并操作来分担。虽然较大级别的合并操作需要成倍地增加工作，但它们发生的频率却成倍地减少。因此，更新从所有级别的合并操作中同等地获得它们的 I/O 成本。</li>
<li>Point lookups：虽然 图1 中沿顶部曲线的主流设计将跨 LSM-tree 所有级别的 Bloom flters 假阳性率设置为相同，但目前最先进的 Monkey 为更小的层此设置更低的假阳性率。他被证明可以最小化所有筛选器的误报率之和，从而最小化点查找的 I/O。与此同时，这意味着进入较小层此的可能性呈指数级下降，因此大多数点查询 I/O 将直接命中最大的层次。</li>
<li>Long range lookup：因为 LSM Tree 的容量呈指数级增长，最大曾通常包含绝大部分数据，所以该层更可能包含给定键范围内的数据，因此大多数由大范围查询引起的 I/O 都将对最大层进行操作。</li>
<li>Short range lookup： 使用极小键范围的范围查找在每次 run 中只能访问大约一个块，而不管 run 的大小，因为每一层 run 的最大个数是固定的，因此小范围查询在所有曾中是相当的。</li>
<li>Space-Amplifcation：空间放大最差的情况就是较低层次的数据被更新到最大层此，因此在最大层中老旧的数据项比例最大。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210112173749.png" alt="20210112173749" loading="lazy"></li>
</ul>
</li>
<li>因为最坏情况下的点查询开销、大范围查询开销、空间放大都主要来源于最大层，LSM-tree 中所有级别的合并操作，除开最大层(即大多数合并操作)在这些指标上几乎没有改进，同时显著增加了更新的平摊成本。这导致了次优的权衡。我们用三个步骤从头开始解决这个问题：
<ul>
<li><strong>Solution 1: Lazy Leveling to Remove Superﬂuous Merging</strong>：
<ul>
<li>我们使用 Lazy level 拓展了 LSM-tree 设计思路，这种新设计除去了 LSM-tree 最大级别之外的所有合并。Lazy Leveling 改进了最坏情况下更新的成本复杂性，同时在点查找成本、大范围查找成本和空间放大上保持相同的限制，同时在小范围查找成本上提供具有竞争力的限制。我们证明改进的更新开销可以用来降低点查找开销和空间放大。这生成了 图1 中的底部曲线，它提供了更丰富的时空权衡，这是迄今为止最先进的设计无法实现的。</li>
</ul>
</li>
<li><strong>Solution 2: Fluid LSM-Tree for Design Space Fluidity.</strong>
<ul>
<li>我们引入了 Fluid LSM-tree 作为新一代的 LSM Tree 支持在整个 LSM-tree 设计思路中流畅地切换。Fluid LSM-tree 通过分别控制最大级别和所有其他级别合并操作的频率，相对于 Lazy leveling, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。</li>
</ul>
</li>
<li><strong>Solution 3: Dostoevsky to Navigate the Design Space</strong>
<ul>
<li>Dostoevsky: Space-Time Optimized Evolvable Scalable Key-Value Store。Dostoevsky 分析地找到了Fluid LSM-tree 的调优方法，以最大限度地提高特定应用程序工作负载和硬件在空间放大方面的用户约束，通过精简搜索空间来快速找到最佳调优，并在运行时对其进行物理调整。因为 Dostoevsky 跨越了所有现有的设计，并能够针对给定的应用程序 navigate 到最佳的设计，因此它在性能和空间放大方面严格控制了现有的键值存储。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="background">BACKGROUND</h2>
<ul>
<li>如下图所示，为了优化写操作，LSM-Tree 初始时缓冲了所有的更新、插入和删除操作到内存中，当 Buffer 满了之后，LSM Tree 将 Buffer 以 Sorted Run 刷回到了第二层存储，LSM Tree 归并排序 runs 是为了：限制查询操作必须访问的 runs 的数量，以及删除老旧的数据项以回收空间。runs 被组织成 L 个呈指数级增长的层此，Level 0 是主存中的 Buffer，其他层次都位于二级存储。</li>
<li>在合并的 I/O 开销和查询 I/O 开销以及空间放大之前的权衡可以由两个参数来控制，第一个是相邻两个层次之间的比例 T，T 控制了层级的个数因此决定了一个数据能够在层级之间合并多少次。第二个参数是合并策略，决定了数据项在一个 level 内的合并次数。所有的现有设计都使用了 tiering 或 leveling 两种策略。
<ul>
<li>tiering：当一个 level 到达容量时合并该 level 内的 runs</li>
<li>leveling: 当一个新的 run 出现，就会在 level 中执行合并</li>
</ul>
</li>
<li>如下图所示，size ratio 为 4，buffer 大小为一个 entry 的大小。在两种策略中，当 buffer flushing 造成 Level 1 到达容量时触发合并操作。对于 tiering，Level 1 的所有 runs 都被合并成同一个新的 run 放置在 Level 2。而对于 Leveling，合并操作还会包含 Level2 原有的 run。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210114224107.png" alt="20210114224107" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210115172909.png" alt="20210115172909" loading="lazy"></li>
<li><strong>Number of Levels</strong>：Level 0 拥有的数据项个数 <em>B * P</em>。$$L = [log_T(\frac{N}{B<em>P}</em>\frac{T-1}{T})]$$。层级之间的大小比例 T 被限制到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>≤</mo><mi>T</mi><mo>≤</mo><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">2 ≤ T ≤ T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 被定义成 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>N</mi><mrow><mi>B</mi><mo>∗</mo><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{N}{B*P}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，当 size ratio 达到上界时，levels 的数量减少到接近 1，超过上界之后将无结构性的变化。大于下界就表明 第 i 级合并操作的结果运行永远不会大到超过第 i + 1 级。换句话说就是确保了 runs 不会跨多个 levels。</li>
<li><strong>Finding Entries</strong>：因为数据项是异地更新，相同 key 的数据的多个版本可能出现在多个 level 中，甚至对于 tiering 策略可能存在于一个 level 的多个 runs 中，为了确保查询操作总是能够找到最新版本的数据，LSM Tree 采用了如下措施：1. 当数据项被插入到 buffer 中且 buffer 中包含相同 key 对应的数据时，新的数据项将代替老的数据；2. 当两个包含相同 key 的数据项的 runs 被合并的时候，只有最新版本的数据将被保留；3. 为了能够获取到来自不同 runs 的相同 key 的不同数据项的插入顺序，一个单独的 run 只能够和相邻时间的 run 进行合并。从而保证当有两个 runs 包含不同版本的相同 key 对应的数据时，younger run 包含的是最新版本的数据。</li>
<li><strong>Point Lookups</strong>：点查询通过从最小到最大层次进行遍历来查询最新版本的数据，对于 tiering 则是在一个 level 中从最新到最老的 runs 中遍历进行查询。当找到一个匹配当前 key 的数据时则终止。</li>
<li><strong>Range Lookups</strong>：范围查询需要查找指定范围的键对应的所有最新的数据，通过对所有 levels 所有 runs 的相关键范围进行排序合并。当 sort-merging 时，识别出来自不同 runs 具有相同 key 的数据，然后丢弃掉老版本的数据。</li>
<li><strong>Deletes</strong>：通过给每个数据项添加一位 flag 来实现。如果查询操作找到了该数据想的最新版本，且该数据项上有该 flag 那么将不会返回对应的 value 给应用。当一个删除的数据项和 最老的 run 合并的时候，该数据将被删除，因为该数据项已经代替了之前所有插入的具有当前 key 的数据。</li>
<li><strong>Fragmented Merging</strong>：为了换接较大级别上由于长时间合并操作而导致的性能下降，主流设计把 runs 分区成了文件，也叫 Sorted String Tables，然后一次合并一个 SSTable 和下一个 older run 中具有重叠键范围的多个 SSTables，该技术不会影响最坏情况下的合并 I/O 开销，而只会影响这种开销如何调度。在整篇文章中，为了便于阅读，我们将合并操作讨论为具有 runs 的粒度，尽管它们也可以具有 sstables 的粒度。</li>
<li><strong>Space-Amplifcation</strong>：过时条目的存在使存储空间增大的因素称为空间放大。由于磁盘的可承受性，空间放大传统上并不是数据结构设计的主要关注点。然而，SSD 的出现使空间放大成为一个重要的成本问题。我们将空间放大作为成本指标，以提供我们所引入和评估的设计的完整描述。</li>
<li><strong>Fence Pointers</strong>：所有主要的基于 LSM 树的键值存储都在主存中对每次运行的每个块的第一个键建立索引，也就是图 2 所示的 fence pointer，通常这些指针占据内存空间大小为 O(N/B)，但是让查询操作中找到每个 runs 的 key 范围变成了只需要一次 I/O。</li>
<li><strong>Bloom Filters</strong>：为了加速点查找，只需要在主存中为每个 run 维护一个 BloomFilter，点查找在访问存储中相应的 runs 之前首先检查 Bloom flter。如果 filter 返回 true positive，那么查询操作配合 fence pointer 只需要一次 I/O 就能访问对应的 run，从而找到对应的数据项并终止。如果返回 negative，那么将跳过该 run 并节省一次 I/O 操作。但还有 false positive 的情况，浪费一次 I/O 然后再去下一个 run 继续查找该 key。</li>
<li>Bloom flter 有一个有用的特性，如果它被分割成较小的等大小的 Bloom flter，其中的条目也被等分，每一个新的分区布隆滤片的 FPR 渐近与原滤片的 FPR 相同(虽然实际略高)。为了便于讨论，我们将Bloom flters称为非分区的，尽管它们也可以按照工业中的某些设计进行分区（比如每个 run 的每个 block），从而为空间管理提供更大的灵活性。(例如，对于那些不经常被点查询读取的块，可以将其 offload 到存储器中以节省内存)</li>
<li><strong>Applicability Beyond Key-Value Stores</strong>：根据工业上的设计，我们的讨论假设一个键在运行过程中与它的值相邻存储。为了便于阅读，本文中的所有图形都将条目描述为键，但它们表示键-值对。我们的工作也适用于没有 value 的应用程序(例如，LSM-tree 被用来回答关于键的集合成员查询)，其中的值是指向存储在 LSM-tree 之外的数据对象的指针，或者 LSM-tree 被用作解决更复杂算法问题的构建块(例如，图分析)， FTL 设计等)。我们将分析的范围限制在基本操作和 LSM-tree 的大小上，以便它可以很容易地应用于这些其他情况。</li>
</ul>
<h2 id="design-space-and-problem-analysis">DESIGN SPACE AND PROBLEM ANALYSIS</h2>
<ul>
<li>现在，我们分析更新和查找的最差情况下的空间放大和 I/O 成本是如何从不同的级别派生出与合并策略和大小比例相关的。为了分析更新和查找，我们使用磁盘访问模型来计算每个操作的 I/O 数量，其中 I/O 是从二级存储读取或写入一个块。</li>
<li>分析结果如下所示：
<ul>
<li><strong>Updates</strong>：更新成本通常都是由更新条目参与的后续合并操作产生的，分析假设最坏情况的工作负载，其中所有更新的目标条目都在最大级别。这意味着一个过时的条目不会被删除，直到它相应的更新的条目达到最大级别。因此，每个条目都会在所有级别上合并(即，而不是在某个更小的级别上被最近的条目丢弃，从而减少以后合并操作的开销)。
<ul>
<li>tiering：每层合并 O(1) 次，每个合并过程中的 I/O 操作从原始的 run 中拷贝 B 个数据项到新的 run，因此每个数据项平均的更新操作成本开销如图所示。填满 level i，需要 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">B · P · T^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 次更新，导致合并操作拷贝 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">B · P · T^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 个数据项。</li>
<li>leveling：到达 level i 的第 j 个 run 触发了一个合并操作，合并操作包括 level i 现有的 runs，这些 runs 是自上次 level i 为空以来到达的前 T−j 个 runs 的合并操作产生的。因此平均每个数据项在该层数据到达容量之前合并了 T/2 次，可以表示为 O(T)，同样需要除以一个块对应的 B 个数据项。每 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">B · P · T^{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 次更新（每次有一个新的 run come in）之后执行一次合并操作，然后拷贝平均 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>B</mi><mo separator="true">⋅</mo><mi>P</mi><mo separator="true">⋅</mo><msup><mi>T</mi><mi>i</mi></msup></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{B · P · T^i}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3704599999999998em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0254599999999998em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mpunct mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9020857142857143em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 项数据，通过将复制的条目数除以级别 i 的合并操作的频率，<strong>我们观察到，在长期运行中，每个级别上的合并操作所做的工作量是相同的，直觉是，虽然合并操作在更大的级别上以指数方式完成更多的工作，但它们的频率也以指数方式降低</strong></li>
</ul>
</li>
<li><strong>Analyzing Point Lookups</strong>：为了分析最坏情况下的点查找代价，我们将重点放在 zero-result 点查找（例如查询不存在的 Key）上，因为它们最大化了浪费的 I/O 的平均值。这种分析对于插入前判断是否存在的操作就很有用。开销最大的情况即为所有的 BloomFilter 返回 false positive，此时点查询操作会对每一个 run 发起一次 I/O，对于 leveling 浪费的 I/O 为 O(L)，对于 tiering 浪费的 I/O 为 O(T · L) 。但实际上，Bloom flters 对于不存在的 key 能节省很大一部分 I/O，在工业中，键值存储对每一个Bloom flters使用 10 位，这会导致误报率(FPR)为每个过滤器约为 1%，出于这个原因，我们将重点放在预期的最坏情况点查找成本上，它将点查找发出的 I/O 数量作为关于 Bloom flters FPRs 的长期平均值进行估计。我们估计这个成本为所有Bloom flters的FPRs之和。原因是，查询单个 run 的 I/O 成本是一个独立的随机变量，其期望值等于相应的 Bloom flter 的 FPR，多个独立随机变量的期望值之和等于它们各自的期望值之和。在工业界的键值存储中，所有级别的 BloomFilter 的每个条目的比特数是相同的。因此，最大级别的 Bloom flter(s) 比所有较小级别的 filter 的总和要大，因为它们以指数形式表示更多的条目。根据公式 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mo>(</mo><mi>b</mi><mi>i</mi><mi>t</mi><mi>s</mi><mi mathvariant="normal">/</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>s</mi><mo>)</mo><mo separator="true">⋅</mo><mi>l</mi><mi>n</mi><mo>(</mo><mn>2</mn><msup><mo>)</mo><mn>2</mn></msup></mrow></msup></mrow><annotation encoding="application/x-tex">FPR = e^{−(bits/entries)·ln(2)^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9869199999999998em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869199999999998em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">s</span><span class="mord mtight">/</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">s</span><span class="mclose mtight">)</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">n</span><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，最大层的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><msub><mi>R</mi><mrow><mi>p</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">FPR_{pL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 上界被限制在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，所以对于 leveling，即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span>，对于 tiering，即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo separator="true">⋅</mo><mi>T</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L · T )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span>.
<ul>
<li>关于这个问题的最新论文 Monkey 表明，为所有级别的过滤器设置相同的每个条目的比特数并不能最小化浪费的I/O 的预期数量。相反，Monkey 在最大级别上对 filter 中的每个条目重新分配≈1比特，它使用这些比特来设置较小级别上每个条目的比特数，作为不断增加的等差数列：即 Level i 的每个数据项为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo separator="true">⋅</mo><mo>(</mo><mi>L</mi><mo>−</mo><mi>i</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">a + b · (L - i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span>，a 和 b 都是比较小的常数，这导致 FPR 在最大水平上有一个小的、渐近恒定的增加，在较小的水平上有一个指数下降，因为它们包含较少的条目。由于 FPRs 在较小的级别是指数递减的，所以 FPRs 的总和收敛于一个与级别数无关的乘法常数。Monkey 从点查找的复杂性中去掉了一个 L 的因素，这种复杂性导致了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span> I/O (level)和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">−</mi><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo separator="true">⋅</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e^{−M/N} · L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mclose">)</span></span></span></span> I/O (tiering)，如图3 (B)所示。对于 zero and non-zero result 的结果点查找以及任何类型的偏差，使用 Monkey 总是有益的。</li>
<li>总的来说，我们观察到使用 Monkey 的<strong>点查找成本主要来自于最大的 level，因为较小的 level 的 FPRs 呈指数级下降，所以访问它们的可能性也呈指数级下降</strong>。</li>
</ul>
</li>
<li><strong>Analyzing Range Lookups</strong>：我们将范围查找的 selectivity 表示为在目标键范围内的所有 run 的唯一条目的数量。范围查找在所有 runs 中扫描和排序合并目标键范围，并从结果集中删除过时的条目。范围查询扫描并排序合并所有 runs 的目标键范围，从结果集中消除老数据。为了分析，如果访问的块数至少是可能的最大级别数的两倍，那么就认为范围查询的范围很大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>s</mi><mi>B</mi></mfrac><mo>&gt;</mo><mn>2</mn><mo separator="true">⋅</mo><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{s}{B} &gt; 2 · L_{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，在均匀随机分布的更新下，这个条件意味着在目标键范围内的大多数条目都有很高的概率处于最大级别。
<ul>
<li>小范围查询对每个 run 发起近一个 I/O，叠加起来就是 leveling o(L)，tiering 就是 O(L·T)。对于长范围查询，在消除过时条目之前的结果集的大小平均是其 selectivity 和空间放大的乘积。我们用这个乘积除以块大小来得到 I/O 成本，tiering 即为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mfrac><mrow><mi>T</mi><mo separator="true">⋅</mo><mi>s</mi></mrow><mi>B</mi></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\frac{T·s}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mpunct mtight">⋅</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>，leveling 为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mfrac><mi>s</mi><mi>B</mi></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\frac{s}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.095em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></li>
<li><strong>一个关键的区别是，短范围查找从所有级别获得的开销大致相同，而长范围查找的大部分开销来自访问最大级别</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210116202849.png" alt="20210116202849" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><strong>Analyzing Space-Amplifcation</strong>：我们将空间放大定义为条目总数 N 除以唯一条目数 unq，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>=</mo><mfrac><mi>N</mi><mrow><mi>u</mi><mi>n</mi><mi>q</mi></mrow></mfrac><mi mathvariant="normal">−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">amp = \frac{N}{unq} − 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3534389999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">−</span><span class="mord">1</span></span></span></span>。为了分析最坏情况的空间放大，我们观察到 LSM-tree 的 1 到 L−1 级包含其容量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的一部分，而 L 级包含其容量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>T</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{T−1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的剩余部分。使用 leveing，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，从而导致 Level L 的最多有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 是过时的。空间放大因此是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>T</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(1/T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span>。对于 tiering，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，且 Level L 的每个 run 包含相同的数据项集时，Level L 完全由过时的条目组成，所以空间放大是 O(T)，因为 Level L 比所有其他 Level 加起来要大 T−1 倍。<strong>总的来说，在最坏的情况下，带有 leveling 和 tiering 的空间放大主要是由于在最大级别上存在过时的条目</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118112648.png" alt="20210118112648" loading="lazy"></li>
<li><strong>Mapping the Design Space to the Trade-Oﬀ Space</strong>：更新成本与查找和空间放大成本之间存在一种内在的权衡。如下图实线绘制了在y轴上查找和空间放大的不同成本，以及在x轴上更新的成本(当我们改变大小比例时)。当大小比例设置为其限制值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>T</mi><mrow><mi>l</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{lim}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>(意味着存储中只有一个级别)时，tiered 的 LSM-tree 退化为日志，而 leveled 的 LSM-tree 退化为排序的数组。当尺寸比设置为其下限2时，随着 level 和 tiering 的行为趋于一致，性能特征逐渐收敛：级别的数量是相同的，当第二个 run come in 时，每个级别都会触发合并操作。一般来说，随着 leveling/tiering 大小比例的增加，查找成本和空间放大相应 减少/增加，更新成本相应 增加/减少。因此，对权衡空间进行了分区:与分层相比，level 相比于 tiering 具有更好的查找成本和空间放大，更糟糕的更新成本。</li>
<li><strong>The Holy Grail</strong>：图5中的实线反映了Monkey的属性，即当前的最先进的设计。图5 还显示了标记为“难以捉摸的最佳”的虚线。指导我们研究的问题是，其他设计是否可能通过时空权衡更接近甚至达到难以捉摸的最佳设计。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118143538.png" alt="20210118143538" loading="lazy"></li>
<li><strong>The Opportunity: Removing Superﬂuous Merging</strong>：我们已经确定了<strong>不对称性:点查找成本、长范围查找成本和空间放大主要来自最大的级别，而更新成本来自所有级别</strong>。这意味着在更小的级别上合并操作显著地放大了更新成本，同时为空间放大、点查找和远程查找带来的好处相对较小。因此，有一个合并策略的启发，在较小的层次上合并较少次数。</li>
</ul>
<h2 id="lazy-leveling-fluid-lsm-tree-and-dostoevsky">LAZY LEVELING, FLUID LSM-TREE, AND DOSTOEVSKY</h2>
<h3 id="lazy-leveling">Lazy Leveling</h3>
<ul>
<li>Lazy Leveling 一种合并策略，除了LSM-tree的最大级别之外，它完全消除了合并。其动机是，在这些更小的级别上合并会显著增加更新成本，同时对点查找、远程查找和空间放大产生的改进相对较小。相比于 Leveling，Lazy Leveling：
<ul>
<li>improves the cost complexity of updates</li>
<li>maintains the same complexity for point lookups, long range lookups, and space-amplifcation</li>
<li>provides competitive performance for short range lookups.<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118160921.png" alt="20210118160921" loading="lazy"></li>
</ul>
</li>
<li><strong>Basic Structure</strong>：Lazy Leveling 结构如下所示，其核心类似于缓和 tiering 和 leveling 两种结构，它在最大 level 上应用 leveling，在所有其他 level 上应用 tiering。结果，最大 level 的 runs 数量为 1，其他 level 的 runs 数量最多为 T−1 (即，合并操作在第 T 个 run 到达时发生)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210118161152.png" alt="20210118161152" loading="lazy"></li>
<li><strong>Bloom Filters Allocation</strong>：如何保持点查找的成本复杂性不变，尽管有更多的 rims 在较小的级别上被检索。我们通过优化不同级别之间的 BloomFilter 内存预算来做到这一点。我们开始建模点查找成本和 filter 的总体内存占用与 FPRs 有关。最坏情况下，每次查找的预期浪费I/O 数由零结果点查询造成，等于每次运行的 Bloom flters 的误阳性率之和。</li>
</ul>
<h3 id="fluid-lsm-tree">Fluid LSM-Tree</h3>
<h2 id="references">References</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/129355502">[1] 知乎 - 叶提：SIGMOD'18|Dostoevsky</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost]]></title>
        <id>https://blog.shunzi.tech/post/CRaft/</id>
        <link href="https://blog.shunzi.tech/post/CRaft/">
        </link>
        <updated>2020-12-16T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>一致性协议主要是在分布式系统中用于保证可靠性和可用性的，现有的一致性协议大多都是要将日志项给备份到所有的服务器中，这种全量的副本的策略在存储和网络上的开销都很大，严重影响性能，所以后来出现了纠删码，即在保证相同的容错能力的条件下减少存储和网络的开销。</li>
<li>RS-Paxos 是第一个支持 EC 数据的一致性协议，但是比起通用的一致性协议，如 Paxos/Raft，可用性都相对更差。我们指出了RSPaxos的活性问题，并试图解决，基于 Raft 提出了 CRaft，既能使用 EC 码像 RS-Paxos 一样降低存储和网络开销，也能保证如 Raft 一样的 liveness。</li>
<li>基于 CRaft 实现了一个 KVs，实验表明相对于 Raft 节省了 66% 的存储空间，写吞吐量提升了 250%，写延迟减少了 60.8%</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>共识算法介绍</strong>：共识协议协议通常保证安全性和活动性，这意味着它们总是返回正确的结果，并且在大多数服务器都没有发生故障的情况下可以完全正常工作。
<ul>
<li>Google’s Chubby 会使用 Paxos 对 metadata 做副本</li>
<li>Gaios(NSDI2011) 表明一致性协议可以被用于所有数据的 replicated</li>
<li>现如今大量应用如 etcd, TinyKV, FSS 等大规模系统都使用了 Raft/Paxos 来 replicated TB 数量级的数据，并提供更好的可用性</li>
</ul>
</li>
<li><strong>多副本介绍</strong>：数据操作通常在分布式系统中被转换为一系列的日志指令，然后使用一致性协议在所有的服务器之间进行备份，所以数据需要经过网络传输到所有的服务器，然后还要刷会到磁盘持久化保存。一致性问题中，容错率如果为 F，那么则至少需要 N = (2F + 1) 的服务器，否则就可能因为分组的原因出现不一致的情况。因此传统的副本策略往往就意味值原始数据量的 N 倍的网络和存储开销，而且随着这些协议在大规模存储系统中得到了越来越多的应用，N 倍的网络和存储开销带来的则是延迟的增加和吞吐量的下降。<strong>所以出现了 Erasure Coding</strong></li>
<li><strong>纠删码介绍</strong>：纠删码相比于全量拷贝的副本策略，极大地减小了存储和网络的开销。通过将数据进行分片，编码分片后的数据并生成一些校验的分片，原始的数据就能从足够数量的分片子集中恢复出来，这时候每个服务器只存储一个分片，而不是数据的全量拷贝，开销极大减小。FSS 中就使用了纠删码来减少存储开销，但是 FSS 在编码之前使用了一个 5 way 流水线 Paxos 来备份完整的用户数据和元数据，因此额外的网络开销还是有 4 倍数据量大小。</li>
<li><strong>RS-Paxos</strong> 是第一个结合了 Paxos 和 EC 的共识协议，虽然减少了存储和网络的开销，但是在可用性上比 Paxos 还是更差，RA-Paxos 牺牲了 liveness 来使用 EC 提升性能，换句话说就是 RS-Paoxs 如果有 N = (2F + 1) 的服务器不再能容忍 F 个错误，即容错率下降了，主要是因为 RS-Paxos 中的提交要求越来越严格。</li>
<li>作者提出了 erasure-coding-supported version of Raft <strong>CRaft</strong> (Coded Raft)。该方案中，一个 leader 有两种方法备份日志项到 followers，如果 leader 能够和足够数量的 followers 通信，那么 leader 将使用分片后的日志项进行备份，即传统纠删码的方式，否则将备份完整的数据以保证可用性。相比于 RS-Paxos，CRaft 最大的不同是拥有和 Paxos/Raft 相同级别的 liveness，而 RS-Paxos 没有，但是两个方案都节省了网络和存储的成本。</li>
</ul>
<h2 id="background">Background</h2>
<h3 id="raft">Raft</h3>
<ul>
<li>https://raft.github.io/</li>
<li>Raft 原始论文：https://raft.github.io/raft.pdf</li>
<li>Raft 中主要有三个角色/三种状态。Candidate 收到了来自大多数 servers 的选票后成为 Leader，一个 Server 只会给 和该 Server 日志同步的 Candidate 投票。每个 Server 每一轮最多投一次，所以 Raft 保证每一轮最多就一个 leader。
<ul>
<li>Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader。</li>
<li>Follower: 类似选民，完全被动</li>
<li>Candidate候选人: 类似Proposer，可以被选为一个新的 Leader</li>
</ul>
</li>
<li>leader 从客户端接收日志条目，并试图将它们复制到其他服务器，迫使其他服务器的日志与自己的日志一致。当 leader 发现这一轮中有日志被被分到了大多数 servers，该日志项和之前的日志将被安全地应用到状态机中。Leader 将提交并应用这些日志项，然后告诉 followers 也 apply 他们。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20201216220445.png" alt="20201216220445" loading="lazy"></li>
<li>用于实际系统的共识协议通常具有以下特性：
<ul>
<li>Safety：它们不会在所有非拜占庭条件下返回错误的结果</li>
<li>Liveness：只要大多数服务器都处于活动状态，并且能够相互通信和与客户端通信，它们就能完全发挥作用。我们称这组服务器是健康的</li>
</ul>
</li>
<li>Raft 中的 Safety 是由 Leader Completeness Property 来保证的， 如果在给定 term 提交了日志条目，那么该条目将出现在所有编号较高的 term 的 leader 日志中。</li>
<li>Liveness 由 Raft 规则保证，通常使用了一致性协议的系统的服务器的数量常常为奇数，假设 N = 2F + 1，Raft 可以容忍 F 个错误，我们定义一个一致性协议可以容忍的失败数量作为 liveness level，所以此时的 liveness level 为 F，更高的 liveness level 意味着更好的 liveness，没有一个协议的 liveness level 可以达到 F+1，因为如果存在这样的协议，则可能存在两个分裂的 F 个健康服务器组，这两个组可以分别就不同的内容达成一致，这是违反安全特性的。</li>
</ul>
<h3 id="erasure-coding">Erasure Coding</h3>
<ul>
<li>擦除编码是存储系统和网络传输中容忍错误的常用技术。们已经提出了大量的编码，其中最常用的是Reed-Solomon (RS)编码。RS 码中有两个可配置的正整数参数 k 和 m，数据被分成了相同大小的 k 个分片，然后使用这 k 个原始的数据分片计算出 m 个类似的校验分片，也就是编码过程，此时总共将有 k+m 个分片，(k,m)-RS 码就意味着所有分片中的任意 k 个分片就能恢复出原始数据，这就是 RS 码的容错原理。（类似于解方程的过程）</li>
<li>当引入一致性协议，k + m = N，N 为服务器的总数量，存储和网络开销将被见效的全拷贝的 1/k，然而如何保证 safety 和 liveness 不容忽视</li>
</ul>
<h3 id="rs-paxos">RS-Paxos</h3>
<ul>
<li>RS-Paxos 是将纠删编码与 Paxos 相结合的一种 Paxos 的改革版本，可以节省存储和网络成本。在 Paxos 中，命令被完全传输。然而，在 RS-Paxos 中，命令是通过代码片段传输的。根据这一变化，服务器在 RS-Paxos 中只能存储和传输片段，从而降低了存储和网络成本。</li>
<li>为了保证安全性和活动性，Paxos和Raft基于以下包容-排斥原则。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>A</mi><mo>∪</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi>A</mi><mi mathvariant="normal">∣</mi><mo>+</mo><mi mathvariant="normal">∣</mi><mi>B</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">−</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>∩</mo><mi>B</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|A∪B| = |A|+|B| −|A∩B|
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord">−</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span></span></span></span></span></p>
<p>包含排除原则保证在两个不同的服务器组合中至少有一个服务器的数量差距，这样安全性就可以得到保证。</p>
<ul>
<li>RS-Paxos 的想法是增加交集集的大小。具体来说，在选择了一个 (k,m)-RS 代码后，读quorum QR、写 quorum QW 和服务器数量 N 应该符合以下公式。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>R</mi></msub><mo>+</mo><msub><mi>Q</mi><mi>W</mi></msub><mi mathvariant="normal">−</mi><mi>N</mi><mo>≥</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">Q_R +Q_W −N ≥ k
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span></span></p>
]]></content>
    </entry>
</feed>